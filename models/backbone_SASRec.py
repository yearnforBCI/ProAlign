import os
import copy
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
# from SASmodules import SASRec
from models.modules import *

# ==================== [NEW] ProAlign éœ€è¦çš„é¢å¤–å¯¼å…¥ ====================
import pickle
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans


# ==================== [END NEW] ==========================================

# =============================================================================
# Item_Embedding ç±»ï¼šç‰©å“åµŒå…¥å±‚
# è¿™æ˜¯æ‰€æœ‰æ¨¡å‹å…±ç”¨çš„åµŒå…¥å±‚ï¼Œæ”¯æŒå¤šç§åµŒå…¥ç­–ç•¥ï¼š
#   - ID: çº¯ ID åµŒå…¥ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
#   - SI: è¯­ä¹‰åˆå§‹åŒ–ï¼ˆç”¨ LLM åµŒå…¥åˆå§‹åŒ– ID åµŒå…¥ï¼‰
#   - SR: è¯­ä¹‰é‡å»ºï¼ˆID åµŒå…¥ + è¯­è¨€åµŒå…¥ç”¨äºé‡å»ºæŸå¤±ï¼‰
#   - Dual_view: åŒè§†å›¾ï¼ˆLLMESR ç”¨ï¼‰
#   - AP: è‡ªé€‚åº”æŠ•å½±ï¼ˆè¯­è¨€åµŒå…¥ + é€‚é…å™¨ï¼‰
#   - WAP: ç™½åŒ–è‡ªé€‚åº”æŠ•å½±ï¼ˆç™½åŒ–åçš„è¯­è¨€åµŒå…¥ + é€‚é…å™¨ï¼‰
#   - AF: AlphaFuseï¼ˆé›¶ç©ºé—´èåˆï¼‰
# =============================================================================
class Item_Embedding(nn.Module):
    def __init__(self, emb_pipline, **key_words):
        """
        åˆå§‹åŒ–ç‰©å“åµŒå…¥å±‚

        Args:
            emb_pipline: åµŒå…¥ç­–ç•¥ç±»å‹ ("ID"/"SI"/"SR"/"Dual_view"/"AP"/"WAP"/"AF")
            key_words: åŒ…å«å„ç§é…ç½®å‚æ•°çš„å­—å…¸
        """
        super(Item_Embedding, self).__init__()
        # è¯»å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        data_statis = pd.read_pickle(
            os.path.join(key_words["language_embs_path"], 'data_statis.df'))  # './data/ASO/data_statis.df'
        self.state_size = data_statis['seq_size'][0]  # åºåˆ—é•¿åº¦  10
        self.item_num = data_statis['item_num'][0]  # ç‰©å“æ•°é‡  18357
        # æ ¹æ®åµŒå…¥ç­–ç•¥æ„å»ºåµŒå…¥å±‚ï¼ˆä¿®æ”¹ selfï¼Œä¸è¿”å›å€¼ï¼‰
        self.construct_item_embeddings(emb_pipline, **key_words)
        print("Item_Embedding ç±»åˆå§‹åŒ–å®Œæˆ")

    def construct_item_embeddings(self, emb_pipline, **key_words):
        """
        æ ¹æ®åµŒå…¥ç­–ç•¥æ„å»ºç‰©å“åµŒå…¥å±‚

        Args:
            emb_pipline: åµŒå…¥ç­–ç•¥ç±»å‹
        """
        # -------------------- ID: çº¯ ID åµŒå…¥ï¼ˆSASRec åŸºçº¿ï¼‰--------------------
        if emb_pipline == "ID":
            # éšæœºåˆå§‹åŒ– ID åµŒå…¥ï¼Œä¸ä½¿ç”¨ä»»ä½•è¯­ä¹‰ä¿¡æ¯
            self.init_ID_embedding(key_words["hidden_dim"], key_words["ID_embs_init_type"])

        # -------------------- SI: è¯­ä¹‰åˆå§‹åŒ–ï¼ˆLLMInitï¼‰--------------------
        elif emb_pipline == "SI":  # semantic initialization
            # ç”¨ LLM è¯­è¨€åµŒå…¥åˆå§‹åŒ– ID åµŒå…¥ï¼Œä¹‹åå¯å¾®è°ƒ
            self.init_ID_embedding(key_words["hidden_dim"], "language_embeddings", **key_words)

        # -------------------- SR: è¯­ä¹‰é‡å»ºï¼ˆRLMRecï¼‰--------------------
        elif emb_pipline == "SR":  # semantic reconstruction
            # ID åµŒå…¥éšæœºåˆå§‹åŒ–ï¼ŒåŒæ—¶åŠ è½½å†»ç»“çš„è¯­è¨€åµŒå…¥ç”¨äºé‡å»ºæŸå¤±
            self.init_ID_embedding(key_words["hidden_dim"], key_words["ID_embs_init_type"], **key_words)
            language_embs = self.load_language_embeddings(key_words["language_embs_path"],
                                                          key_words["language_model_type"],
                                                          key_words["language_embs_scale"])
            # padding_emb = np.random.rand(language_embs.shape[1])  # padding ID embedding
            # language_embs = np.vstack([language_embs, padding_emb])
            # è¯­è¨€åµŒå…¥å†»ç»“ï¼Œä»…ç”¨äºè®¡ç®—é‡å»ºæŸå¤±
            self.language_embeddings = nn.Embedding.from_pretrained(
                torch.tensor(language_embs, dtype=torch.float32),
                freeze=True,
            )

        # -------------------- Dual_view: åŒè§†å›¾ï¼ˆLLMESRï¼‰--------------------
        elif emb_pipline == "Dual_view":  # Dual view modeling of LLNESR
            # åŒæ—¶ä½¿ç”¨ ID åµŒå…¥å’Œè¯­è¨€åµŒå…¥ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›èåˆ
            self.init_ID_embedding(key_words["hidden_dim"], "language_embeddings", **key_words)
            language_embs = self.load_language_embeddings(key_words["language_embs_path"],
                                                          key_words["language_model_type"],
                                                          key_words["language_embs_scale"])
            padding_emb = np.random.rand(language_embs.shape[1])  # padding ä½ç½®ç”¨éšæœºå‘é‡
            language_embs = np.vstack([language_embs, padding_emb])
            self.language_embeddings = nn.Embedding.from_pretrained(
                torch.tensor(language_embs, dtype=torch.float32),
                freeze=True,
                padding_idx=self.item_num
            )

        # -------------------- AP: è‡ªé€‚åº”æŠ•å½±ï¼ˆMoRec/UniSRecï¼‰--------------------
        elif emb_pipline == "AP":  # Adaptive Projection
            # åŠ è½½è¯­è¨€åµŒå…¥ï¼Œé€šè¿‡é€‚é…å™¨ï¼ˆMLP/MoEï¼‰æŠ•å½±åˆ°éšè—ç©ºé—´
            language_embs = self.load_language_embeddings(key_words["language_embs_path"],
                                                          key_words["language_model_type"],
                                                          key_words["language_embs_scale"])
            padding_emb = np.random.rand(language_embs.shape[1])  # padding ä½ç½®ç”¨éšæœºå‘é‡
            language_embs = np.vstack([language_embs, padding_emb])
            self.language_embeddings = nn.Embedding.from_pretrained(
                torch.tensor(language_embs, dtype=torch.float32),
                freeze=True,
                padding_idx=self.item_num
            )

        # -------------------- WAP: ç™½åŒ–è‡ªé€‚åº”æŠ•å½±ï¼ˆWhitenRecï¼‰--------------------
        elif emb_pipline == "WAP":  # Adaptive Projection for whitened language embeddings
            # å¯¹è¯­è¨€åµŒå…¥è¿›è¡Œ PCA ç™½åŒ–å¤„ç†ï¼Œæ¶ˆé™¤å„ç»´åº¦çš„ç›¸å…³æ€§
            key_words["item_frequency_flag"] = False
            key_words['standardization'] = True
            language_embs = self.semantic_space_decomposion(None, **key_words)
            padding_emb = np.random.rand(language_embs.shape[1])  # padding ä½ç½®ç”¨éšæœºå‘é‡
            language_embs = np.vstack([language_embs, padding_emb])
            self.language_embeddings = nn.Embedding.from_pretrained(
                torch.tensor(language_embs, dtype=torch.float32),
                freeze=True,
                padding_idx=self.item_num
            )

        # -------------------- AF: AlphaFuseï¼ˆæœ¬æ–‡æ–¹æ³•ï¼‰--------------------
        elif emb_pipline == "AF":  # AlphaFuse
            # æ ¸å¿ƒåˆ›æ–°ï¼šåœ¨è¯­è¨€åµŒå…¥çš„é›¶ç©ºé—´ä¸­æ³¨å…¥ ID ä¿¡æ¯
            # 1. å¯¹è¯­è¨€åµŒå…¥è¿›è¡Œ SVD åˆ†è§£ï¼Œè¯†åˆ«é›¶ç©ºé—´ï¼ˆæ–¹å·®å°çš„ç»´åº¦ï¼‰
            # 2. è¯­è¨€åµŒå…¥æŠ•å½±åˆ°ä¸»æˆåˆ†ç©ºé—´ï¼ˆå†»ç»“ï¼‰
            # 3. ID åµŒå…¥åªå­¦ä¹ é›¶ç©ºé—´ç»´åº¦ï¼Œä¸è¯­è¨€åµŒå…¥ç›¸åŠ èåˆ

            cliped_language_embs = self.semantic_space_decomposion(key_words["hidden_dim"], **key_words)  # (18357,128)
            padding_emb = np.random.rand(cliped_language_embs.shape[1])  # padding ä½ç½®ç”¨éšæœºå‘é‡  (128,)
            cliped_language_embs = np.vstack(
                [cliped_language_embs, padding_emb])  # (18358,128)  np.vstackï¼šåœ¨åŸçŸ©é˜µâ€œä¸‹é¢â€å¤šå äº†ä¸€è¡Œ
            # åˆ›å»º LLM ä¾§çš„nn.Embedding
            #
            # å‚æ•°	                    å«ä¹‰
            # from_pretrained	    ç”¨é¢„è®¡ç®—çš„æƒé‡åˆå§‹åŒ–
            # freeze=True	        å†»ç»“æƒé‡ï¼Œä¸æ›´æ–°
            # padding_idx=18357	    ç´¢å¼• 18357 æ˜¯ padding ä½ç½®

            # ä¸ºä»€ä¹ˆä½¿ç”¨ from_pretrainedï¼ŸåŠ è½½é¢„è®¡ç®—å¥½çš„ LLM è¯­è¨€åµŒå…¥ï¼Œè€Œä¸æ˜¯éšæœºåˆå§‹åŒ–
            self.language_embeddings = nn.Embedding.from_pretrained(
                torch.tensor(cliped_language_embs, dtype=torch.float32),  # (18358,128)
                freeze=True,  # â† è¯­è¨€åµŒå…¥å†»ç»“ï¼Œä¸å‚ä¸è®­ç»ƒ
                padding_idx=self.item_num  # (18357)
            )  # (18358,128,padding_idx=18357)

            # åµŒå…¥è¡¨ç»“æ„ï¼š
            #
            # language_embeddings (18358, 128)     ID_embeddings (18358, 64)
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚ ç‰©å“ 0    [128ç»´å‘é‡]    â”‚          â”‚ ç‰©å“ 0 [64] â”‚
            # â”‚ ç‰©å“ 1    [128ç»´å‘é‡]    â”‚          â”‚ ç‰©å“ 1 [64] â”‚
            # â”‚    ...                   â”‚    +    â”‚    ...      â”‚
            # â”‚ ç‰©å“ 18356 [128ç»´å‘é‡]   â”‚          â”‚ ç‰©å“18356   â”‚
            # â”‚ padding   [éšæœº128ç»´]    â”‚          â”‚ padding     â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            #         å†»ç»“                           å¯å­¦ä¹ 

            # self.nullityï¼šè¡¨ç¤º é›¶ç©ºé—´çš„ç»´åº¦æ•°ï¼Œ64
            #
            # key_words["ID_embs_init_type"]
            # ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œæ§åˆ¶ ID åµŒå…¥ç”¨ä»€ä¹ˆæ–¹å¼åˆå§‹åŒ–ï¼Œæ¯”å¦‚ï¼š
            # "normal"ï¼šæ ‡å‡†é«˜æ–¯åˆå§‹åŒ–
            # "uniform"ï¼šå‡åŒ€åˆ†å¸ƒ
            # "zero"ï¼šå…¨ 0
            self.init_ID_embedding(self.nullity, key_words["ID_embs_init_type"])  # (18358, 64) ID åµŒå…¥åªå­¦ä¹ é›¶ç©ºé—´ç»´åº¦ï¼ˆnullity ç»´ï¼‰
            print("åˆå§‹åŒ– language_embeddingså’ŒID_embeddingså®Œæˆï¼")
            # self.init_ID_embedding(self.nullity, "zeros")

    def load_language_embeddings(self, directory, language_model_type, scale):
        """
        åŠ è½½é¢„è®¡ç®—çš„ LLM è¯­è¨€åµŒå…¥

        Args:
            directory: æ•°æ®ç›®å½•è·¯å¾„
            language_model_type: è¯­è¨€æ¨¡å‹ç±»å‹ ("3small" æˆ– "3large")
            scale: ç¼©æ”¾å› å­ï¼ˆæ”¾å¤§åµŒå…¥å€¼ï¼Œé¿å…æ•°å€¼è¿‡å°ï¼‰

        Returns:
            language_embs: [item_num, language_dim] çš„è¯­è¨€åµŒå…¥çŸ©é˜µ
        """
        # ä» pickle æ–‡ä»¶åŠ è½½è¯­è¨€åµŒå…¥
        language_embs = pd.read_pickle(os.path.join(directory,
                                                    language_model_type + '_emb.pickle'))  # './data/ASO/3large_emb.pickle'   (18357,3072)
        self.item_num = len(language_embs)  # ç‰©å“æ•°é‡   18357
        self.language_dim = len(language_embs[0])  # è¯­è¨€åµŒå…¥ç»´åº¦ï¼ˆ3small=1536, 3large=3072ï¼‰  3072
        # np.stack å°†list/pandas.Series å˜æˆä¸€ä¸ªçœŸæ­£çš„ np.ndarrayï¼Œshape = (N, D) çš„çŸ©é˜µ
        # language_embs å·²ç»æ˜¯ä¸€ä¸ª np.ndarrayï¼Œå½¢çŠ¶æ˜¯ (18357, 3072)ï¼Œä¸åŠ  np.stackä¹Ÿå¯ä»¥
        #
        # *scale:å¯¹è¿™ä¸ªçŸ©é˜µçš„æ‰€æœ‰å…ƒç´ ä¹˜ä»¥scale
        return np.stack(language_embs) * scale  # å †å å¹¶ç¼©æ”¾

    def init_ID_embedding(self, ID_dim, init_type, **key_words):
        """
        åˆå§‹åŒ– ID åµŒå…¥å±‚

        Args:
            ID_dim: ID åµŒå…¥ç»´åº¦
            init_type: åˆå§‹åŒ–æ–¹å¼
                - "language_embeddings": ç”¨è¯­è¨€åµŒå…¥åˆå§‹åŒ–ï¼ˆå¯å¾®è°ƒï¼‰
                - "normal": æ ‡å‡†æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
                - "zeros": é›¶åˆå§‹åŒ–
                - "uniform": å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–
                - "ortho": æ­£äº¤åˆå§‹åŒ–
                - "xavier": Xavier åˆå§‹åŒ–
                - "sparse": ç¨€ç–åˆå§‹åŒ–
        """
        if init_type == "language_embeddings":
            # ç”¨ LLM è¯­è¨€åµŒå…¥åˆå§‹åŒ– ID åµŒå…¥ï¼ˆå¯å¾®è°ƒï¼Œfreeze=Falseï¼‰
            language_embs = self.load_language_embeddings(key_words["language_embs_path"],
                                                          key_words["language_model_type"],
                                                          key_words["language_embs_scale"])
            if self.language_dim == ID_dim:
                # è¯­è¨€åµŒå…¥ç»´åº¦ä¸ ID ç»´åº¦ç›¸åŒï¼Œç›´æ¥ä½¿ç”¨
                padding_emb = np.random.rand(language_embs.shape[1])  # padding ID embedding
                language_embs = np.vstack([language_embs, padding_emb])
                # language_embs = np.vstack([language_embs, padding_emb])
                self.ID_embeddings = nn.Embedding.from_pretrained(
                    torch.tensor(language_embs, dtype=torch.float32),
                    freeze=False,  # å¯å¾®è°ƒ
                    padding_idx=self.item_num
                )
            else:
                # è¯­è¨€åµŒå…¥ç»´åº¦ä¸ ID ç»´åº¦ä¸åŒï¼Œéœ€è¦ PCA é™ç»´
                clipped_language_embs = self.semantic_space_decomposion(ID_dim, **key_words)
                padding_emb = np.random.rand(clipped_language_embs.shape[1])  # padding ID embedding
                clipped_language_embs = np.vstack([clipped_language_embs, padding_emb])
                # language_embs = np.vstack([language_embs, padding_emb])
                self.ID_embeddings = nn.Embedding.from_pretrained(
                    torch.tensor(clipped_language_embs, dtype=torch.float32),
                    freeze=False,  # å¯å¾®è°ƒ
                    padding_idx=self.item_num
                )
        else:
            # éšæœºåˆå§‹åŒ– ID åµŒå…¥
            self.ID_embeddings = nn.Embedding(
                num_embeddings=self.item_num + 1,  # +1 æ˜¯ padding ä½ç½®   18358
                embedding_dim=ID_dim,  # 64 ç»´ï¼ˆé›¶ç©ºé—´ç»´åº¦ï¼‰
                # padding_idx=self.item_num  # â† å»ºè®®åŠ ä¸Š
            )  # (18358,64)
            # æ ¹æ® init_type é€‰æ‹©åˆå§‹åŒ–æ–¹å¼
            if init_type == "uniform":
                nn.init.uniform_(self.ID_embeddings.weight, a=0.0, b=1.0)  # U(0, 1)
            elif init_type == "normal":
                nn.init.normal_(self.ID_embeddings.weight, 0, 1)  # N(0, 1)   ç”¨å‡å€¼ 0ã€æ ‡å‡†å·® 1 çš„é«˜æ–¯åˆ†å¸ƒéšæœºåˆå§‹åŒ– ID embedding çš„å‚æ•°
            elif init_type == "zeros":
                nn.init.zeros_(self.ID_embeddings.weight)  # å…¨é›¶
            elif init_type == "ortho":
                nn.init.orthogonal_(self.ID_embeddings.weight, gain=1.0)  # æ­£äº¤çŸ©é˜µ
            elif init_type == "xavier":
                nn.init.xavier_uniform_(self.ID_embeddings.weight, gain=1.0)  # Xavier
            elif init_type == "sparse":
                nn.init.sparse_(self.ID_embeddings.weight, 0.01, std=1)  # ç¨€ç–çŸ©é˜µ
            else:
                raise NotImplementedError("This kind of init for ID embeddings is not implemented yet.")

    def semantic_space_decomposion(self, clipped_dim, **key_words):
        """
        è¯­ä¹‰ç©ºé—´åˆ†è§£ï¼ˆAlphaFuse çš„æ ¸å¿ƒç®—æ³•ï¼‰

        è¿™æ˜¯ AlphaFuse çš„æ ¸å¿ƒåˆ›æ–°ï¼šå¯¹è¯­è¨€åµŒå…¥è¿›è¡Œ SVD åˆ†è§£ï¼Œ
        è¯†åˆ«"é›¶ç©ºé—´"ï¼ˆæ–¹å·®å°çš„ç»´åº¦ï¼‰ï¼Œç”¨äºæ³¨å…¥ ID ä¿¡æ¯ã€‚

        ç®—æ³•æ­¥éª¤ï¼š
        1. è®¡ç®—è¯­è¨€åµŒå…¥çš„åæ–¹å·®çŸ©é˜µ
        2. SVD åˆ†è§£å¾—åˆ°ç‰¹å¾å‘é‡ U å’Œç‰¹å¾å€¼ S
        3. æ ¹æ®é˜ˆå€¼æˆ–ç»´åº¦ç¡®å®šé›¶ç©ºé—´
        4. å¯é€‰ï¼šç™½åŒ–æ ‡å‡†åŒ–ï¼ˆä½¿å„æ–¹å‘æ–¹å·®ä¸º 1ï¼‰
        5. æŠ•å½±åˆ°ä¸»æˆåˆ†ç©ºé—´

        Args:
            clipped_dim: ç›®æ ‡ç»´åº¦ï¼ˆæŠ•å½±åçš„ç»´åº¦ï¼‰

        Returns:
            clipped_language_embs: [item_num, clipped_dim] æŠ•å½±åçš„è¯­è¨€åµŒå…¥
        """
        # åŠ è½½è¯­è¨€åµŒå…¥
        language_embs = self.load_language_embeddings(key_words["language_embs_path"], key_words["language_model_type"],
                                                      key_words["language_embs_scale"])  # (18357,3072)

        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        if not key_words["item_frequency_flag"]:  # ä¸è€ƒè™‘ç‰©å“é¢‘ç‡æ—¶ï¼ŒæŒ‰å‡åŒ€æƒé‡ç®—è¯­è¨€åµŒå…¥çš„å‡å€¼å’Œåæ–¹å·®ï¼ˆä¸æŒ‰ç‰©å“å‡ºç°é¢‘ç‡åŠ æƒï¼Œä¹Ÿå°±æ˜¯é»˜è®¤æ¯ä¸ª item çš„æƒé‡ä¸€æ ·ï¼‰
            # é»˜è®¤ï¼šå‡åŒ€åˆ†å¸ƒï¼ˆæ‰€æœ‰ç‰©å“æƒé‡ç›¸åŒï¼‰
            self.language_mean = np.mean(language_embs, axis=0)  # è®¡ç®—å‡å€¼ (3072,)
            # language_embs - self.language_meanï¼š æŠŠæ¯ä¸ª item çš„å‘é‡éƒ½å‡å»å‡å€¼ ğœ‡ï¼Œåšä¸­å¿ƒåŒ–
            cov = np.cov(language_embs - self.language_mean, rowvar=False)  # è®¡ç®—åæ–¹å·®çŸ©é˜µ (3072, 3072)
        else:
            # å¯é€‰ï¼šæŒ‰ç‰©å“é¢‘ç‡åŠ æƒ
            items_pop = np.load(os.path.join(key_words["language_embs_path"], 'items_pop.npy'))
            items_freq_scale = 1.0 / items_pop.sum()
            items_freq = (items_pop * items_freq_scale).reshape(-1, 1)
            self.language_mean = np.sum(language_embs * items_freq, axis=0)
            cov = np.cov((language_embs - self.language_mean) * np.sqrt(items_freq), rowvar=False)
            # raise NotImplementedError("Custom item distribution is not implemented yet.")

        # SVDåˆ†è§£ï¼ˆSingular Value Decompositionï¼Œä¸­æ–‡ä¸€èˆ¬å«â€œå¥‡å¼‚å€¼åˆ†è§£â€ï¼‰
        # SVD = æŠŠä¸€ä¸ªçŸ©é˜µæ‹†æˆ â€œæ–¹å‘ Ã— æ‹‰ä¼¸å¼ºåº¦ Ã— æ–¹å‘â€ çš„ä¹˜ç§¯
        # Cov = U @ diag(S) @ U^T
        # U: ç‰¹å¾å‘é‡çŸ©é˜µï¼ˆåˆ—ä¸ºä¸»æˆåˆ†æ–¹å‘ï¼‰
        # S: ç‰¹å¾å€¼ï¼ˆå„æ–¹å‘çš„æ–¹å·®ï¼‰

        # cov æ˜¯ (D, D) çš„åæ–¹å·®çŸ©é˜µï¼Œè¿™é‡Œ D=3072
        # SVD ç»“æœï¼š
        # U.shape = (D, D)ï¼šåˆ—å‘é‡ u_i å°±æ˜¯ä¸»æˆåˆ†æ–¹å‘ï¼ˆæ–¹å·®å¤§çš„æ–¹å‘ â†’ è¯­ä¹‰ä¿¡æ¯ä¸°å¯Œï¼ˆrow spaceï¼‰æ–¹å·®å¾ˆå°çš„æ–¹å‘ â†’ å‡ ä¹æ²¡è¯­ä¹‰ï¼ˆæ¥è¿‘é›¶ç©ºé—´ï¼‰ï¼‰ï¼›
        # S.shape = (D,)ï¼šå¥‡å¼‚å€¼ï¼Œå¯¹åº”æ¯ä¸ªè¯­ä¹‰æ–¹å‘ä¸Šçš„æ–¹å·®å¤§å° ï¼ˆè¶Šå¤§ä»£è¡¨è¯­ä¹‰è¶Šå¼ºï¼‰
        U, S, _ = np.linalg.svd(cov, full_matrices=False)

        # ç¡®å®šé›¶ç©ºé—´ç»´åº¦ï¼ˆnullityï¼‰
        if key_words["null_thres"] is not None:
            # æ–¹å¼1ï¼šæ ¹æ®é˜ˆå€¼ç¡®å®šï¼ˆç‰¹å¾å€¼ < é˜ˆå€¼çš„ç»´åº¦ä¸ºé›¶ç©ºé—´ï¼‰
            indices_null = np.where(S <= key_words["null_thres"])[0]
            self.nullity = len(indices_null)
        elif key_words["null_dim"] is not None:
            # æ–¹å¼2ï¼šç›´æ¥æŒ‡å®šé›¶ç©ºé—´ç»´åº¦
            self.nullity = key_words["null_dim"]  # 64
        # print("The Nullity is", self.nullity)
        # self.squared_singular_values = S
        # self.language_bases = U

        # ç¡®å®šæŠ•å½±ç»´åº¦
        if clipped_dim is None:  # 128
            clipped_dim = self.language_dim
        if key_words["cover"]:  # False
            # cover=True	è¦†ç›–	å 64 ç»´å®Œå…¨æ˜¯ ID åµŒå…¥
            # cover=False	æ³¨å…¥	ID åµŒå…¥å åŠ åˆ°å¼±è¯­ä¹‰åŒº
            #
            # å…³é”®åŒºåˆ«ï¼š
            # è¦†ç›–ï¼ˆcoverï¼‰ï¼šå 64 ç»´å®Œå…¨æ˜¯ ID åµŒå…¥
            # æ³¨å…¥ï¼ˆinjectï¼‰ï¼šå 64 ç»´æ˜¯ è¯­ä¹‰ + ID çš„æ··åˆ

            # åŸå§‹ LLM åµŒå…¥ (3072 ç»´)
            #          â†“
            #       SVD åˆ†è§£
            #          â†“
            # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            # â”‚  ç‰¹å¾å€¼å¤§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ç‰¹å¾å€¼å°           â”‚
            # â”‚  â†“                               â†“             â”‚
            # â”‚  ä¸»è¯­ä¹‰ç©ºé—´                      é›¶ç©ºé—´          â”‚
            # â”‚  (semantic space)            (null space)      â”‚
            # â”‚  ä¿ç•™è¯­è¨€æ¨¡å‹çš„                å¯ä»¥æ³¨å…¥ ID        â”‚
            # â”‚  æ ¸å¿ƒè¯­ä¹‰ä¿¡æ¯                  ååŒä¿¡æ¯           â”‚
            # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            # 3072 ç»´é™ç»´åˆ°128 ç»´
            # â”œâ”€â”€ å‰ 64 ç»´ï¼šå¼ºè¯­ä¹‰ï¼ˆä¿ç•™ï¼‰
            # â””â”€â”€ å 64 ç»´ï¼šå¼±è¯­ä¹‰/é›¶ç©ºé—´ï¼ˆè¢« ID åµŒå…¥è¦†ç›–æˆ–æ³¨å…¥ï¼‰
            clipped_dim = clipped_dim - self.nullity

        # æ„é€ æŠ•å½±çŸ©é˜µ
        #
        # U.shape = (D, D)
        # U çš„æ¯ä¸€åˆ— U[:, i] å°±æ˜¯ç¬¬ i ä¸ªä¸»æˆåˆ†æ–¹å‘ï¼ˆè¯­ä¹‰æ–¹å‘ï¼‰ï¼ŒæŒ‰ S ä»å¤§åˆ°å°æ’å¥½
        #
        # U[...,:clipped_dim] æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
        # ... åœ¨ NumPy é‡Œå°±æ˜¯â€œä¿æŒå‰é¢çš„ç»´åº¦éƒ½ä¸åŠ¨â€çš„æ„æ€
        # å¯¹äºŒç»´çŸ©é˜µæ¥è¯´ï¼š U[..., :clipped_dim]  ==  U[:, :clipped_dim]
        # ä¹Ÿå°±æ˜¯ï¼š
        # è¡Œç»´åº¦ï¼š: â†’ å–æ‰€æœ‰è¡Œï¼ˆä¿æŒ 3072 è¡Œï¼‰
        # åˆ—ç»´åº¦ï¼š:clipped_dim â†’ å–å‰ clipped_dim åˆ—
        # ä¹Ÿå°±æ˜¯æŠŠå‰ clipped_dim ä¸ªä¸»æˆåˆ†æ–¹å‘æ‹¿å‡ºæ¥ï¼Œå †æˆä¸€ä¸ªçŸ©é˜µ
        Projection_matrix = U[..., :clipped_dim]  # å–å‰ clipped_dim ä¸ªä¸»æˆåˆ†  (3072,128)

        # å¯é€‰ï¼šç™½åŒ–æ ‡å‡†åŒ–ï¼ˆä½¿å„æ–¹å‘æ–¹å·®ä¸º 1ï¼‰
        if key_words['standardization']:
            # 1. ç™½åŒ–ç³»æ•°è®¡ç®—
            # ç¬¦å·	            å«ä¹‰	                        å½¢çŠ¶
            # S	                SVD ç‰¹å¾å€¼ï¼ˆå„æ–¹å‘çš„æ–¹å·®ï¼‰	    (3072,)
            # 1/S	            æ–¹å·®çš„å€’æ•°	                (3072,)
            # np.sqrt(1/S)	    æ ‡å‡†å·®çš„å€’æ•°	                (3072,)
            # [:clipped_dim]	åªå–å‰ 128 ç»´	            (128,)
            #
            # ä½œç”¨ï¼šæ¶ˆé™¤å„æ–¹å‘çš„æ–¹å·®å·®å¼‚ï¼Œä½¿ç™½åŒ–åæ¯ä¸ªæ–¹å‘æ–¹å·® = 1
            Diagnals = np.sqrt(1 / S)[:clipped_dim]  # 1/sqrt(ç‰¹å¾å€¼)  (128,)
            # 2. æ„é€ ç™½åŒ–æŠ•å½±çŸ©é˜µ
            # åŸå§‹ Projection_matrix	        np.diag(Diagnals)	        ç»“æœ
            # U[:, :128]	                å¯¹è§’çŸ©é˜µ	                ç™½åŒ–æŠ•å½±çŸ©é˜µ
            # (3072, 128)	                (128, 128)	            (3072, 128)
            #
            # æ•°å­¦å«ä¹‰ï¼š
            # åŸå§‹æŠ•å½±ï¼šX @ U â†’ æŠ•å½±åˆ°ä¸»æˆåˆ†ç©ºé—´ï¼ˆå„æ–¹å‘æ–¹å·®ä¸åŒï¼‰
            # ç™½åŒ–æŠ•å½±ï¼šX @ U @ diag(1/âˆšS) â†’ æŠ•å½± + ç¼©æ”¾ï¼ˆå„æ–¹å‘æ–¹å·®=1ï¼‰
            Projection_matrix = Projection_matrix.dot(np.diag(Diagnals))  # V_{\lambda} -> V_1   (3072,128)

        # 3. æœ€ç»ˆæŠ•å½±
        # æ­¥éª¤	                æ“ä½œ	                å½¢çŠ¶å˜åŒ–
        # 1	                  ä¸­å¿ƒåŒ–ï¼ˆå‡å‡å€¼ï¼‰	    (18357, 3072)
        # 2	                  æŠ•å½±ï¼ˆé™ç»´ + ç™½åŒ–ï¼‰	    (18357, 3072) @ (3072, 128) = (18357, 128)
        #
        # å›¾ç¤º
        #
        # åŸå§‹ LLM åµŒå…¥                              ç™½åŒ–åçš„åµŒå…¥
        # (18357, 3072)                             (18357, 128)
        #
        # â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        # â”‚                 â”‚                      â”‚         â”‚
        # â”‚  æ¯ä¸ªç‰©å“       â”‚   å‡å‡å€¼               â”‚  æ¯ä¸ª   â”‚
        # â”‚  3072 ç»´å‘é‡    â”‚  â”€â”€â”€â”€â”€â”€â†’  æŠ•å½±çŸ©é˜µ     â”‚  ç‰©å“   â”‚
        # â”‚                 â”‚          (3072,128)  â”‚  128ç»´  â”‚
        # â”‚  18357 ä¸ªç‰©å“   â”‚                       â”‚         â”‚
        # â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        #
        # æŠ•å½±åç‰¹æ€§ï¼š
        #  å„ç»´åº¦ç‹¬ç«‹ï¼ˆåæ–¹å·®çŸ©é˜µæ˜¯å¯¹è§’é˜µï¼‰
        #  å„ç»´åº¦æ–¹å·® = 1ï¼ˆç™½åŒ–ï¼‰
        #  å‰é¢çš„ç»´åº¦ = å¼ºè¯­ä¹‰
        #  åé¢çš„ç»´åº¦ = å¼±è¯­ä¹‰ï¼ˆé›¶ç©ºé—´ï¼‰
        #
        # ç™½åŒ–çš„ä½œç”¨
        #   ä¸ç™½åŒ–	                    ç™½åŒ–å
        # å‰å‡ ç»´æ–¹å·®å¾ˆå¤§ï¼ˆä¸»æˆåˆ†ï¼‰	    æ‰€æœ‰ç»´åº¦æ–¹å·® = 1
        # åå‡ ç»´æ–¹å·®å¾ˆå°ï¼ˆé›¶ç©ºé—´ï¼‰	    æ‰€æœ‰ç»´åº¦æ–¹å·® = 1
        # ID åµŒå…¥éš¾ä»¥ä¸è¯­ä¹‰åµŒå…¥åŒ¹é…	ID åµŒå…¥æ›´å®¹æ˜“èåˆ
        # æœ¬è´¨ï¼šç™½åŒ–è®©å„ç»´åº¦"å¹³ç­‰"ï¼ŒID ä¿¡æ¯æ³¨å…¥æ—¶ä¸ä¼šè¢«å¼ºè¯­ä¹‰ç»´åº¦å‹åˆ¶
        clipped_language_embs = (language_embs - self.language_mean).dot(
            Projection_matrix)  # (18357,128)  æŠ•å½±ï¼š(X - mean) @ Projection_matrix
        return clipped_language_embs


# =============================================================================
# SASRec_backboneï¼šSASRec éª¨å¹²ç½‘ç»œåŸºç±»
# è¿™æ˜¯æ‰€æœ‰åºåˆ—æ¨èæ¨¡å‹çš„åŸºç±»ï¼Œå®ç°äº†ï¼š
#   - Transformer ç¼–ç å™¨ç»“æ„
#   - ä¸‰ç§æŸå¤±å‡½æ•°ï¼šCEã€BCEã€InfoNCE
#   - é¢„æµ‹æ¥å£
# å­ç±»åªéœ€å®ç° embed_ID() å’Œ return_item_emb() æ–¹æ³•
# =============================================================================
class SASRec_backbone(nn.Module):
    def __init__(self, device, **key_words):
        """
        åˆå§‹åŒ– SASRec éª¨å¹²ç½‘ç»œ

        æ¶æ„ï¼š
        Input -> Embedding + Position -> Dropout -> LayerNorm ->
        MultiHeadAttention -> FeedForward -> LayerNorm -> Output

        Args:
            device: è®¡ç®—è®¾å¤‡ cuda
            key_words: é…ç½®å‚æ•°å­—å…¸
        """
        super(SASRec_backbone, self).__init__()

        # è¯»å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        data_statis = pd.read_pickle(
            os.path.join(key_words["language_embs_path"], 'data_statis.df'))  # './data/ASO/data_statis.df'   './data/Beauty/data_statis.df'
        self.seq_len = data_statis['seq_size'][0]  # åºåˆ—é•¿åº¦   10   50
        self.item_num = data_statis['item_num'][0]  # ç‰©å“æ•°é‡ï¼ˆpadding_idx = item_numï¼‰18357   12101
        # self.item_embeddings = Item_Embedding("ID", **key_words)
        # self.item_num = item_num
        # self.seq_len = seq_len

        # åŸºæœ¬é…ç½®
        self.dropout = key_words["dropout_rate"]  # Dropout æ¦‚ç‡  0.1
        self.device = device  # è®¡ç®—è®¾å¤‡
        self.ce_loss = nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±
        self.bce_loss = nn.BCEWithLogitsLoss()  # äºŒå…ƒäº¤å‰ç†µæŸå¤±

        # self.language_dim = self.item_embeddings.language_dim
        self.hidden_dim = key_words["hidden_dim"]  # éšè—å±‚ç»´åº¦    128

        # ä½ç½®åµŒå…¥ï¼šå­¦ä¹ åºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„è¡¨ç¤º
        self.positional_embeddings = nn.Embedding(
            num_embeddings=self.seq_len,  # åºåˆ—é•¿åº¦           10           50
            embedding_dim=self.hidden_dim  # ä¸éšè—å±‚ç»´åº¦ç›¸åŒ   128          128
        )  # (10,128)   (50,128)

        # Transformer ç»„ä»¶
        #
        # ä¸æ ‡å‡† Transformer å¯¹æ¯”
        # æ ‡å‡† Transformerï¼ˆ2 å±‚ LNï¼ŒPost-LNï¼‰
        #
        # x â†’ Attention â†’ + â†’ LN â”€â”
        #                    â†‘    â”‚
        #                    â””â”€â”€â”€â”€â”˜ æ®‹å·®
        #
        #   â†’ FFN â†’ + â†’ LN â”€â”
        #              â†‘    â”‚
        #              â””â”€â”€â”€â”€â”˜ æ®‹å·®
        #
        # AlphaFuseï¼ˆ3 å±‚ LNï¼ŒPre-LN + è¾“å‡º LNï¼‰
        # x â†’ LN â†’ Attention â†’ (+) â†’ LN â†’ FFN â†’ (+) â†’ mask â†’ LN â†’ è¾“å‡º
        #     â†‘                       â†‘                       â†‘
        #    ln_1                    ln_2                    ln_3
        #
        # AlphaFuse å¤šäº†ä¸€ä¸ª ln_3 åœ¨æœ€åè¾“å‡ºæ—¶ä½¿ç”¨
        self.emb_dropout = nn.Dropout(self.dropout)  # åµŒå…¥å±‚ Dropout      0.1
        self.ln_1 = nn.LayerNorm(self.hidden_dim)  # æ³¨æ„åŠ›å‰çš„ LayerNorm
        self.ln_2 = nn.LayerNorm(self.hidden_dim)  # FFN å‰çš„ LayerNorm
        self.ln_3 = nn.LayerNorm(self.hidden_dim)  # è¾“å‡ºå‰çš„ LayerNorm
        # å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼ˆå¸¦å› æœæ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯ï¼‰
        self.mh_attn = MultiHeadAttention(self.hidden_dim, self.hidden_dim, key_words["num_heads"], self.dropout)
        # å‰é¦ˆç½‘ç»œå±‚
        self.feed_forward = PositionwiseFeedForward(self.hidden_dim, self.hidden_dim, self.dropout)
        # self.s_fc = nn.Linear(self.hidden_size, self.item_num)
        # self.ac_func = nn.ReLU()

    def embed_ID(self, x):
        """
        è·å–ç‰©å“ ID åµŒå…¥ï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            x: [B, S] æˆ– [B] ç‰©å“ ID åºåˆ—

        Returns:
            embeddings: [B, S, D] æˆ– [B, D] ç‰©å“åµŒå…¥
        """
        # return self.item_embeddings.ID_embeddings(x)
        pass

    def return_item_emb(self, ):
        """
        è¿”å›å…¨é‡ç‰©å“åµŒå…¥çŸ©é˜µï¼ˆæŠ½è±¡æ–¹æ³•ï¼Œå­ç±»å¿…é¡»å®ç°ï¼‰

        Returns:
            item_embs: [item_num+1, D] æ‰€æœ‰ç‰©å“çš„åµŒå…¥ï¼ˆåŒ…å« paddingï¼‰
        """
        # return self.item_embeddings.ID_embeddings.weight
        pass

    # è°ƒç”¨ class SASRec_backbone ä¸­çš„ forward()
    # è®­ç»ƒï¼š
    # train_loader â†’ calculate_infonce_loss() â†’ forward()
    #
    # æ¨ç†/è¯„ä¼°ï¼š
    # val_loader â†’ evaluate() â†’ model.predict() â†’ forward()
    def forward(self, sequences):
        """
        å‰å‘ä¼ æ’­ï¼šåºåˆ—ç¼–ç 

        æµç¨‹ï¼š
        1. ç‰©å“åµŒå…¥ + ä½ç½®åµŒå…¥
        2. Dropout
        3. Padding æ©ç 
        4. LayerNorm -> MultiHeadAttentionï¼ˆå¸¦å› æœæ©ç ï¼‰
        5. LayerNorm -> FeedForward
        6. å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºç”¨æˆ·è¡¨ç¤º

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—ï¼ˆç‰©å“ IDï¼‰

        Returns:
            logits: [B, D] ç”¨æˆ·è¡¨ç¤ºï¼ˆæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼‰
        """
        # ç‰©å“åµŒå…¥  æ³¨æ„ï¼šè¿™è¾¹çš„å64ç»´æ˜¯èåˆåçš„
        inputs_emb = self.embed_ID(sequences)  # sequencesï¼š(256,10) â€”â€”> inputs_embï¼š(256,10,128)
        # ä½ç½®åµŒå…¥
        inputs_emb += self.positional_embeddings(torch.arange(self.seq_len).to(self.device))  # (256,10) â€”â€”>(256,10,128)
        # BSARec å’Œ AlphaFuse ä¸¤è€…çš„åŒºåˆ«æ˜¯ï¼š
        #
        # BSARecï¼šå…ˆ LayerNormï¼Œå† Dropout
        # AlphaFuseï¼šç›´æ¥ Dropoutï¼ˆæ²¡æœ‰ LayerNormï¼‰
        seq = self.emb_dropout(inputs_emb)  # 0.1   (256,10,128)

        # Padding æ©ç ï¼šæ ¹æ® padding IDï¼ˆself.item_numï¼‰ä¸ºæ¯ä¸ªåºåˆ—ä½ç½®ç”Ÿæˆä¸€ä¸ª 0/1 æ©ç 

        # ä¸¾ä¸ªå°ä¾‹å­ï¼ˆç”¨å° batch æ›´å¥½ç†è§£ï¼‰ï¼š
        # 1.sequences =
        # tensor([[1, 2, 3, 5],
        #         [4, 6, 7, 5]])
        #
        # self.item_num = 5  # çº¦å®š 5 æ˜¯ padding ID
        #
        # åˆ™ï¼š
        #
        # torch.ne(sequences, 5)
        # = tensor([[ True,  True,  True, False],
        #           [ True,  True,  True, False]])
        #
        #
        # True ä»£è¡¨â€œçœŸå® itemï¼ˆé paddingï¼‰â€ï¼ŒFalse ä»£è¡¨â€œpadding ä½ç½®â€
        # æ­¤æ—¶ mask å¸ƒå°”å¼ é‡çš„å½¢çŠ¶ï¼š(256, 10)ï¼Œdtype æ˜¯ torch.bool
        #
        # 2.float()
        # ... .float()
        # æŠŠå¸ƒå°”å€¼è½¬æˆæµ®ç‚¹æ•°ï¼š
        # True â†’ 1.0
        # False â†’ 0.0
        #
        # 3.unsqueeze(-1)
        # ... .unsqueeze(-1)
        #
        # åœ¨æœ€åä¸€ç»´åŠ ä¸€ä¸ªç»´åº¦ï¼Œç›¸å½“äºï¼š
        # åŸæ¥ï¼š(batch_size, seq_len) â†’ (batch_size, seq_len, 1)
        # å³ï¼š(256, 10) â†’ (256, 10, 1)
        mask = torch.ne(sequences, self.item_num).float().unsqueeze(-1).to(self.device)  # (256,10,1)
        # é padding ä½ç½®ï¼šembedding * 1 = ä¿ç•™
        # padding ä½ç½®ï¼šembedding * 0 = ç½®é›¶
        #
        #
        # ä¸ºä»€ä¹ˆæ³¨æ„åŠ›æ©ç ä¸å¤Ÿï¼Ÿ
        # ä½ å¯èƒ½ä¼šé—®ï¼šæ³¨æ„åŠ›å±‚é‡Œå·²ç»æœ‰ Key Masking äº†ï¼Œä¸ºä»€ä¹ˆè¿˜è¦åœ¨è¿™é‡Œç½®é›¶ï¼Ÿ
        #
        # åŸå›  1ï¼šLayerNorm çš„å½±å“
        # seq_normalized = self.ln_1(seq)  # LayerNorm ä¼šç”¨åˆ°æ‰€æœ‰ä½ç½®ï¼
        # LayerNorm è®¡ç®—å‡å€¼å’Œæ–¹å·®æ—¶ä¼šåŒ…å« padding ä½ç½®çš„å€¼ï¼š
        #
        # Î¼ = mean(seq)  # å¦‚æœ padding éé›¶ï¼Œä¼šæ±¡æŸ“å‡å€¼
        # Ïƒ = std(seq)   # åŒæ ·ä¼šå—å½±å“
        # å¦‚æœ padding ä¸ç½®é›¶ï¼ŒLayerNorm çš„ç»“æœä¼šå—åˆ°å½±å“ï¼
        #
        # åŸå›  2ï¼šå‰é¦ˆç½‘ç»œçš„å½±å“
        # ff_out = self.feed_forward(...)  # FFN ä¹Ÿä¼šå¤„ç†æ‰€æœ‰ä½ç½®
        # FFN æ˜¯é€ä½ç½®æ“ä½œï¼Œå¦‚æœ padding ä½ç½®æœ‰éé›¶å€¼ï¼Œä¼šäº§ç”Ÿéé›¶è¾“å‡ºã€‚
        #
        # åŸå›  3ï¼šæ®‹å·®è¿æ¥
        # output = output + seq  # æ®‹å·®è¿æ¥ä¼šæŠŠ padding çš„å€¼å¸¦åˆ°è¾“å‡º
        # BSARec ä¹Ÿæœ‰ç±»ä¼¼æ“ä½œå—ï¼Ÿ
        # æ²¡æœ‰æ˜¾å¼çš„ seq *= maskï¼Œä½† BSARec é€šè¿‡å…¶ä»–æ–¹å¼å¤„ç†ï¼š
        #
        # BSARec åœ¨ nn.Embedding ä¸­è®¾ç½®äº† padding_idx=0
        # self.item_embeddings = nn.Embedding(..., padding_idx=0)
        # è¿™ä¿è¯ id=0 çš„åµŒå…¥å‘é‡å…¨æ˜¯ 0 ä¸”ä¸æ›´æ–°
        # ä½† AlphaFuse çš„ padding_idx = item_numï¼ˆæœ€åä¸€ä¸ªä½ç½®ï¼‰ï¼Œæ‰€ä»¥éœ€è¦æ˜¾å¼ç½®é›¶
        seq *= mask  # (256,10,128)

        # Transformer ç¼–ç 
        #
        # ========== Pre-LN ç»“æ„ ==========
        seq_normalized = self.ln_1(seq)  # (256,10,128)                                         1. å…ˆåš LayerNorm
        mh_attn_out = self.mh_attn(seq_normalized, seq)  # å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆå¸¦å› æœæ©ç ï¼‰ (256,10,128)  2. å†åš Attention
        ff_out = self.feed_forward(self.ln_2(mh_attn_out))  # å‰é¦ˆç½‘ç»œ    (256,10,128)           3. FFN ä¹Ÿæ˜¯ Pre-LN
        # ä¸ºä»€ä¹ˆ FFN åè¦å†æ¬¡ ff_out *= maskï¼Ÿ
        # å› ä¸ºç»è¿‡å¤šå¤´æ³¨æ„åŠ›å’Œ FFN åï¼Œpadding ä½ç½®å¯èƒ½å†æ¬¡å˜æˆéé›¶ï¼
        #
        # å¯¹æ¯” BSARec
        # BSARec æ²¡æœ‰æ˜¾å¼çš„ ff_out *= maskï¼Œä½†å®ƒä¹Ÿèƒ½æ­£å¸¸å·¥ä½œï¼ŒåŸå› ï¼š
        #
        # padding_idx=0ï¼šåµŒå…¥æœ¬èº«å°±æ˜¯ 0 å‘é‡
        # ä¸åŒçš„æ®‹å·®ç»“æ„ï¼šBSARec çš„ Transformer å®ç°æ–¹å¼ç•¥æœ‰ä¸åŒ
        # æ©ç è®¾è®¡ï¼šBSARec çš„åŠ æ€§æ©ç åœ¨ softmax å‰å·²ç»å¤„ç†å¥½äº†
        #
        #
        # å› ä¸ºBSARec padding_idx=0ï¼šåµŒå…¥æœ¬èº«å°±æ˜¯ 0 å‘é‡ï¼ŒAlphaFuse æ˜¯Item_num å—
        # æ˜¯çš„ï¼Œä½ ç†è§£æ­£ç¡®ï¼
        #
        # å¯¹æ¯”
        # é¡¹ç›®	        padding_idx	        padding         å€¼	                    ç‰¹ç‚¹
        # BSARec	    0	                åºåˆ—ä¸­ç”¨         0 å¡«å……	            æœ€å‰é¢çš„ ID æ˜¯ padding
        # AlphaFuse	    item_num	        åºåˆ—ä¸­ç”¨        item_num å¡«å……	        æœ€åé¢çš„ ID æ˜¯ padding
        # AlphaFuse çš„ä»£ç è¯æ®
        # backbone_SASRec.py
        # class Item_Embedding(nn.Module):
        #     def __init__(self, ...):
        #         data_statis = pd.read_pickle(...)
        #         self.item_num = data_statis['item_num'][0]  # ä¾‹å¦‚ 18357
        #
        #     # AF æ¨¡å¼
        #     self.language_embeddings = nn.Embedding.from_pretrained(
        #         ...,
        #         freeze=True,
        #         padding_idx=self.item_num  # â† padding_idx = 18357
        #     )
        #
        #     self.ID_embeddings = nn.Embedding(
        #         num_embeddings=self.item_num + 1,  # 18358 ä¸ªåµŒå…¥
        #         embedding_dim=...,
        #         # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰è®¾ç½® padding_idxï¼
        #     )
        # AlphaFuse çš„ ID èŒƒå›´ï¼š
        #
        # ç‰©å“ ID: 0, 1, 2, ..., 18356  (å…± 18357 ä¸ªç‰©å“)
        # padding: 18357                (æœ€åä¸€ä¸ªä½ç½®)
        # è¿™å¯¼è‡´äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
        # BSARecï¼ˆpadding_idx=0ï¼‰
        # self.item_embeddings = nn.Embedding(..., padding_idx=0)
        # # ID=0 çš„åµŒå…¥å‘é‡ï¼š
        # # - åˆå§‹åŒ–ä¸º 0
        # # - è®­ç»ƒæ—¶æ¢¯åº¦ä¸æ›´æ–°
        # # - å§‹ç»ˆä¿æŒä¸º 0
        #  å¤©ç„¶å°±æ˜¯ 0 å‘é‡ï¼Œä¸éœ€è¦é¢å¤–å¤„ç†
        #
        # AlphaFuseï¼ˆpadding_idx=item_numï¼‰
        # self.language_embeddings: padding_idx=item_num  # â† æœ‰è®¾ç½®
        # self.ID_embeddings: æ²¡æœ‰è®¾ç½® padding_idxï¼      # â† é—®é¢˜åœ¨è¿™é‡Œï¼
        #  ID_embeddings çš„ padding ä½ç½®å¯èƒ½ä¸æ˜¯ 0
        #
        # è€Œä¸”ä½ç½®ç¼–ç ä¹Ÿä¼šåŠ ä¸Šå»ï¼š
        #
        # python
        # inputs_emb = self.embed_ID(sequences)               # padding å¯èƒ½éé›¶
        # inputs_emb += self.positional_embeddings(...)       # åŠ ä¸Šä½ç½®ç¼–ç 
        # # ç°åœ¨ padding ä½ç½®è‚¯å®šéé›¶äº†ï¼
        # æ‰€ä»¥å¿…é¡»æ˜¾å¼ seq *= mask ç½®é›¶ï¼
        #
        # æ€»ç»“
        # é—®é¢˜	                BSARec	                AlphaFuse
        # padding ä½ç½®	        ID=0ï¼ˆæœ€å‰é¢ï¼‰	        ID=item_numï¼ˆæœ€åé¢ï¼‰
        # åµŒå…¥æ˜¯å¦ä¸º 0	         æ˜¯ï¼ˆpadding_idx=0ï¼‰	 ä¸ä¸€å®šï¼ˆID_embeddings æ²¡è®¾ padding_idxï¼‰
        # éœ€è¦æ‰‹åŠ¨ç½®é›¶å—ï¼Ÿ	     ä¸éœ€è¦	             éœ€è¦ seq *= mask
        # è¿™å°±æ˜¯ AlphaFuse ä»£ç æ›´å¤æ‚çš„åŸå› ä¹‹ä¸€ï¼
        ff_out *= mask  # å†æ¬¡åº”ç”¨æ©ç   (256,10,128)
        ff_out = self.ln_3(ff_out)  # (256,10,128)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºç”¨æˆ·è¡¨ç¤º
        logits = ff_out[:, -1].squeeze()  # [B, D]  (256,128)
        return logits

    def predict(self, sequences):
        """
        é¢„æµ‹ï¼šè®¡ç®—ç”¨æˆ·å¯¹æ‰€æœ‰ç‰©å“çš„å¾—åˆ†

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—

        Returns:
            scores: [B, item_num] ç”¨æˆ·å¯¹æ¯ä¸ªç‰©å“çš„é¢„æµ‹å¾—åˆ†
        """
        # inputs_emb = self.item_embeddings(states) * self.item_embeddings.embedding_dim ** 0.5
        state_hidden = self.forward(sequences)  # [B, D] ç”¨æˆ·è¡¨ç¤º           (256,128)  è°ƒç”¨ Class SASRec_backbone çš„forward()
        item_embs = self.return_item_emb()  # è°ƒç”¨ class AlphaFuse(SASRec_backbone) çš„ def return_item_emb(self,)    [item_num+1, D] ç‰©å“åµŒå…¥  (18358,128)
        # ä¸ºä»€ä¹ˆå»æ‰ padding
        #
        # ä¸€å¥è¯æ€»ç»“ï¼šå› ä¸º LLM ä¾§çš„ padding åµŒå…¥æ˜¯éšæœºåˆå§‹åŒ–çš„éé›¶å‘é‡ï¼Œå¯èƒ½è¢« Top-K é€‰ä¸­
        #
        # è¯¦ç»†åŸå› 
        # 1. LLM ä¾§çš„ padding åµŒå…¥ä¸æ˜¯é›¶å‘é‡
        # ä»£ç ï¼šbackbone_SASRec.py
        # padding_emb = np.random.rand(128)  # â† éšæœºåˆå§‹åŒ–ï¼ä¸æ˜¯ 0ï¼
        # cliped_language_embs = np.vstack([cliped_language_embs, padding_emb])
        #
        # 2. é¢„æµ‹æ—¶ä¼šè®¡ç®— padding çš„å¾—åˆ†
        # scores = torch.matmul(user_state, item_embs.transpose(0, 1))
        # user_state Â· padding_emb â‰  0  â† å¯èƒ½å¾—åˆ†è¾ƒé«˜ï¼
        #
        # 3. Top-K å¯èƒ½é€‰ä¸­ padding
        # _, topK = scores.topk(100, largest=True)
        # topK å¯èƒ½åŒ…å« 18357ï¼ˆpaddingï¼‰
        #
        # 4. å¯¼è‡´è¯„ä¼°æŒ‡æ ‡é”™è¯¯
        # é¢„æµ‹ï¼š[ç‰©å“5, padding, ç‰©å“3, ...]  â† é”™è¯¯çš„æ¨èï¼
        #
        #
        # ä¸ BSARec å¯¹æ¯”
        # é¡¹ç›®	        padding åµŒå…¥	            éœ€è¦æ‰‹åŠ¨å»æ‰å—ï¼Ÿ
        # BSARec	    0 å‘é‡	                 ä¸éœ€è¦ï¼ˆå¾—åˆ†=0ï¼Œä¸ä¼šè¢«é€‰ä¸­ï¼‰
        # AlphaFuse	    éšæœºéé›¶å‘é‡	             å¿…é¡»å»æ‰
        scores = torch.matmul(state_hidden, item_embs[:-1].transpose(0, 1))  # [B, item_num]ï¼ˆå»æ‰ paddingï¼‰ (256,18357)
        return scores

    def calculate_ce_loss(self, sequences, target):
        """
        è®¡ç®— Cross-Entropy æŸå¤±ï¼ˆå…¨ç‰©å“ softmaxï¼‰

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—
            target: [B] ç›®æ ‡ç‰©å“ ID

        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        seq_output = self.forward(sequences)  # [B, D]
        item_embs = self.return_item_emb()  # [item_num+1, D]
        # item_embs = self.item_emb.return_embs()
        logits = torch.matmul(seq_output, item_embs[:-1].transpose(0, 1))  # [B, item_num]
        loss = self.ce_loss(logits, target)
        return loss

    def calculate_bce_loss(self, sequences, target, neg_ratio, emb_type="both"):
        """
        è®¡ç®— Binary Cross-Entropy æŸå¤±ï¼ˆè´Ÿé‡‡æ ·äºŒåˆ†ç±»ï¼‰

        æ€è·¯ï¼š
        - æ­£æ ·æœ¬ï¼šç›®æ ‡ç‰©å“ï¼Œæ ‡ç­¾ä¸º 1
        - è´Ÿæ ·æœ¬ï¼šéšæœºé‡‡æ · neg_ratio ä¸ªç‰©å“ï¼Œæ ‡ç­¾ä¸º 0
        - äºŒåˆ†ç±»ï¼šsigmoid(user Â· item) â†’ 0/1

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—
            target: [B] æ­£æ ·æœ¬ç‰©å“ ID
            neg_ratio: è´Ÿé‡‡æ ·æ•°é‡

        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # ==================== è´Ÿé‡‡æ · ====================
        # éšæœºé‡‡æ ·è´Ÿæ ·æœ¬ï¼Œç¡®ä¿ä¸ä¸æ­£æ ·æœ¬é‡å¤
        # sequences_set = set(sequences.view(-1).tolist())
        batch_size = target.shape[0]
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()
        # expanded_sequences = sequences.view(batch_size, -1, 1).expand(batch_size, sequences.shape[1], neg_ratio).cpu()
        # mask_target = neg_samples == expanded_target
        # mask_sequences = (neg_samples.unsqueeze(1).expand(-1, sequences.shape[1], -1) == expanded_sequences).any(dim=1)
        # mask = mask_target | mask_sequences
        mask = neg_samples == expanded_target
        # é‡é‡‡æ ·ä¸æ­£æ ·æœ¬é‡å¤çš„è´Ÿæ ·æœ¬
        while mask.any():
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
            neg_samples = torch.where(mask, new_samples, neg_samples)
            mask = neg_samples == expanded_target
            # mask_target = neg_samples == expanded_target
            # mask_sequences = (neg_samples.unsqueeze(1).expand(-1, sequences.shape[1], -1) == expanded_sequences).any(dim=1)
            # mask = mask_target | mask_sequences
        target_neg = neg_samples.to(target.device)

        # ==================== è®¡ç®—å¾—åˆ† ====================
        # pos_embs = self.item_embeddings(target)
        pos_embs = self.embed_ID(target)  # [B, D] æ­£æ ·æœ¬åµŒå…¥
        neg_embs = self.embed_ID(target_neg)  # [B, neg_ratio, D] è´Ÿæ ·æœ¬åµŒå…¥

        log_feats = self.forward(sequences)  # [B, D] ç”¨æˆ·è¡¨ç¤º

        # ç‚¹ç§¯å¾—åˆ†
        pos_logits = (log_feats * pos_embs).sum(dim=-1)  # [B]
        neg_logits = (log_feats.unsqueeze(1) * neg_embs).sum(dim=-1)  # [B, neg_ratio]

        # ==================== BCE æŸå¤± ====================
        pos_labels, neg_labels = torch.ones(pos_logits.shape, device=self.device), torch.zeros(neg_logits.shape,
                                                                                               device=self.device)
        loss = self.bce_loss(pos_logits, pos_labels)  # æ­£æ ·æœ¬æŸå¤±
        loss += self.bce_loss(neg_logits, neg_labels)  # è´Ÿæ ·æœ¬æŸå¤±

        return loss

    # def calculate_infonce_loss  æœ€åæ˜¯æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œå¯¹åº”ä¸€ä¸ªæ­£æ ·æœ¬ï¼Œ64ä¸ªè´Ÿæ ·æœ¬
    def calculate_infonce_loss(self, sequences, target, neg_ratio, temperature, emb_type="both"):
        """
        è®¡ç®— InfoNCE æŸå¤±ï¼ˆå¯¹æ¯”å­¦ä¹ æŸå¤±ï¼‰

        è¿™æ˜¯æ¨èç³»ç»Ÿä¸­æœ€å¸¸ç”¨çš„æŸå¤±å‡½æ•°ï¼Œæ ¸å¿ƒæ€æƒ³ï¼š
        - æ­£æ ·æœ¬å¯¹ (user, positive_item) åº”è¯¥ç›¸ä¼¼
        - è´Ÿæ ·æœ¬å¯¹ (user, negative_items) åº”è¯¥ä¸ç›¸ä¼¼
        - ä½¿ç”¨ softmax å½’ä¸€åŒ–ï¼Œæ­£æ ·æœ¬æ¦‚ç‡åº”è¯¥æœ€å¤§

        å…¬å¼ï¼š
        L = -log( exp(sim(u, i+)/Ï„) / Î£ exp(sim(u, i)/Ï„) )

        å…¶ä¸­ Ï„ æ˜¯æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„é”åº¦

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—
            target: [B] æ­£æ ·æœ¬ç‰©å“ ID
            neg_ratio: è´Ÿé‡‡æ ·æ•°é‡
            temperature: æ¸©åº¦å‚æ•°ï¼ˆé€šå¸¸ 0.07ï¼‰

        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """
        # ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šè´Ÿé‡‡æ · ====================
        # è´Ÿé‡‡æ ·çš„ç›®çš„ï¼šä»å…¨ä½“ç‰©å“ä¸­éšæœºé€‰æ‹©ä¸€äº›ç‰©å“ä½œä¸º"è´Ÿæ ·æœ¬"
        # è´Ÿæ ·æœ¬åº”è¯¥æ˜¯ç”¨æˆ·æ²¡æœ‰äº¤äº’è¿‡çš„ç‰©å“ï¼Œç”¨äºå¯¹æ¯”å­¦ä¹ 

        # sequences_set = set(sequences.view(-1).tolist())  # ï¼ˆå·²æ³¨é‡Šï¼‰å°†åºåˆ—ä¸­çš„ç‰©å“ ID è½¬ä¸ºé›†åˆï¼Œç”¨äºå»é‡

        batch_size = target.shape[0]  # è·å– batch å¤§å°ï¼Œä¾‹å¦‚ 256

        # ä» [0, item_num) èŒƒå›´å†…éšæœºé‡‡æ · neg_ratio ä¸ªè´Ÿæ ·æœ¬
        # å½¢çŠ¶ï¼š[B, neg_ratio]ï¼Œä¾‹å¦‚ [256, 64]ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ 64 ä¸ªè´Ÿæ ·æœ¬
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))  # (256, 64)

        # å°†æ­£æ ·æœ¬ target æ‰©å±•ä¸º [B, neg_ratio]ï¼Œç”¨äºåç»­ä¸è´Ÿæ ·æœ¬æ¯”è¾ƒ
        # target: [256,] â†’ [256, 1] â†’ [256, 64]ï¼ˆæ¯è¡Œéƒ½æ˜¯ç›¸åŒçš„æ­£æ ·æœ¬ IDï¼‰
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()  # (256, 64)

        # å°†åºåˆ—æ‰©å±•ä¸º [B, S, neg_ratio]ï¼Œç”¨äºæ£€æŸ¥è´Ÿæ ·æœ¬æ˜¯å¦åœ¨ç”¨æˆ·å†å²ä¸­ï¼ˆå·²æ³¨é‡Šæ‰ï¼‰
        expanded_sequences = sequences.view(batch_size, -1, 1).expand(batch_size, sequences.shape[1], neg_ratio).cpu()

        # mask_target = neg_samples == expanded_target           # ï¼ˆå·²æ³¨é‡Šï¼‰æ£€æŸ¥è´Ÿæ ·æœ¬æ˜¯å¦ä¸æ­£æ ·æœ¬ç›¸åŒ
        # mask_sequences = (neg_samples.unsqueeze(1).expand(-1, sequences.shape[1], -1) == expanded_sequences).any(dim=1)  # ï¼ˆå·²æ³¨é‡Šï¼‰æ£€æŸ¥è´Ÿæ ·æœ¬æ˜¯å¦åœ¨åºåˆ—ä¸­
        # mask = mask_target | mask_sequences                    # ï¼ˆå·²æ³¨é‡Šï¼‰åˆå¹¶ä¸¤ä¸ªæ©ç 

        # ç”Ÿæˆæ©ç ï¼šæ ‡è®°å“ªäº›è´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬ç›¸åŒï¼ˆéœ€è¦é‡æ–°é‡‡æ ·ï¼‰
        # mask[i, j] = True è¡¨ç¤ºç¬¬ i ä¸ªæ ·æœ¬çš„ç¬¬ j ä¸ªè´Ÿæ ·æœ¬ä¸æ­£æ ·æœ¬ç›¸åŒ
        #
        # neg_samples å½¢çŠ¶æ˜¯ [B, neg_ratio]ï¼Œexpanded_target å½¢çŠ¶ä¹Ÿæ˜¯ [B, neg_ratio]
        # åšæ¯”è¾ƒ == ä¹‹åï¼Œmask ä¹Ÿæ˜¯ [B, neg_ratio] çš„ å¸ƒå°”å¼ é‡ï¼š
        #
        # mask[b, j] == True è¡¨ç¤ºï¼š
        # ç¬¬ b ä¸ªæ ·æœ¬çš„ç¬¬ j ä¸ªè´Ÿæ ·æœ¬ åˆšå¥½ç­‰äº æ­£æ ·æœ¬ ID â†’ è¿™å…¶å®æ˜¯â€œå‡è´Ÿæ ·æœ¬â€ï¼Œéœ€è¦é‡é‡‡æ ·
        #
        # mask[b, j] == False è¡¨ç¤ºï¼š
        # è¿™ä¸ªè´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬ä¸ä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªâ€œåˆæ ¼çš„è´Ÿæ ·æœ¬â€
        mask = neg_samples == expanded_target  # [256, 64] å¸ƒå°”å¼ é‡

        # é‡é‡‡æ ·ä¸æ­£æ ·æœ¬é‡å¤çš„è´Ÿæ ·æœ¬ï¼ˆå¾ªç¯ç›´åˆ°æ²¡æœ‰é‡å¤ï¼‰
        #
        # å¯¹ä¸€ä¸ªå¸ƒå°”å¼ é‡è°ƒç”¨ .any() æ—¶ï¼Œä¼šåœ¨æ‰€æœ‰ç»´åº¦ä¸Šåšé€»è¾‘æˆ–ï¼š
        #
        # ä¹Ÿå°±æ˜¯ï¼š
        # åªè¦ mask é‡Œ æœ‰ä»»æ„ä¸€ä¸ªä½ç½®æ˜¯ Trueï¼Œmask.any() å°±ä¼šè¿”å› Trueï¼›
        # å¦‚æœ mask é‡Œ å…¨éƒ¨éƒ½æ˜¯ Falseï¼Œmask.any() æ‰ä¼šè¿”å› Falseã€‚
        #
        # æ‰€ä»¥ï¼š
        # mask.any() == True â†’ è¯´æ˜å½“å‰è¿˜æœ‰è‡³å°‘ä¸€ä¸ªè´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬é‡å¤ï¼›
        # mask.any() == False â†’ æ‰€æœ‰è´Ÿæ ·æœ¬éƒ½å·²ç»å’Œæ­£æ ·æœ¬ä¸é‡å¤äº†
        while mask.any():  # åªè¦è¿˜æœ‰é‡å¤çš„è´Ÿæ ·æœ¬
            # é‡æ–°é‡‡æ · [B, neg_ratio] ä¸ªæ–°çš„éšæœºç‰©å“
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
            # åªæ›¿æ¢é‡å¤çš„ä½ç½®ï¼šmask=True çš„ä½ç½®ç”¨ new_samplesï¼Œå¦åˆ™ä¿ç•™ neg_samples
            neg_samples = torch.where(mask, new_samples, neg_samples)
            # é‡æ–°æ£€æŸ¥æ˜¯å¦è¿˜æœ‰é‡å¤
            mask = neg_samples == expanded_target
            # mask_target = neg_samples == expanded_target
            # mask_sequences = (neg_samples.unsqueeze(1).expand(-1, sequences.shape[1], -1) == expanded_sequences).any(dim=1)
            # mask = mask_target | mask_sequences

        # å°†è´Ÿæ ·æœ¬ç§»åŠ¨åˆ°ä¸ target ç›¸åŒçš„è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        target_neg = neg_samples.to(
            target.device)  # è´Ÿæ ·æœ¬ [256, 64]ï¼Œbatch é‡Œæœ‰ 256 ä¸ªæ ·æœ¬ï¼ˆ256 ä¸ªç”¨æˆ· / åºåˆ—ï¼‰ï¼Œç»™æ¯ä¸ªæ ·æœ¬é‡‡äº† 64 ä¸ª è´Ÿæ ·æœ¬ item çš„ ID

        # ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šè·å–åµŒå…¥å‘é‡ ====================

        # pos_embs = self.item_embeddings(target)  # ï¼ˆå·²æ³¨é‡Šï¼‰ç›´æ¥ä»åµŒå…¥è¡¨æŸ¥æ‰¾ï¼Œä¸ç»è¿‡æ¨¡å‹å¤„ç†

        # è·å–æ­£æ ·æœ¬çš„åµŒå…¥å‘é‡
        # embed_ID ä¼šæ ¹æ®æ¨¡å‹ç±»å‹ï¼ˆå¦‚ AlphaFuseï¼‰è¿›è¡Œç›¸åº”çš„åµŒå…¥èåˆ
        pos_embs = self.embed_ID(target)  # [B, D] = [256, 128] æ­£æ ·æœ¬åµŒå…¥

        # è·å–è´Ÿæ ·æœ¬çš„åµŒå…¥å‘é‡
        # target_neg: [B, neg_ratio] â†’ neg_embs: [B, neg_ratio, D]
        neg_embs = self.embed_ID(target_neg)  # [B, neg_ratio, D] = [256, 64, 128] è´Ÿæ ·æœ¬åµŒå…¥

        # å‰å‘ä¼ æ’­ï¼Œè·å–ç”¨æˆ·è¡¨ç¤ºï¼ˆåºåˆ—ç¼–ç åçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        log_feats = self.forward(sequences)  # [B, D] = [256, 128] ç”¨æˆ·è¡¨ç¤º   è°ƒç”¨ Class SASRec_backbone çš„forward()

        # ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šL2 å½’ä¸€åŒ–ï¼ˆè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰====================
        # å½’ä¸€åŒ–çš„ç›®çš„ï¼šå°†å‘é‡æŠ•å½±åˆ°å•ä½è¶…çƒé¢ä¸Šï¼Œä½¿ç‚¹ç§¯ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
        # ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´ [-1, 1]ï¼Œä¸å—å‘é‡æ¨¡é•¿å½±å“

        # å¯¹ç”¨æˆ·è¡¨ç¤ºè¿›è¡Œ L2 å½’ä¸€åŒ–ï¼š||log_feats|| = 1
        log_feats = F.normalize(log_feats, p=2, dim=-1)  # [B, D]  (256,128)

        # å¯¹æ­£æ ·æœ¬åµŒå…¥è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼š||pos_embs|| = 1
        pos_embs = F.normalize(pos_embs, p=2, dim=-1)  # [B, D]  (256,128)

        # å¯¹è´Ÿæ ·æœ¬åµŒå…¥è¿›è¡Œ L2 å½’ä¸€åŒ–ï¼š||neg_embs|| = 1
        neg_embs = F.normalize(neg_embs, p=2, dim=-1)  # [B, neg_ratio, D]  (256,64,128)

        # ï¼ˆå·²æ³¨é‡Šï¼‰æ‰‹åŠ¨å®ç° L2 å½’ä¸€åŒ–çš„ç­‰ä»·å†™æ³•
        # normed_log_feats = log_feats / torch.sqrt(1e-8 + log_feats.square().sum(-1, keepdim=True))
        # normed_pos_embs = pos_embs / torch.sqrt(1e-8 + pos_embs.square().sum(-1, keepdim=True))
        # normed_neg_embs = neg_embs / torch.sqrt(1e-8 + neg_embs.square().sum(-1, keepdim=True))

        # ==================== ç¬¬å››éƒ¨åˆ†ï¼šè®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ† ====================

        # è®¡ç®—æ­£æ ·æœ¬å¾—åˆ†ï¼šç”¨æˆ·è¡¨ç¤º Â· æ­£æ ·æœ¬åµŒå…¥ï¼ˆé€å…ƒç´ ä¹˜ç§¯åæ±‚å’Œ = ç‚¹ç§¯ï¼‰
        # log_feats: [B, D], pos_embs: [B, D]
        # é€å…ƒç´ ä¹˜ç§¯: [B, D]ï¼Œç„¶åæ²¿ dim=-1 æ±‚å’Œå¾—åˆ° [B]ï¼Œkeepdim=True ä¿æŒå½¢çŠ¶ä¸º [B, 1]
        pos_logits = (log_feats * pos_embs).sum(dim=-1, keepdim=True)  # [B, 1] (256,1) æ­£æ ·æœ¬å¾—åˆ†

        # è®¡ç®—è´Ÿæ ·æœ¬å¾—åˆ†ï¼šç”¨æˆ·è¡¨ç¤º Â· æ¯ä¸ªè´Ÿæ ·æœ¬åµŒå…¥
        # neg_embs: [B, neg_ratio, D]
        # log_feats.unsqueeze(-1): [B, D, 1]
        # bmm (batch matrix multiply): [B, neg_ratio, D] @ [B, D, 1] = [B, neg_ratio, 1]
        # squeeze(-1): [B, neg_ratio]
        neg_logits = torch.bmm(neg_embs, log_feats.unsqueeze(-1)).squeeze(-1)  # [B, neg_ratio] (256,64) è´Ÿæ ·æœ¬å¾—åˆ†

        # æ‹¼æ¥æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬å¾—åˆ†ï¼šæ­£æ ·æœ¬åœ¨ç¬¬ 0 ä½
        # pos_logits: [B, 1], neg_logits: [B, neg_ratio]
        # logits: [B, 1 + neg_ratio] = [256, 65]
        logits = torch.cat([pos_logits, neg_logits], dim=-1)  # [B, 1+neg_ratio]  (256,65)

        # æ¸©åº¦ç¼©æ”¾ï¼šlogits / Ï„
        # æ¸©åº¦è¶Šå°ï¼Œsoftmax åˆ†å¸ƒè¶Šå°–é”ï¼ˆæ›´è‡ªä¿¡ï¼‰ï¼Œæ¸©åº¦è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå¹³æ»‘
        # å¸¸ç”¨å€¼ï¼šÏ„ = 0.07ï¼ˆCLIPï¼‰ã€Ï„ = 0.1ï¼ˆå¯¹æ¯”å­¦ä¹ ï¼‰
        logits /= temperature  # æ¸©åº¦ç¼©æ”¾  (256,65)

        # ==================== ç¬¬äº”éƒ¨åˆ†ï¼šäº¤å‰ç†µæŸå¤± ====================
        # InfoNCE æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª (1 + neg_ratio) åˆ†ç±»é—®é¢˜
        # æ­£æ ·æœ¬åœ¨ç¬¬ 0 ä½ï¼Œæ‰€ä»¥æ ‡ç­¾å…¨ä¸º 0

        # åˆ›å»ºæ ‡ç­¾ï¼šå…¨ä¸º 0ï¼Œè¡¨ç¤ºæ­£ç¡®ç­”æ¡ˆæ˜¯ç¬¬ 0 ä¸ªä½ç½®ï¼ˆæ­£æ ·æœ¬ï¼‰
        #
        # ä¸ºä»€ä¹ˆå…¨æ˜¯ 0ï¼Ÿ
        # å‰é¢æ„é€  logits æ˜¯è¿™æ ·çš„ï¼š
        # pos_logits = ...                     # [B, 1]   æ­£æ ·æœ¬å¾—åˆ†
        # neg_logits = ...                     # [B, neg_ratio] è´Ÿæ ·æœ¬å¾—åˆ†
        # logits = torch.cat([pos_logits, neg_logits], dim=-1)  # [B, 1 + neg_ratio]
        #
        # å¯¹äºç¬¬ b ä¸ªæ ·æœ¬ï¼Œlogits[b] æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º 1 + neg_ratio çš„å‘é‡ï¼š
        # logits[b, 0]ï¼šæ­£æ ·æœ¬çš„ logit
        # logits[b, 1:]ï¼šneg_ratio ä¸ªè´Ÿæ ·æœ¬çš„ logit
        #
        # F.cross_entropy(logits, labels) çš„è¯­ä¹‰æ˜¯ï¼š
        # å‡è®¾ logits å½¢çŠ¶æ˜¯ [B, C]ï¼ˆC ä¸ªç±»åˆ«ï¼‰
        # labels å½¢çŠ¶æ˜¯ [B]ï¼Œé‡Œé¢å­˜çš„æ˜¯ æ¯ä¸ªæ ·æœ¬çš„â€œæ­£ç¡®ç±»åˆ«ç´¢å¼•â€
        # æ¯”å¦‚ labels[b] = 3ï¼Œæ„æ€æ˜¯ï¼š
        # ç¬¬ b ä¸ªæ ·æœ¬çš„â€œæ­£ç¡®ç±»åˆ«â€æ˜¯ç¬¬ 3 ç±»ï¼ˆç´¢å¼• 3ï¼‰
        #
        # åœ¨ä½ çš„è®¾è®¡é‡Œï¼Œæˆ‘ä»¬çº¦å®šï¼š
        # æ¯ä¸ªæ ·æœ¬çš„ç¬¬ 0 ç»´ï¼ˆlogits[..., 0]ï¼‰æ˜¯æ­£æ ·æœ¬ï¼Œå…¶ä½™éƒ½æ˜¯è´Ÿæ ·æœ¬
        #
        # æ‰€ä»¥æ ‡ç­¾è‡ªç„¶å°±æ˜¯ï¼š
        # labels = [0, 0, 0, 0, ..., 0]  # é•¿åº¦ Bï¼Œæ¯ä¸ªä½ç½®éƒ½æ˜¯ 0
        #
        # å«ä¹‰å°±æ˜¯ï¼š
        # å¯¹äº batch é‡Œçš„æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œ
        # æ­£ç¡®ç±»çš„ index = 0ï¼ˆä¹Ÿå°±æ˜¯æ­£æ ·æœ¬æ‰€åœ¨çš„ä½ç½®
        labels = torch.zeros(batch_size, dtype=torch.long,
                             device=logits.device)  # [B] = [256]  (256,)   ä¸€ä¸ª batch ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ­£æ ·æœ¬åœ¨è¿™ä¸€è¡Œ logits é‡Œçš„ä½ç½®ï¼ˆç´¢å¼•ï¼‰

        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        # F.cross_entropy = softmax + negative log likelihood
        # L = -log( exp(logits[0]) / Î£ exp(logits[i]) )
        #   = -logits[0] + log(Î£ exp(logits[i]))
        loss = F.cross_entropy(logits, labels)

        return loss  # è¿”å›æ ‡é‡æŸå¤±å€¼


# =============================================================================
# SASRecï¼šçº¯ ID åµŒå…¥çš„åºåˆ—æ¨èæ¨¡å‹ï¼ˆåŸºçº¿ï¼‰
# è¿™æ˜¯æœ€åŸºæœ¬çš„ SASRec å®ç°ï¼Œåªä½¿ç”¨éšæœºåˆå§‹åŒ–çš„ ID åµŒå…¥
# =============================================================================
class SASRec(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨çº¯ ID åµŒå…¥ç­–ç•¥
        self.item_embeddings = Item_Embedding("ID", **key_words)

    def embed_ID(self, x):
        """è·å–ç‰©å“ ID åµŒå…¥"""
        return self.item_embeddings.ID_embeddings(x)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        return self.item_embeddings.ID_embeddings.weight

    # =============================================================================


# MoRecï¼šè¯­è¨€åµŒå…¥ + MLP é€‚é…å™¨
# ç›´æ¥ä½¿ç”¨ LLM è¯­è¨€åµŒå…¥ï¼Œé€šè¿‡ MLP æŠ•å½±åˆ°éšè—ç©ºé—´
# =============================================================================
class MoRec(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨è‡ªé€‚åº”æŠ•å½±ç­–ç•¥ï¼ˆåªæœ‰è¯­è¨€åµŒå…¥ï¼‰
        self.item_embeddings = Item_Embedding("AP", **key_words)
        self.language_dim = self.item_embeddings.language_dim
        # MLP é€‚é…å™¨ï¼šè¯­è¨€ç»´åº¦ -> éšè—ç»´åº¦
        self.adapter = nn.Sequential(
            nn.Linear(self.language_dim, key_words['hidden_dim']),
            nn.GELU()  # GELU æ¿€æ´»å‡½æ•°
        )

    def embed_ID(self, x):
        """è·å–ç‰©å“åµŒå…¥ï¼šè¯­è¨€åµŒå…¥ -> MLP é€‚é…å™¨"""
        language_embs = self.item_embeddings.language_embeddings(x)
        return self.adapter(language_embs)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        language_embs = self.item_embeddings.language_embeddings.weight
        return self.adapter(language_embs)


# =============================================================================
# WhitenRecï¼šç™½åŒ–è¯­è¨€åµŒå…¥ + MLP é€‚é…å™¨
# ä¸ MoRec ç±»ä¼¼ï¼Œä½†å…ˆå¯¹è¯­è¨€åµŒå…¥è¿›è¡Œ PCA ç™½åŒ–å¤„ç†
# ç™½åŒ–æ¶ˆé™¤äº†å„ç»´åº¦ä¹‹é—´çš„ç›¸å…³æ€§
# =============================================================================
class WhitenRec(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨ç™½åŒ–è‡ªé€‚åº”æŠ•å½±ç­–ç•¥
        self.item_embeddings = Item_Embedding("WAP", **key_words)
        self.language_dim = self.item_embeddings.language_dim
        # MLP é€‚é…å™¨ï¼šç™½åŒ–åç»´åº¦ -> éšè—ç»´åº¦
        self.adapter = nn.Sequential(
            nn.Linear(self.language_dim, key_words['hidden_dim']),
            nn.GELU()
        )

    def embed_ID(self, x):
        """è·å–ç‰©å“åµŒå…¥ï¼šç™½åŒ–è¯­è¨€åµŒå…¥ -> MLP é€‚é…å™¨"""
        language_embs = self.item_embeddings.language_embeddings(x)
        return self.adapter(language_embs)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        language_embs = self.item_embeddings.language_embeddings.weight
        return self.adapter(language_embs)

    # =============================================================================


# LLMInitï¼šè¯­ä¹‰åˆå§‹åŒ–
# ç”¨ LLM è¯­è¨€åµŒå…¥åˆå§‹åŒ– ID åµŒå…¥ï¼Œç„¶åå¾®è°ƒ
# ç›¸å½“äºç”¨è¯­ä¹‰ä¿¡æ¯è¿›è¡Œé¢„è®­ç»ƒ
# =============================================================================
class LLMInit(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨è¯­ä¹‰åˆå§‹åŒ–ç­–ç•¥ï¼ˆID åµŒå…¥ç”¨è¯­è¨€åµŒå…¥åˆå§‹åŒ–ï¼Œå¯å¾®è°ƒï¼‰
        self.item_embeddings = Item_Embedding("SI", **key_words)
        # self.language_dim = self.item_embeddings.language_dim

    def embed_ID(self, x):
        """è·å–ç‰©å“ ID åµŒå…¥ï¼ˆå·²ç”¨è¯­è¨€åµŒå…¥åˆå§‹åŒ–ï¼‰"""
        return self.item_embeddings.ID_embeddings(x)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        return self.item_embeddings.ID_embeddings.weight

    # =============================================================================


# =============================================================================
# RLMRecï¼šè¯­ä¹‰é‡å»º
# =============================================================================
# ä½¿ç”¨é‡å»ºæŸå¤±å¯¹é½ ID åµŒå…¥å’Œè¯­è¨€åµŒå…¥
#
# ã€ç‰©å“ä¾§å¯¹é½ã€‘ï¼ˆåŸæœ‰ï¼‰
#   ä¸¤ç§å¯¹é½æ–¹å¼ï¼š
#   - con (contrastive): è¯­è¨€åµŒå…¥ -> ID åµŒå…¥ï¼ˆå¯¹æ¯”å¼ï¼‰
#   - gen (generative): ID åµŒå…¥ -> è¯­è¨€åµŒå…¥ï¼ˆç”Ÿæˆå¼ï¼‰
#
# ã€ç”¨æˆ·ä¾§å¯¹é½ã€‘ï¼ˆNEW 2024-12-15ï¼‰
#   ä½¿ç”¨ usr_intent_emb.pklï¼ˆç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥ï¼Œå½¢çŠ¶ [N_users, 3072]ï¼‰
#   
#   æ•°æ®æµï¼š
#     usr_intent_emb[user_id]  â†’  MLP æ˜ å°„  â†’  ç”¨æˆ·åºåˆ—è¡¨ç¤º
#          (3072)                              (hidden_dim)
#                                     â†“
#                            InfoNCE / Cosine å¯¹é½æŸå¤±
#
#   ä¸¤ç§å¯¹é½æ¨¡å¼ï¼ˆé€šè¿‡ --user_align_mode é€‰æ‹©ï¼‰ï¼š
#   - infonce: InfoNCE å¯¹æ¯”å­¦ä¹ ï¼ˆä¸åŸç‰ˆ RLMRec ä¸€è‡´ï¼Œæ¨èï¼‰
#   - cosine:  ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆä¸ AlphaFuse ç‰©å“ä¾§ä¸€è‡´ï¼‰
#
#   ä¸ LLMESR çš„ RASD åŒºåˆ«ï¼š
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚  RLMRec ç”¨æˆ·ä¾§å¯¹é½              vs        LLMESR RASD               â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚  usr_intent_emb.pkl                    sim_user_100.pkl             â”‚
#   â”‚  (ç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥)                    (ç›¸ä¼¼ç”¨æˆ·åˆ—è¡¨)                â”‚
#   â”‚  ç›´æ¥è¯­ä¹‰å¯¹é½                           ç›¸ä¼¼ç”¨æˆ·è’¸é¦                  â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
#   å‘½ä»¤è¡Œå‚æ•°ï¼š
#     --use_user_llm:     æ˜¯å¦å¯ç”¨ç”¨æˆ·ä¾§ LLM ä¿¡æ¯ (é»˜è®¤ False)
#     --alpha_user:       ç”¨æˆ·ä¾§å¯¹é½æŸå¤±æƒé‡ (é»˜è®¤ 1.0)
#     --user_align_mode:  å¯¹é½æ¨¡å¼ infonce/cosine (é»˜è®¤ infonce)
#     --user_align_temp:  InfoNCE æ¸©åº¦å‚æ•° (é»˜è®¤ 1.0)
#
#   æŸå¤±å‡½æ•°ï¼š
#     L_total = L_main + Î² * L_item + Î±_user * Î² * L_user
#     å…¶ä¸­ï¼š
#       - L_main: ä¸»æŸå¤±ï¼ˆInfoNCEï¼‰
#       - L_item: ç‰©å“ä¾§é‡å»ºæŸå¤±ï¼ˆåŸæœ‰ï¼‰
#       - L_user: ç”¨æˆ·ä¾§é‡å»ºæŸå¤±ï¼ˆæ–°å¢ï¼‰
#
# =============================================================================
# ==================== [NEW 2024-12-15] æ·»åŠ ç”¨æˆ·ä¾§ LLM ä¿¡æ¯æ”¯æŒ ====================
class RLMRec(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨è¯­ä¹‰é‡å»ºç­–ç•¥
        self.item_embeddings = Item_Embedding("SR", **key_words)
        self.language_dim = self.item_embeddings.language_dim
        
        # ==================== [NEW] ä¿å­˜ key_words ä¾›åç»­ä½¿ç”¨ ====================
        self.key_words = key_words
        # ==================== [END NEW] ====================

        # æ ¹æ®å¯¹é½ç±»å‹æ„å»ºé‡å»ºå™¨
        if key_words['SR_aligement_type'] == 'con':
            # å¯¹æ¯”å¼ï¼šè¯­è¨€åµŒå…¥ -> ID åµŒå…¥
            self.reconstructor = nn.Sequential(
                nn.Linear(self.language_dim, (self.language_dim + key_words['hidden_dim']) // 2),
                nn.LeakyReLU(),
                nn.Linear((self.language_dim + key_words['hidden_dim']) // 2, key_words['hidden_dim'])
            )
        elif key_words['SR_aligement_type'] == 'gen':
            # ç”Ÿæˆå¼ï¼šID åµŒå…¥ -> è¯­è¨€åµŒå…¥
            self.reconstructor = nn.Sequential(
                nn.Linear(key_words['hidden_dim'], (self.language_dim + key_words['hidden_dim']) // 2),
                nn.LeakyReLU(),
                nn.Linear((self.language_dim + key_words['hidden_dim']) // 2, self.language_dim)
            )
        
        # ==================== [NEW 2024-12-15] ç”¨æˆ·ä¾§ LLM ä¿¡æ¯ç›¸å…³ç»„ä»¶ ====================
        # ç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.usr_intent_emb = None
        
        # ç”¨æˆ·ä¾§é‡å»ºå™¨ï¼ˆç”¨æˆ· LLM è¯­ä¹‰ â†’ åºåˆ—è¡¨ç¤ºç»´åº¦ï¼‰
        # æ³¨æ„ï¼šåªæœ‰å¯¹æ¯”å¼å¯¹é½éœ€è¦æ­¤ç»„ä»¶ï¼Œç”Ÿæˆå¼å¯¹é½æ–¹å‘ç›¸å
        if key_words['SR_aligement_type'] == 'con':
            self.usr_reconstructor = nn.Sequential(
                nn.Linear(self.language_dim, (self.language_dim + key_words['hidden_dim']) // 2),
                nn.LeakyReLU(),
                nn.Linear((self.language_dim + key_words['hidden_dim']) // 2, key_words['hidden_dim'])
            )
        elif key_words['SR_aligement_type'] == 'gen':
            # ç”Ÿæˆå¼ï¼šåºåˆ—è¡¨ç¤º â†’ ç”¨æˆ· LLM è¯­ä¹‰
            self.usr_reconstructor = nn.Sequential(
                nn.Linear(key_words['hidden_dim'], (self.language_dim + key_words['hidden_dim']) // 2),
                nn.LeakyReLU(),
                nn.Linear((self.language_dim + key_words['hidden_dim']) // 2, self.language_dim)
            )
        # ==================== [END NEW] ====================

    def embed_ID(self, x):
        """è·å–ç‰©å“ ID åµŒå…¥"""
        return self.item_embeddings.ID_embeddings(x)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        return self.item_embeddings.ID_embeddings.weight

    def reconstruct_gen_loss(self, ):
        """
        ç”Ÿæˆå¼é‡å»ºæŸå¤±ï¼šID åµŒå…¥ -> è¯­è¨€åµŒå…¥
        L = 1 - cosine_similarity(reconstructor(ID_emb), language_emb)
        """
        rec_language_embs = self.reconstructor(self.return_item_emb()[:-1])  # å»æ‰ padding åµŒå…¥
        language_embs = self.item_embeddings.language_embeddings.weight
        # L2 å½’ä¸€åŒ–è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        rec_language_embs = F.normalize(rec_language_embs, p=2, dim=-1)
        language_embs = F.normalize(language_embs, p=2, dim=-1)
        return 1 - (rec_language_embs * language_embs).sum() / self.item_num

    def reconstruct_con_loss(self, ):
        """
        å¯¹æ¯”å¼é‡å»ºæŸå¤±ï¼šè¯­è¨€åµŒå…¥ -> ID åµŒå…¥
        L = 1 - cosine_similarity(reconstructor(language_emb), ID_emb)
        """
        language_embs = self.item_embeddings.language_embeddings.weight
        rec_ID_embs = self.reconstructor(language_embs)  # å»æ‰ padding åµŒå…¥
        ID_embs = self.return_item_emb()[:-1]
        # L2 å½’ä¸€åŒ–è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        rec_ID_embs = F.normalize(rec_ID_embs, p=2, dim=-1)
        ID_embs = F.normalize(ID_embs, p=2, dim=-1)
        return 1 - (rec_ID_embs * ID_embs).sum() / self.item_num

    # ==================== [NEW 2024-12-15] ç”¨æˆ·ä¾§ LLM ä¿¡æ¯ç›¸å…³æ–¹æ³• ====================
    
    def load_user_intent_embedding(self, user_intent_path):
        """
        åŠ è½½ç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥
        
        Args:
            user_intent_path: ç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥æ–‡ä»¶è·¯å¾„ (usr_intent_emb.pkl)
        """
        import os
        import pickle  # [FIX] æ·»åŠ  pickle å¯¼å…¥
        if os.path.exists(user_intent_path):
            with open(user_intent_path, 'rb') as f:
                user_intent = pickle.load(f)
            self.usr_intent_emb = torch.tensor(user_intent, dtype=torch.float32)
            print(f"[RLMRec] Loaded user intent embedding: {self.usr_intent_emb.shape}")
        else:
            print(f"[RLMRec] Warning: User intent file not found: {user_intent_path}")
            self.usr_intent_emb = None
    
    def cal_infonce_loss(self, embeds1, embeds2, all_embeds2, temp=1.0):
        """
        InfoNCE å¯¹æ¯”æŸå¤±ï¼ˆä¸åŸç‰ˆ RLMRec ä¸€è‡´ï¼‰
        
        Args:
            embeds1: [B, D] é”šç‚¹åµŒå…¥ï¼ˆç”¨æˆ·åºåˆ—è¡¨ç¤ºï¼‰
            embeds2: [B, D] æ­£æ ·æœ¬åµŒå…¥ï¼ˆå½“å‰ batch ç”¨æˆ·çš„ MLP æ˜ å°„åçš„ LLM è¯­ä¹‰ï¼‰
            all_embeds2: [N, D] å…¨éƒ¨æ ·æœ¬åµŒå…¥ï¼ˆå…¨éƒ¨ç”¨æˆ·çš„ MLP æ˜ å°„åçš„ LLM è¯­ä¹‰ï¼Œä½œä¸ºè´Ÿæ ·æœ¬æ± ï¼‰
            temp: æ¸©åº¦å‚æ•°
        
        Returns:
            InfoNCE loss
        """
        # L2 å½’ä¸€åŒ–
        normed_embeds1 = F.normalize(embeds1, p=2, dim=-1)
        normed_embeds2 = F.normalize(embeds2, p=2, dim=-1)
        normed_all_embeds2 = F.normalize(all_embeds2, p=2, dim=-1)
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆåˆ†å­ï¼‰
        nume_term = -(normed_embeds1 * normed_embeds2 / temp).sum(-1)
        
        # ä¸å…¨éƒ¨æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåˆ†æ¯ï¼ŒåŒ…å«è´Ÿæ ·æœ¬ï¼‰
        # ==================== [OLD] log(sum(exp(...)))ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰====================
        # deno_term = torch.log(torch.sum(torch.exp(normed_embeds1 @ normed_all_embeds2.T / temp), dim=-1))
        # ==================== [END OLD] ====================
        
        # ==================== [NEW 2024-12-17] logsumexpï¼ˆæ›´ç¨³å®šæ›´å¿«ï¼‰====================
        deno_term = torch.logsumexp(normed_embeds1 @ normed_all_embeds2.T / temp, dim=-1)
        # ==================== [END NEW] ====================
        
        cl_loss = (nume_term + deno_term).sum()
        return cl_loss
    
    def user_alignment_loss_infonce(self, user_embeds, user_ids, temperature=1.0):
        """
        ç”¨æˆ·ä¾§ InfoNCE å¯¹é½æŸå¤±ï¼ˆæ–¹æ¡ˆ Aï¼šå®Œå…¨å¤åˆ»åŸç‰ˆ RLMRecï¼‰
        
        Args:
            user_embeds: [B, hidden_dim] ç”¨æˆ·åºåˆ—è¡¨ç¤º (forward è¾“å‡º)
            user_ids: [B] ç”¨æˆ· ID
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            InfoNCE å¯¹é½æŸå¤±
        """
        if self.usr_intent_emb is None:
            return torch.tensor(0.0, device=user_embeds.device)
        
        # 1. è·å–å½“å‰ batch ç”¨æˆ·çš„ LLM è¯­ä¹‰åµŒå…¥
        usr_llm_batch = self.usr_intent_emb[user_ids.cpu()].to(user_embeds.device)  # [B, language_dim]
        
        # 2. MLP æ˜ å°„ï¼ˆå¯¹æ¯”å¼ï¼šLLM è¯­ä¹‰ â†’ åºåˆ—è¡¨ç¤ºç»´åº¦ï¼‰
        usr_llm_mapped = self.usr_reconstructor(usr_llm_batch)  # [B, hidden_dim]
        
        # 3. å…¨éƒ¨ç”¨æˆ·çš„ LLM è¯­ä¹‰åµŒå…¥ï¼ˆä½œä¸ºè´Ÿæ ·æœ¬æ± ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å°†å…¨éƒ¨ç”¨æˆ·çš„ LLM è¯­ä¹‰åµŒå…¥æ˜ å°„åˆ°åºåˆ—è¡¨ç¤ºç©ºé—´
        all_usr_llm = self.usr_intent_emb.to(user_embeds.device)  # [N, language_dim]
        all_usr_llm_mapped = self.usr_reconstructor(all_usr_llm)  # [N, hidden_dim]
        
        # 4. InfoNCE å¯¹æ¯”æŸå¤±
        loss = self.cal_infonce_loss(user_embeds, usr_llm_mapped, all_usr_llm_mapped, temperature)
        return loss / user_embeds.shape[0]
    
    def user_alignment_loss_cosine(self, user_embeds, user_ids):
        """
        ç”¨æˆ·ä¾§ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½æŸå¤±ï¼ˆæ–¹æ¡ˆ Bï¼šä¸ AlphaFuse ç‰©å“ä¾§ä¿æŒä¸€è‡´ï¼‰
        
        Args:
            user_embeds: [B, hidden_dim] ç”¨æˆ·åºåˆ—è¡¨ç¤º (forward è¾“å‡º)
            user_ids: [B] ç”¨æˆ· ID
        
        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦å¯¹é½æŸå¤±ï¼š1 - cos_sim(MLP(usr_llm_emb), user_embeds)
        """
        if self.usr_intent_emb is None:
            return torch.tensor(0.0, device=user_embeds.device)
        
        # 1. è·å–å½“å‰ batch ç”¨æˆ·çš„ LLM è¯­ä¹‰åµŒå…¥
        usr_llm_batch = self.usr_intent_emb[user_ids.cpu()].to(user_embeds.device)  # [B, language_dim]
        
        # 2. MLP æ˜ å°„
        usr_llm_mapped = self.usr_reconstructor(usr_llm_batch)  # [B, hidden_dim]
        
        # 3. L2 å½’ä¸€åŒ–
        usr_llm_mapped = F.normalize(usr_llm_mapped, p=2, dim=-1)
        user_embeds_norm = F.normalize(user_embeds, p=2, dim=-1)
        
        # 4. ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        loss = 1 - (usr_llm_mapped * user_embeds_norm).sum(dim=-1).mean()
        return loss
    
    def user_alignment_loss(self, user_embeds, user_ids, mode='infonce', temperature=1.0):
        """
        ç”¨æˆ·ä¾§å¯¹é½æŸå¤±ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            user_embeds: [B, hidden_dim] ç”¨æˆ·åºåˆ—è¡¨ç¤º (forward è¾“å‡º)
            user_ids: [B] ç”¨æˆ· ID
            mode: å¯¹é½æ¨¡å¼ï¼Œ'infonce' æˆ– 'cosine'
            temperature: InfoNCE æ¸©åº¦å‚æ•°ï¼ˆä»… mode='infonce' æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            å¯¹é½æŸå¤±
        """
        if mode == 'infonce':
            return self.user_alignment_loss_infonce(user_embeds, user_ids, temperature)
        elif mode == 'cosine':
            return self.user_alignment_loss_cosine(user_embeds, user_ids)
        else:
            raise ValueError(f"Unknown user alignment mode: {mode}, expected 'infonce' or 'cosine'")
    
    def user_alignment_loss_gen(self, user_embeds, user_ids):
        """
        ç”¨æˆ·ä¾§ç”Ÿæˆå¼å¯¹é½æŸå¤±ï¼ˆç”Ÿæˆå¼ï¼šåºåˆ—è¡¨ç¤º â†’ LLM è¯­ä¹‰ï¼‰
        
        Args:
            user_embeds: [B, hidden_dim] ç”¨æˆ·åºåˆ—è¡¨ç¤º (forward è¾“å‡º)
            user_ids: [B] ç”¨æˆ· ID
        
        Returns:
            ç”Ÿæˆå¼å¯¹é½æŸå¤±ï¼š1 - cos_sim(MLP(user_embeds), usr_llm_emb)
        """
        if self.usr_intent_emb is None:
            return torch.tensor(0.0, device=user_embeds.device)
        
        # 1. è·å–å½“å‰ batch ç”¨æˆ·çš„ LLM è¯­ä¹‰åµŒå…¥
        usr_llm_batch = self.usr_intent_emb[user_ids.cpu()].to(user_embeds.device)  # [B, language_dim]
        
        # 2. MLP æ˜ å°„ï¼ˆç”Ÿæˆå¼ï¼šåºåˆ—è¡¨ç¤º â†’ LLM è¯­ä¹‰ç»´åº¦ï¼‰
        rec_usr_llm = self.usr_reconstructor(user_embeds)  # [B, language_dim]
        
        # 3. L2 å½’ä¸€åŒ–
        rec_usr_llm = F.normalize(rec_usr_llm, p=2, dim=-1)
        usr_llm_batch = F.normalize(usr_llm_batch, p=2, dim=-1)
        
        # 4. ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        loss = 1 - (rec_usr_llm * usr_llm_batch).sum(dim=-1).mean()
        return loss
    # ==================== [END NEW 2024-12-15] ====================


# =============================================================================
# UniSRecï¼šè¯­è¨€åµŒå…¥ + MoE é€‚é…å™¨
# ä½¿ç”¨ Mixture of Experts (MoE) æ›¿ä»£ç®€å•çš„ MLP
# MoE å¯ä»¥æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©ä¸åŒçš„ä¸“å®¶ç½‘ç»œ
# =============================================================================
class UniSRec(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨è‡ªé€‚åº”æŠ•å½±ç­–ç•¥
        self.item_embeddings = Item_Embedding("AP", **key_words)
        self.language_dim = self.item_embeddings.language_dim
        # MoE é€‚é…å™¨ï¼š8 ä¸ªä¸“å®¶ï¼ŒDropout 0.2
        self.adapter = MoEAdaptorLayer(
            8,  # ä¸“å®¶æ•°é‡
            [self.language_dim, key_words['hidden_dim']],  # è¾“å…¥è¾“å‡ºç»´åº¦
            0.2  # Dropout æ¦‚ç‡
        )

    def embed_ID(self, x):
        """è·å–ç‰©å“åµŒå…¥ï¼šè¯­è¨€åµŒå…¥ -> MoE é€‚é…å™¨"""
        language_embs = self.item_embeddings.language_embeddings(x)
        return self.adapter(language_embs)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        language_embs = self.item_embeddings.language_embeddings.weight
        return self.adapter(language_embs)


# =============================================================================
# LLMESRï¼šåŒè§†å›¾æ¨¡å‹
# =============================================================================
# åŒæ—¶ä½¿ç”¨ ID åµŒå…¥å’Œè¯­è¨€åµŒå…¥ï¼Œé€šè¿‡äº¤å‰æ³¨æ„åŠ›è¿›è¡Œäº¤äº’
# æœ€ç»ˆè¾“å‡ºæ˜¯ ID è§†å›¾å’Œè¯­è¨€è§†å›¾çš„æ‹¼æ¥
#
# ã€ç‰©å“ä¾§ LLM ä¿¡æ¯ã€‘ï¼ˆåŸæœ‰ï¼‰
#   - ä½¿ç”¨ 3large_emb.pickle / itm_intent_emb.pklï¼ˆç‰©å“ LLM è¯­ä¹‰åµŒå…¥ï¼‰
#   - é€šè¿‡ Adapter (MLP) é™ç»´åä½œä¸ºè¯­è¨€è§†å›¾è¾“å…¥
#   - ä¸ ID è§†å›¾é€šè¿‡äº¤å‰æ³¨æ„åŠ›äº¤äº’
#   - reg_loss: åŒè§†å›¾å¯¹æ¯”æ­£åˆ™åŒ–æŸå¤±
#
# ã€ç”¨æˆ·ä¾§ LLM ä¿¡æ¯ã€‘ï¼ˆRASD - Retrieval Augmented Self-Distillationï¼‰
#   ä½¿ç”¨ sim_user_100.pklï¼ˆç›¸ä¼¼ç”¨æˆ·åˆ—è¡¨ï¼Œå½¢çŠ¶ [N_users, 100]ï¼‰
#   
#   æ•°æ®æµï¼š
#     sim_user_100[user_id]  â†’  [ç›¸ä¼¼ç”¨æˆ· ID åˆ—è¡¨]  â†’  ç›¸ä¼¼ç”¨æˆ·åºåˆ—
#                                                          â†“
#                                                    forward(sim_seqs)
#                                                          â†“
#                                            Contrastive / KD è’¸é¦æŸå¤±
#
#   ä¸ RLMRec ç”¨æˆ·ä¾§å¯¹é½çš„åŒºåˆ«ï¼š
#   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
#   â”‚  RLMRec ç”¨æˆ·ä¾§å¯¹é½              vs        LLMESR RASD               â”‚
#   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
#   â”‚  usr_intent_emb.pkl                    sim_user_100.pkl             â”‚
#   â”‚  (ç”¨æˆ· LLM è¯­ä¹‰åµŒå…¥, 3072ç»´)            (ç›¸ä¼¼ç”¨æˆ·åˆ—è¡¨, 100ä¸ª)         â”‚
#   â”‚  ç›´æ¥è¯­ä¹‰å¯¹é½                           ç›¸ä¼¼ç”¨æˆ·è’¸é¦                  â”‚
#   â”‚  MLP æ˜ å°„å InfoNCE/Cosine             åºåˆ—è¡¨ç¤ºå¯¹æ¯”å­¦ä¹               â”‚
#   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
#
#   RASD å‘½ä»¤è¡Œå‚æ•°ï¼š
#     --use_rasd:       æ˜¯å¦å¯ç”¨ RASD (é»˜è®¤ False)
#     --alpha_rasd:     RASD æŸå¤±æƒé‡ (é»˜è®¤ 0.1)
#     --sim_user_num:   ä½¿ç”¨çš„ç›¸ä¼¼ç”¨æˆ·æ•°é‡ K (é»˜è®¤ 10)
#     --user_sim_func:  è’¸é¦å‡½æ•° cl/kd (é»˜è®¤ cl)
#
#   æŸå¤±å‡½æ•°ï¼š
#     L_total = L_main + Î² * L_reg + Î±_rasd * L_rasd
#     å…¶ä¸­ï¼š
#       - L_main: ä¸»æŸå¤±ï¼ˆInfoNCEï¼‰
#       - L_reg:  åŒè§†å›¾å¯¹æ¯”æ­£åˆ™åŒ–æŸå¤±ï¼ˆåŸæœ‰ï¼‰
#       - L_rasd: RASD è’¸é¦æŸå¤±ï¼ˆç”¨æˆ·ä¾§ï¼Œæ–°å¢ï¼‰
#
# =============================================================================
class LLMESR(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        # ä½¿ç”¨åŒè§†å›¾ç­–ç•¥
        self.item_embeddings = Item_Embedding("Dual_view", **key_words)
        self.language_dim = self.item_embeddings.language_dim
        # è¯­è¨€åµŒå…¥é€‚é…å™¨ï¼šé™ç»´åˆ°éšè—ç»´åº¦
        self.adapter = nn.Sequential(
            nn.Linear(self.language_dim, int(self.language_dim / 2)),
            nn.Linear(int(self.language_dim / 2), key_words['hidden_dim'])
        )

        # äº¤å‰æ³¨æ„åŠ›å±‚ï¼šID å’Œè¯­è¨€è§†å›¾ç›¸äº’å¢å¼º
        self.language2ID = Multi_CrossAttention(self.hidden_dim, self.hidden_dim, 2)  # è¯­è¨€ -> ID
        self.ID2language = Multi_CrossAttention(self.hidden_dim, self.hidden_dim, 2)  # ID -> è¯­è¨€

        # å¯¹æ¯”æŸå¤±ï¼šç”¨äºæ­£åˆ™åŒ–ä¸¤ä¸ªè§†å›¾
        self.reg = Contrastive_Loss2()

    def embed_ID_text(self, x):
        """è·å– ID åµŒå…¥å’Œè¯­è¨€åµŒå…¥"""
        language_embs = self.item_embeddings.language_embeddings(x)
        ID_embs = self.item_embeddings.ID_embeddings(x)
        return ID_embs, self.adapter(language_embs)

    def embed_ID(self, x):
        """è·å–æ‹¼æ¥åçš„åµŒå…¥ï¼š[ID_emb, language_emb]"""
        ID_embs, language_embs = self.embed_ID_text(x)
        return torch.cat([ID_embs, language_embs], dim=-1)

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥ï¼ˆæ‹¼æ¥ï¼‰"""
        ID_embs = self.item_embeddings.ID_embeddings.weight
        language_embs = self.item_embeddings.language_embeddings.weight
        language_embs = self.adapter(language_embs)
        return torch.cat([ID_embs, language_embs], dim=-1)  # [item_num+1, 2*D]

    def forward(self, sequences):
        """
        å‰å‘ä¼ æ’­ï¼šåŒè§†å›¾ç¼–ç 

        æµç¨‹ï¼š
        1. åˆ†åˆ«è·å– ID åµŒå…¥å’Œè¯­è¨€åµŒå…¥
        2. äº¤å‰æ³¨æ„åŠ›ï¼šID <-> è¯­è¨€
        3. åˆ†åˆ«ç»è¿‡ Transformer ç¼–ç 
        4. æ‹¼æ¥ä¸¤ä¸ªè§†å›¾çš„è¾“å‡º
        """
        # è·å–ä¸¤ç§åµŒå…¥
        inputs_id_emb, inputs_text_emb = self.embed_ID_text(sequences)
        inputs_text_emb += self.positional_embeddings(torch.arange(self.seq_len).to(self.device))
        inputs_id_emb += self.positional_embeddings(torch.arange(self.seq_len).to(self.device))

        text_seq = self.emb_dropout(inputs_text_emb)
        # id_seq = self.emb_dropout(inputs_text_emb)  # âŒ [BUG] åŸä»£ç é”™è¯¯ï¼šåº”è¯¥ç”¨ inputs_id_emb
        # ==================== [FIX] ä¿®å¤ id_seq ä½¿ç”¨é”™è¯¯çš„åµŒå…¥ ====================
        id_seq = self.emb_dropout(inputs_id_emb)  # âœ… æ­£ç¡®ï¼šä½¿ç”¨ ID åµŒå…¥
        # ==================== [END FIX] ====================

        # äº¤å‰æ³¨æ„åŠ›ï¼šä¸¤ä¸ªè§†å›¾ç›¸äº’å¢å¼º
        cross_id_seqs = self.language2ID(text_seq, id_seq, sequences, self.item_num)  # è¯­è¨€ -> ID
        cross_text_seqs = self.ID2language(id_seq, text_seq, sequences, self.item_num)  # ID -> è¯­è¨€
        cross_id_seqs = 1 * cross_id_seqs + 0 * id_seq  # æ®‹å·®è¿æ¥ï¼ˆæƒé‡ 1:0ï¼‰
        cross_text_seqs = 1 * cross_text_seqs + 0 * text_seq

        # ID è§†å›¾çš„ Transformer ç¼–ç 
        mask = torch.ne(sequences, self.item_num).float().unsqueeze(-1).to(self.device)
        cross_id_seqs *= mask
        seq_normalized = self.ln_1(cross_id_seqs)
        mh_attn_out = self.mh_attn(seq_normalized, cross_id_seqs)
        ff_out = self.feed_forward(self.ln_2(mh_attn_out))
        ff_out *= mask
        ff_out = self.ln_3(ff_out)
        id_logits = ff_out[:, -1].squeeze()  # [B, D]

        # è¯­è¨€è§†å›¾çš„ Transformer ç¼–ç 
        # mask = torch.ne(states, self.item_num).float().unsqueeze(-1).to(self.device)
        cross_text_seqs *= mask
        seq_normalized = self.ln_1(cross_text_seqs)
        mh_attn_out = self.mh_attn(seq_normalized, cross_text_seqs)
        ff_out = self.feed_forward(self.ln_2(mh_attn_out))
        ff_out *= mask
        ff_out = self.ln_3(ff_out)
        text_logits = ff_out[:, -1].squeeze()  # [B, D]

        # æ‹¼æ¥ä¸¤ä¸ªè§†å›¾çš„è¾“å‡º
        log_feats = torch.cat([id_logits, text_logits], dim=-1)  # [B, 2*D]

        return log_feats

    def reg_loss(self, sequences):
        """
        æ­£åˆ™åŒ–æŸå¤±ï¼šå¯¹é½ ID è§†å›¾å’Œè¯­è¨€è§†å›¾
        ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æŸå¤±é¼“åŠ±ä¸¤ä¸ªè§†å›¾çš„ä¸€è‡´æ€§
        """
        unfold_item_id = torch.masked_select(sequences, sequences != self.item_num)
        language_emb, id_emb = self.embed_ID_text(unfold_item_id)
        reg_loss = self.reg(language_emb, id_emb)
        return reg_loss

    # ==================== [NEW] RASD (Retrieval Augmented Self-Distillation) æŸå¤± ====================
    def calculate_rasd_loss(self, sequences, sim_seqs, user_sim_func='cl'):
        """
        è®¡ç®— RASD å¯¹é½æŸå¤±ï¼ˆLLM-ESR åŸå§‹æ–¹æ³•ï¼‰
        
        æ€è·¯ï¼šç”¨ç›¸ä¼¼ç”¨æˆ·çš„è¡¨ç¤ºä½œä¸º"æ•™å¸ˆ"ï¼Œè®©å½“å‰ç”¨æˆ·çš„è¡¨ç¤ºå‘æ•™å¸ˆé æ‹¢
        
        Args:
            sequences: [B, S] å½“å‰ç”¨æˆ·çš„ç‰©å“åºåˆ—
            sim_seqs: [B, K, S] ç›¸ä¼¼ç”¨æˆ·çš„ç‰©å“åºåˆ—ï¼ˆK ä¸ªç›¸ä¼¼ç”¨æˆ·ï¼‰
            user_sim_func: 'cl' (å¯¹æ¯”å­¦ä¹ ) æˆ– 'kd' (çŸ¥è¯†è’¸é¦/MSE)
        
        Returns:
            rasd_loss: æ ‡é‡æŸå¤±å€¼
        """
        B, K, S = sim_seqs.shape
        
        # 1. è·å–å½“å‰ç”¨æˆ·çš„è¡¨ç¤º
        h_u = self.forward(sequences)  # [B, 2*D]
        
        # 2. è·å–ç›¸ä¼¼ç”¨æˆ·çš„è¡¨ç¤º
        sim_seqs_flat = sim_seqs.view(B * K, S)  # [B*K, S]
        h_sim = self.forward(sim_seqs_flat)  # [B*K, 2*D]
        
        # 3. å…³é”®ï¼šstop gradientï¼Œç›¸ä¼¼ç”¨æˆ·ä½œä¸º"æ•™å¸ˆ"ä¸æ›´æ–°æ¢¯åº¦
        h_sim = h_sim.detach()
        
        # 4. é‡å¡‘å¹¶å–å¹³å‡
        h_sim = h_sim.view(B, K, -1)  # [B, K, 2*D]
        h_sim_avg = h_sim.mean(dim=1)  # [B, 2*D] å¤šä¸ªç›¸ä¼¼ç”¨æˆ·çš„å¹³å‡è¡¨ç¤º
        
        # 5. è®¡ç®—å¯¹é½æŸå¤±
        if user_sim_func == 'cl':
            # å¯¹æ¯”å­¦ä¹ æŸå¤±
            rasd_loss = self.reg(h_u, h_sim_avg)
        elif user_sim_func == 'kd':
            # çŸ¥è¯†è’¸é¦æŸå¤± (MSE)
            rasd_loss = F.mse_loss(h_u, h_sim_avg)
        else:
            raise ValueError(f"Unknown user_sim_func: {user_sim_func}")
        
        return rasd_loss
    # ==================== [END NEW] ====================

    # ==================== [FIX 2024-12-20] æ·»åŠ ç¼ºå¤±çš„ calculate_infonce_lossï¼ˆä¸ GRU/BERT4Rec ç‰ˆæœ¬ä¸€è‡´ï¼‰====================
    def calculate_infonce_loss(self, sequences, target, neg_ratio, temperature):
        """
        è®¡ç®— InfoNCE å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆåŒè§†å›¾ç‰ˆæœ¬ï¼‰
        
        æ³¨æ„ï¼šLLMESR çš„ forward è¿”å› [B, 2D]ï¼Œembed_ID ä¹Ÿè¿”å› [B, 2D]
        """
        batch_size = target.shape[0]
        
        # è´Ÿé‡‡æ ·
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()
        mask = neg_samples == expanded_target
        while mask.any():
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
            neg_samples = torch.where(mask, new_samples, neg_samples)
            mask = neg_samples == expanded_target
        target_neg = neg_samples.to(target.device)

        # L2 å½’ä¸€åŒ–
        logits = self.forward(sequences)  # [B, 2D]
        logits = F.normalize(logits, p=2, dim=-1)

        # æ­£è´Ÿæ ·æœ¬åµŒå…¥ï¼ˆä½¿ç”¨ embed_ID è¿”å› [B, 2D]ï¼‰
        target_emb = self.embed_ID(target)  # [B, 2D]
        target_emb = F.normalize(target_emb, p=2, dim=-1)
        
        neg_emb = self.embed_ID(target_neg)  # [B, neg_ratio, 2D]
        neg_emb = F.normalize(neg_emb, p=2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        pos_sim = torch.sum(logits * target_emb, dim=-1, keepdim=True) / temperature  # [B, 1]
        neg_sim = torch.bmm(neg_emb, logits.unsqueeze(-1)).squeeze(-1) / temperature  # [B, neg_ratio]

        # InfoNCE æŸå¤±
        all_sim = torch.cat([pos_sim, neg_sim], dim=1)  # [B, 1 + neg_ratio]
        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)
        loss = F.cross_entropy(all_sim, labels)

        return loss
    # ==================== [END FIX] ====================


# =============================================================================
# AlphaFuseï¼šæœ¬æ–‡æå‡ºçš„æ–¹æ³•
# æ ¸å¿ƒåˆ›æ–°ï¼šåœ¨è¯­è¨€åµŒå…¥çš„"é›¶ç©ºé—´"ä¸­æ³¨å…¥ ID ä¿¡æ¯
#
# æ•°å­¦åŸç†ï¼š
# 1. å¯¹è¯­è¨€åµŒå…¥çŸ©é˜µè¿›è¡Œ SVD åˆ†è§£ï¼Œå¾—åˆ°ä¸»æˆåˆ†ç©ºé—´å’Œé›¶ç©ºé—´
# 2. é›¶ç©ºé—´æ˜¯è¯­è¨€åµŒå…¥æ–¹å·®è¾ƒå°çš„ç»´åº¦ï¼ŒåŒ…å«è¾ƒå°‘çš„è¯­ä¹‰ä¿¡æ¯
# 3. åœ¨é›¶ç©ºé—´ä¸­æ³¨å…¥ ID ä¿¡æ¯ï¼Œé¿å…ç ´åè¯­ä¹‰ä¿¡æ¯
#
# èåˆæ–¹å¼ï¼š
# - cover=Falseï¼ˆé»˜è®¤ï¼‰ï¼šfuse_emb = language_emb + ID_embï¼ˆé›¶ç©ºé—´éƒ¨åˆ†ç›¸åŠ ï¼‰
# - cover=Trueï¼šfuse_emb = [language_emb, ID_emb]ï¼ˆæ‹¼æ¥ï¼‰
# =============================================================================
class AlphaFuse(SASRec_backbone):
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)  # è°ƒç”¨ SASRec_backbone çš„ init()
        # ä½¿ç”¨ AlphaFuse åµŒå…¥ç­–ç•¥
        #
        # Item_Embedding(
        #   (language_embeddings): Embedding(18358, 128, padding_idx=18357)
        #   (ID_embeddings): Embedding(18358, 64)
        # )
        self.item_embeddings = Item_Embedding("AF", **key_words)  # è°ƒç”¨ Item_Embedding çš„ init()
        # self.language_dim = self.item_embeddings.language_dim
        # è°ƒç”¨é“¾
        # 1. åˆ›å»º Item_Embedding å®ä¾‹
        # self.item_embeddings = Item_Embedding("AF", **key_words)
        #
        # 2. åœ¨ __init__ ä¸­è°ƒç”¨
        # self.construct_item_embeddings("AF", **key_words)
        #
        # 3. AF æ¨¡å¼è°ƒç”¨ semantic_space_decomposion
        # cliped_language_embs = self.semantic_space_decomposion(hidden_dim, **key_words)
        #                             â†‘ è¿™é‡Œè®¾ç½®äº† self.nullity = 64
        #
        # 4. ç°åœ¨ item_embeddings å°±æœ‰ nullity å±æ€§äº†
        # self.item_embeddings.nullity  # 64
        self.nullity = self.item_embeddings.nullity  # é›¶ç©ºé—´ç»´åº¦  64
        self.cover = key_words["cover"]  # èåˆæ¨¡å¼   False

    def embed_ID(self, x):
        """
        è·å–èåˆåçš„ç‰©å“åµŒå…¥

        èåˆç­–ç•¥ï¼š
        - cover=Falseï¼šè¯­è¨€åµŒå…¥çš„é›¶ç©ºé—´éƒ¨åˆ† + ID åµŒå…¥
        - cover=Trueï¼šè¯­è¨€åµŒå…¥å’Œ ID åµŒå…¥æ‹¼æ¥
        """
        language_embs = self.item_embeddings.language_embeddings(x)  # x:(B,S) â€”â€”> language_embs:[B, S, D] (256,10,128)
        # fuse_embs = language_embs.clone()
        ID_embs = self.item_embeddings.ID_embeddings(x)  # [B, S, nullity] (256,10,64)
        if self.cover:
            # æ‹¼æ¥æ¨¡å¼ï¼š[è¯­è¨€åµŒå…¥, IDåµŒå…¥]

            # æ³¨æ„ï¼š
            # æ ¹æ®è®ºæ–‡ 4.1.5 èŠ‚ï¼š
            #
            # "drop the original values in the null space, and replace them with E_ID"
            # è®ºæ–‡æè¿°çš„æ˜¯æ›¿æ¢ï¼ˆreplaceï¼‰ï¼Œè€Œè¿™æ®µä»£ç æ˜¯ç›¸åŠ ï¼ˆaddï¼‰
            # å¦‚æœè¦ä¸¥æ ¼æŒ‰è®ºæ–‡å®ç°ï¼Œåº”è¯¥æ˜¯ï¼š
            # fuse_embs[..., -self.nullity:] = ID_embs  # æ›¿æ¢ï¼Œè€Œéç›¸åŠ 
            return torch.cat((language_embs, ID_embs), dim=-1)  # (256, 10, 192)
        else:
            # ç›¸åŠ æ¨¡å¼ï¼šåœ¨é›¶ç©ºé—´ç»´åº¦ï¼ˆæœ€å nullity ç»´ï¼‰ç›¸åŠ 
            fuse_embs = language_embs.clone()  # (256,10,128)
            # language_embs: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 128ç»´
            #                 â†‘è¯­ä¹‰64ç»´      â†‘é›¶ç©ºé—´64ç»´
            # ID_embs:                       [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 64ç»´
            #                               â†“ ç›¸åŠ 
            # è¾“å‡º:          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 128ç»´
            #                â†‘è¯­ä¹‰ä¸å˜       â†‘é›¶ç©ºé—´+ID

            # ...ï¼ˆellipsisï¼‰åœ¨ numpy / PyTorch é‡Œè¡¨ç¤ºï¼šâ€œå‰é¢çš„æ‰€æœ‰ç»´åº¦éƒ½ä¸è¦åŠ¨ï¼Œæˆ‘åªå…³å¿ƒæœ€åè¿™å‡ ç»´
            # x[..., -k:] ç­‰ä»·äºï¼šx[:, -k:]
            fuse_embs[..., -self.nullity:] = language_embs[..., -self.nullity:] + ID_embs  # (256,10,128)
        return fuse_embs

    def return_item_emb(self, ):
        """è¿”å›å…¨é‡èåˆç‰©å“åµŒå…¥"""
        # è¿™é‡Œä¸ºä»€ä¹ˆå–æƒé‡ .weight
        # å› ä¸º  return_item_emb éœ€è¦è¿”å›æ‰€æœ‰ç‰©å“çš„åµŒå…¥ï¼Œè€Œä¸æ˜¯æŸ¥æ‰¾ç‰¹å®šç‰©å“
        language_embs = self.item_embeddings.language_embeddings.weight  # (18358,128)
        # fuse_embs = language_embs.clone()
        ID_embs = self.item_embeddings.ID_embeddings.weight  # (18358,64)
        if self.cover:
            # æ‹¼æ¥æ¨¡å¼
            return torch.cat((language_embs, ID_embs), dim=-1)
        else:
            # ç›¸åŠ æ¨¡å¼
            fuse_embs = language_embs.clone()
            fuse_embs[..., -self.nullity:] = language_embs[..., -self.nullity:] + ID_embs  # (18358,128)
        return fuse_embs


# =============================================================================
# [NEW] ProAlign-FA: åŸå‹å¯¹é½çš„åºåˆ—æ¨èæ¨¡å‹
# æ ¸å¿ƒåˆ›æ–°ï¼š
# 1. åˆ©ç”¨ LLM çš„"åäº‹å®æœªæ¥æ¨ç†"ç”Ÿæˆç”¨æˆ·æœªæ¥æ„å›¾ z_next
# 2. é€šè¿‡å…±äº«åŸå‹ç©ºé—´ P è¿›è¡ŒçŸ¥è¯†è’¸é¦
# 3. é—¨æ§è‡ªé€‚åº”èåˆå¾®è§‚çŠ¶æ€ h_u å’Œå®è§‚æ„å›¾ r_u
# 4. æ¨ç†æ—¶æ— éœ€ LLMï¼Œå»¶è¿Ÿä¸åŸå§‹ SASRec ç›¸å½“
#
# ==================== [NEW 2024-12-15] æ¶ˆèå®éªŒæ§åˆ¶ ====================
# é€šè¿‡ --use_user_intent å‚æ•°æ§åˆ¶æ˜¯å¦ä½¿ç”¨ç”¨æˆ·ä¾§ LLM ä¿¡æ¯ï¼š
#
# â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# â”‚  ProAlign (Item-only)           vs        ProAlign (Full)           â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚  --use_user_intent False              --use_user_intent True        â”‚
# â”‚  Item ä¾§: âœ… itm_intent_emb.pkl       Item ä¾§: âœ… itm_intent_emb.pkl â”‚
# â”‚  User ä¾§: âŒ usr_intent_emb.pkl       User ä¾§: âœ… usr_intent_emb.pkl â”‚
# â”‚  L_align = 0                          L_align = æ­£å¸¸è®¡ç®—              â”‚
# â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
# â”‚  ç”¨é€”ï¼šæ¶ˆèå®éªŒï¼Œè¯æ˜ User ä¾§ LLM ä¿¡æ¯çš„è´¡çŒ®                            â”‚
# â”‚  å¯¹æ¯”åŸºçº¿ï¼šLLMInitï¼ˆåªç”¨ Item ä¾§ LLM ä¿¡æ¯ï¼‰                            â”‚
# â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# ==================== [END NEW] æ¶ˆèå®éªŒæ§åˆ¶ ====================
# =============================================================================
class ProAlign(SASRec_backbone):
    """
    ProAlign-FA + SASRec æ¨¡å‹ï¼ˆé€‚é… AlphaFuse æ¡†æ¶ï¼‰

    æ¶æ„ï¼š
    - Base Model: SASRec (ç»§æ‰¿è‡ª SASRec_backbone)
    - åŸå‹çŸ©é˜µ P: ä»ç‰©å“æ„å›¾ embedding èšç±»åˆå§‹åŒ–
    - Adapter: å°† LLM æ„å›¾ (3072ç»´) é™ç»´åˆ° hidden_size
    - é—¨æ§èåˆ: è‡ªé€‚åº”èåˆå¾®è§‚çŠ¶æ€ h_u å’Œå®è§‚æ„å›¾ r_u
    
    æ¶ˆèå®éªŒ:
    - --use_user_intent True  : ProAlign (Full) - ä½¿ç”¨ Item + User ä¾§ LLM ä¿¡æ¯
    - --use_user_intent False : ProAlign (Item-only) - ä»…ä½¿ç”¨ Item ä¾§ LLM ä¿¡æ¯
    """

    def __init__(self, device, **key_words):
        super().__init__(device, **key_words) # è°ƒç”¨ class SASRec_backbone çš„ init()

        # ==================== ä¿å­˜å‚æ•° ====================
        self.key_words = key_words

        # ==================== ID Embeddingï¼ˆä¸ SASRec ä¸€è‡´ï¼‰====================
        self.item_embeddings = nn.Embedding(
            num_embeddings=self.item_num + 1,  # +1 for padding
            embedding_dim=self.hidden_dim,
            padding_idx=self.item_num
        ) # Embedding(12102, 128, padding_idx=12101)

        # ==================== ProAlign-FA è¶…å‚æ•° ====================
        # åŸå‹æ•°é‡ K
        self.num_prototypes = key_words.get('num_prototypes', 64) # 64
        # æ¸©åº¦å‚æ•° Ï„ï¼ˆæ§åˆ¶ softmax é”åº¦ï¼‰
        self.temperature = key_words.get('proto_temperature', 0.1) # 0.1
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèå¼€å…³ ====================
        # no_prototype=True: ç¦ç”¨åŸå‹æœºåˆ¶ï¼ˆæ¶ˆèå®éªŒï¼šw/o Prototypeï¼‰
        # no_prototype=False (é»˜è®¤): æ­£å¸¸ä½¿ç”¨åŸå‹æœºåˆ¶
        self.no_prototype = key_words.get('no_prototype', False)
        if self.no_prototype:
            print("[ProAlign] âš ï¸ ABLATION MODE: Prototype mechanism DISABLED (w/o Prototype)")
        # ==================== [END NEW 2025-01-17] ====================
        # å¯¹é½æŸå¤±æƒé‡ Î±
        self.alpha = key_words.get('alpha', 0.1) # 0.1
        # èšç±»æŸå¤±æƒé‡ Î²
        self.beta_proto = key_words.get('beta_proto', 0.01) # 0.01
        # LLM æ„å›¾ç»´åº¦
        self.llm_dim = key_words.get('llm_dim', 3072) # 3072
        # èåˆæ¨¡å¼: 'add' æˆ– 'concat'
        self.fusion_mode = key_words.get('fusion_mode', 'concat') # 'concat'
        # åŠ æ³•èåˆæ—¶çš„è¯­ä¹‰æƒé‡
        self.semantic_weight = key_words.get('semantic_weight', 0.5) # 0.5
        
        # ==================== [NEW-MultiHead] å¤šå¤´è§£è€¦åŸå‹å‚æ•° ====================
        # åŸå‹æ³¨æ„åŠ›å¤´æ•° Hï¼ˆè§£å†³"è¯­ä¹‰ä¸­å’Œ"é—®é¢˜ï¼‰
        # æ ¸å¿ƒæ€æƒ³ï¼šå°† D ç»´åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ª head ç‹¬ç«‹å¯»å€
        # Head 1: å…³æ³¨"åŠŸèƒ½"ç»´åº¦, Head 2: å…³æ³¨"å“ç‰Œ/ä»·æ ¼"ç»´åº¦, etc.
        self.num_heads_proto = key_words.get('num_heads_proto', 1)  # é»˜è®¤1=å•å¤´ï¼ˆå‘åå…¼å®¹ï¼‰
        self.head_dim = self.hidden_dim // self.num_heads_proto # 128//1=128
        assert self.hidden_dim % self.num_heads_proto == 0, \
            f"hidden_dim ({self.hidden_dim}) must be divisible by num_heads_proto ({self.num_heads_proto})"
        # ==================== [END NEW-MultiHead] ====================

        # ==================== åŸå‹çŸ©é˜µ P: [K, hidden_size] ====================

        # åŸå‹çŸ©é˜µ P: [K, hidden_size]
        # åˆå§‹åŒ–æ–¹å¼ï¼šä»ç‰©å“æ„å›¾ embedding èšç±»å¾—åˆ°
        # nn.Parameter æŠŠä¸€ä¸ª Tensor å˜æˆâ€œè¿™ä¸ªæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°â€
        self.prototypes = nn.Parameter(torch.zeros(self.num_prototypes, self.hidden_dim)) # (64,128)

        # ==================== Adapter: LLM æ„å›¾é™ç»´ ====================

        # Sequential(
        #   (0): Linear(in_features=3072, out_features=768, bias=True)
        #   (1): ReLU()
        #   (2): Linear(in_features=768, out_features=128, bias=True)
        # )
        self.adapter = nn.Sequential(
            nn.Linear(self.llm_dim, self.llm_dim // 4),
            nn.ReLU(),
            nn.Linear(self.llm_dim // 4, self.hidden_dim)
        )

        # ==================== é—¨æ§ç½‘ç»œï¼ˆæ‹¼æ¥æ¨¡å¼ä½¿ç”¨ï¼‰====================

        # é—¨æ§ç½‘ç»œ: è‡ªé€‚åº”èåˆæƒé‡
        # è¾“å…¥: [h_u; r_u] (2 * hidden_size)
        # è¾“å‡º:
        #   - æ ‡é‡é—¨æ§ (é»˜è®¤): g âˆˆ (0, 1)^1ï¼Œå…¨å±€ç»Ÿä¸€åŠ æƒ

        # Sequential(
        #   (0): Linear(in_features=256, out_features=128, bias=True)
        #   (1): ReLU()
        #   (2): Linear(in_features=128, out_features=1, bias=True)
        #   (3): Sigmoid()
        # )
        #
        self.gate = nn.Sequential(
            nn.Linear(2 * self.hidden_dim, self.hidden_dim),
            nn.ReLU(),
            nn.Linear(self.hidden_dim, 1),
            nn.Sigmoid()
        )

        # ==================== å¯å­¦ä¹ ç¼©æ”¾å› å­ ====================
        self.macro_scale = nn.Parameter(torch.tensor(1.0))

        # ==================== [NEW-SLSI] åºåˆ—çº§è¯­ä¹‰æ³¨å…¥å‚æ•° ====================
        # æ˜¯å¦å¯ç”¨ SLSIï¼ˆåœ¨æ¯ä¸ªåºåˆ—ä½ç½®æ³¨å…¥è¯­ä¹‰ä¿¡æ¯ï¼‰
        self.use_slsi = key_words.get('use_slsi', False)
        # SLSI è¯­ä¹‰æ³¨å…¥æƒé‡
        self.slsi_weight = key_words.get('slsi_weight', 0.3)
        # ==================== [NEW-SLSI-ContextAware] ä¸Šä¸‹æ–‡æ„ŸçŸ¥ SLSI ====================
        # æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥ SLSIï¼ˆç»“åˆå†å²ä½ç½®çš„ä¿¡æ¯ï¼‰
        # False: æ¯ä¸ªä½ç½®ç‹¬ç«‹åšåŸå‹å¯»å€ï¼ˆé»˜è®¤ï¼Œç®€å•é«˜æ•ˆï¼‰
        # True: ç»“åˆå†å²ä½ç½®çš„ç´¯ç§¯è¡¨ç¤ºåšåŸå‹å¯»å€ï¼ˆä¸Šä¸‹æ–‡æ„ŸçŸ¥ï¼Œæ›´å¤æ‚ï¼‰
        self.slsi_context_aware = key_words.get('slsi_context_aware', False)
        # ==================== [END NEW-SLSI-ContextAware] ====================
        # ==================== [END NEW-SLSI] ====================

        # # ==================== [NEW] Forward Predictor (å‰å‘é¢„æµ‹å™¨) (å·²æ³¨é‡Šï¼Œæ¢å¤åŸå§‹çŠ¶æ€) ====================
        # # h_u â†’ Predictor â†’ h_pred, è®©æ¨¡å‹"å­¦ä¼šé¢„æµ‹æœªæ¥æ„å›¾"
        # self.forward_predictor = nn.Sequential(
        #     nn.Linear(self.hidden_dim, self.hidden_dim * 2),
        #     nn.ReLU(),
        #     nn.Dropout(0.1),
        #     nn.Linear(self.hidden_dim * 2, self.hidden_dim)
        # )
        # # æ˜¯å¦ä½¿ç”¨å‰å‘é¢„æµ‹å™¨ï¼ˆå¯é€šè¿‡å‚æ•°æ§åˆ¶ï¼Œä¾¿äºæ¶ˆèå®éªŒï¼‰
        # self.use_forward_predictor = key_words.get('use_forward_predictor', True)

        # ==================== [NEW 2024-12-31] åŠ¨æ€æ³¨æ„åŠ›èåˆ ====================
        # æ›¿ä»£ç®€å•çš„åŠ æ³•/é—¨æ§ï¼Œå®ç°åŠ¨æ€çš„åŸå‹å¯»å€
        # ä¸ ProAlign_BERT4Rec ä¿æŒä¸€è‡´
        self.use_attn_fusion = key_words.get('use_attn_fusion', True)
        if self.use_attn_fusion:
            self.proto_attn = nn.MultiheadAttention(
                embed_dim=self.hidden_dim,
                num_heads=4,  # 4å¤´æ•æ‰ä¸åŒç»´åº¦çš„æ„å›¾
                batch_first=True
            )
        # ==================== [END NEW 2024-12-31] ====================

        # ==================== [NEW 2024-12-31] è¯¾ç¨‹å­¦ä¹ å‚æ•° ====================
        # ä¸ ProAlign_BERT4Rec ä¿æŒä¸€è‡´
        self.warmup_epochs = key_words.get('warmup_epochs', 5)
        self.current_epoch = 0  # ç”± train.py æ›´æ–°
        # ==================== [END NEW 2024-12-31] ====================

        # ==================== [NEW 2024-12-31] è¯­ä¹‰å›°éš¾è´Ÿæ ·æœ¬ ====================
        # ä¸ ProAlign_BERT4Rec ä¿æŒä¸€è‡´
        self.hard_neg_indices = None  # [V, K] æ¯ä¸ªç‰©å“çš„ Top-K ç›¸ä¼¼ç‰©å“
        self.hard_neg_top_k = key_words.get('hard_neg_top_k', 10)
        self.item_intent_emb_for_align = None  # ä¿å­˜åŸå§‹ç‰©å“æ„å›¾åµŒå…¥ä¾› L_align ä½¿ç”¨
        # ==================== [END NEW 2024-12-31] ====================

        # ==================== æ„å›¾ Embeddingï¼ˆå»¶è¿ŸåŠ è½½ï¼‰====================
        self.user_intent_emb = None  # ç”¨æˆ·æœªæ¥æ„å›¾
        self.item_intent_emb = None  # ç‰©å“æ„å›¾ï¼ˆç”¨äºåˆå§‹åŒ–åŸå‹ï¼‰
        self.item_emb_reduced = None  # PCA é™ç»´åçš„ç‰©å“ embedding
        self.prototype_initialized = False

        # ==================== [NEW 2025-01-17] æ¨ç†æ•ˆç‡ä¼˜åŒ–ç¼“å­˜ ====================
        self._inference_mode = False  # æ˜¯å¦å¤„äºæ¨ç†ä¼˜åŒ–æ¨¡å¼
        self._item_proto_cache = None  # [V+1, D] ç‰©å“åŸå‹è¡¨ç¤ºç¼“å­˜
        self._fused_item_cache = None  # [V+1, D] æˆ– [V+1, 2D] èåˆåçš„ç‰©å“åµŒå…¥ç¼“å­˜
        # ==================== [END NEW 2025-01-17] ====================

        # ==================== åˆå§‹åŒ–æƒé‡ ====================
        # ==================== [OLD] åŸå§‹åˆå§‹åŒ–æ–¹å¼ï¼ˆå·²æ³¨é‡Šï¼‰====================
        # self._init_proalign_weights()
        # ==================== [END OLD] ====================
        
        # ==================== [NEW 2024-12-17] BSARec é£æ ¼ç»Ÿä¸€åˆå§‹åŒ– ====================
        # ä½¿ç”¨ apply ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—ï¼Œç„¶åå¤„ç†ç‰¹æ®Šå‚æ•°
        self.apply(self._init_bsarec_weights)
        self._init_special_params()
        # ==================== [END NEW] ====================

    def _init_proalign_weights(self):
        """åˆå§‹åŒ– ProAlign ç‰¹æœ‰çš„æƒé‡"""
        # ID Embedding åˆå§‹åŒ–
        nn.init.normal_(self.item_embeddings.weight, 0, 0.02)
        # Adapter åˆå§‹åŒ–
        for module in self.adapter:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, 0, 0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        # Gate åˆå§‹åŒ–
        for module in self.gate:
            if isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, 0, 0.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
        # # [NEW] Forward Predictor åˆå§‹åŒ– (å·²æ³¨é‡Šï¼Œæ¢å¤åŸå§‹çŠ¶æ€)
        # for module in self.forward_predictor:
        #     if isinstance(module, nn.Linear):
        #         nn.init.normal_(module.weight, 0, 0.02)
        #         if module.bias is not None:
        #             nn.init.zeros_(module.bias)

    # ==================== [NEW 2024-12-17] BSARec é£æ ¼ç»Ÿä¸€åˆå§‹åŒ– ====================
    # å‚è€ƒ BSARec çš„ _abstract_model.py ä¸­çš„ init_weights æ–¹æ³•
    # ä¼˜ç‚¹ï¼š
    #   1. ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—ï¼ˆEmbedding/Linear/LayerNorm/GRUï¼‰
    #   2. è‡ªåŠ¨å¤„ç† padding_idx è¡Œæ¸…é›¶
    #   3. é¢„ç•™ GRU æ”¯æŒ
    
    def _init_bsarec_weights(self, module):
        """
        BSARec-style: ç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰å­æ¨¡å—å‚æ•°
        - Embedding: normal_(0, 0.02) + padding è¡Œæ¸…é›¶
        - Linear: normal_(0, 0.02) + bias æ¸…é›¶
        - LayerNorm: weight=1, bias=0
        - GRU: xavier_uniform (input-hidden), orthogonal (hidden-hidden)
        """
        if isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            # å…³é”®ï¼šå°† padding_idx å¯¹åº”çš„è¡Œæ¸…é›¶
            if module.padding_idx is not None:
                with torch.no_grad():
                    module.weight[module.padding_idx].fill_(0.0)
                    
        elif isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
                
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
            
        elif isinstance(module, nn.GRU):
            # é¢„ç•™ï¼šåç»­æ·»åŠ  GRU æ—¶è‡ªåŠ¨åˆå§‹åŒ–
            for name, param in module.named_parameters():
                if "weight_ih" in name:
                    nn.init.xavier_uniform_(param.data)
                elif "weight_hh" in name:
                    nn.init.orthogonal_(param.data)
                elif "bias" in name:
                    nn.init.zeros_(param.data)

    def _init_special_params(self):
        """
        åˆå§‹åŒ–ä¸ä¼šè¢« module.apply è¦†ç›–çš„å‚æ•°ï¼ˆå¦‚ nn.Parameterï¼‰
        """
        # prototypes æ˜¯ nn.Parameterï¼Œä¸å±äºæŸä¸ªå­ module çš„ weight
        with torch.no_grad():
            if torch.all(self.prototypes == 0):
                nn.init.normal_(self.prototypes, mean=0.0, std=0.02)
        
        # macro_scale ä¿æŒé»˜è®¤å€¼ 1.0ï¼ˆå·²åœ¨å®šä¹‰æ—¶åˆå§‹åŒ–ï¼‰
        
        # ä¿é™©ï¼šå†æ¬¡ç¡®ä¿ padding è¡Œä¸º 0
        with torch.no_grad():
            self.item_embeddings.weight[self.item_num].fill_(0.0)
    # ==================== [END NEW] BSARec é£æ ¼ç»Ÿä¸€åˆå§‹åŒ– ====================

    def load_intent_embeddings(self, user_intent_path, item_intent_path):
        """
        åŠ è½½é¢„è®¡ç®—çš„ LLM æ„å›¾ embedding

        Args:
            user_intent_path: ç”¨æˆ·æœªæ¥æ„å›¾ embedding è·¯å¾„
            item_intent_path: ç‰©å“æ„å›¾ embedding è·¯å¾„
        """
        # åŠ è½½ç”¨æˆ·æ„å›¾ embedding
        if os.path.exists(user_intent_path):
            with open(user_intent_path, 'rb') as f:
                user_intent = pickle.load(f)
            self.user_intent_emb = torch.tensor(user_intent, dtype=torch.float32) # (22363,3072)
            print(f"[ProAlign] Loaded user intent embedding: {self.user_intent_emb.shape}") # [ProAlign] Loaded user intent embedding: torch.Size([22363, 3072])
        else:
            print(f"[ProAlign] Warning: User intent file not found: {user_intent_path}")

        # åŠ è½½ç‰©å“æ„å›¾ embedding
        if os.path.exists(item_intent_path):
            with open(item_intent_path, 'rb') as f:
                item_intent = pickle.load(f)
            self.item_intent_emb = item_intent  # numpy array   # (12101,3072)
            print(f"[ProAlign] Loaded item intent embedding: {self.item_intent_emb.shape}") # [ProAlign] Loaded item intent embedding: (12101, 3072)
        else:
            print(f"[ProAlign] Warning: Item intent file not found: {item_intent_path}")

    def initialize_item_embeddings(self):
        """
        ä½¿ç”¨ LLM è¯­ä¹‰å‘é‡åˆå§‹åŒ– ID Embedding (Semantic Warm-up)
        """

        #                 item_intent_emb [12101, 3072]
        #                     (åŸå§‹ LLM å‘é‡)
        #                           â”‚
        #                           â–¼
        #                    PCA é™ç»´ (åªæ‰§è¡Œä¸€æ¬¡)
        #                           â”‚
        #                           â–¼
        #               item_emb_reduced [12101, 128]
        #                       (ç¼“å­˜)
        #                     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
        #                     â–¼         â–¼
        #             ID Embedding   K-Means èšç±»
        #                åˆå§‹åŒ–          â”‚
        #                   â”‚           â–¼
        #                   â–¼       Prototypes [K, 128]
        #          item_embeddings     (åŸå‹çŸ©é˜µ)
        #            [12102, 128]

        if self.item_intent_emb is None:
            print("[ProAlign] Warning: No item intent found, using random init.")
            return

        print("[ProAlign] Initializing ID Embeddings from LLM semantics...")

        # 1. PCA é™ç»´
        if self.item_emb_reduced is None:
            pca = PCA(n_components=self.hidden_dim) # PCA(n_components=128)
            self.item_emb_reduced = pca.fit_transform(self.item_intent_emb)  # self.item_emb_reduced ï¼ˆPCA ç»“æœç¼“å­˜åœ¨ self.item_emb_reducedï¼‰
            print(f"  PCA: {self.item_intent_emb.shape} -> {self.item_emb_reduced.shape}") #  PCA: (12101, 3072) -> (12101, 128)

        # 2. å½’ä¸€åŒ–
        with torch.no_grad():
            pretrained_weight = torch.tensor(self.item_emb_reduced, dtype=torch.float32) # numpy â†’ torch.Tensor   [V, D]  (12101,128)
            pretrained_weight = F.normalize(pretrained_weight, p=2, dim=-1) # (12101,128) [V, D], æ¨¡é•¿=1

            # 3. æ‹¼æ¥ Padding (Index = item_num)
            padding = torch.zeros(1, self.hidden_dim) # [1, 128]
            # ä½¿ç”¨é›¶å‘é‡ä½œä¸º Padding æ˜¯æ­£ç¡®çš„ï¼
            # åŸå› è§£é‡Š
            # 1. æ•°å­¦æ„ä¹‰
            # Padding çš„ä½œç”¨ï¼šè¡¨ç¤º"æ— äº¤äº’"æˆ–"æ— æ•ˆä½ç½®"
            #
            # é›¶å‘é‡çš„ç‰¹æ€§ï¼š
            # - ç‚¹ç§¯: h_u Â· 0 = 0  â†’ ä¸å½±å“å¾—åˆ†è®¡ç®—
            # - æ³¨æ„åŠ›: é›¶å‘é‡ä¸è´¡çŒ®ä»»ä½•ä¿¡æ¯
            #
            # 2. ä¸ nn.Embedding çš„ padding_idx ä¸€è‡´
            # self.item_embeddings = nn.Embedding(item_num + 1, hidden_dim, padding_idx=item_num)
            # å½“è®¾ç½® padding_idx=12101 æ—¶ï¼ŒPyTorch ä¼šè‡ªåŠ¨ï¼š
            # å°† index=12101 çš„å‘é‡åˆå§‹åŒ–ä¸ºé›¶å‘é‡
            # è®­ç»ƒæ—¶ä¸æ›´æ–°è¿™ä¸ªä½ç½®çš„æ¢¯åº¦
            #
            # 3. æˆ‘ä»¬æ‰‹åŠ¨åˆå§‹åŒ–ä¹Ÿç”¨é›¶å‘é‡
            # padding = torch.zeros(1, self.hidden_dim)  # é›¶å‘é‡ï¼Œä¸ padding_idx çš„é»˜è®¤è¡Œä¸ºä¸€è‡´

            # ç»“æœï¼š
            # new_weight[0:12101] = ç‰©å“ 0-12100 çš„è¯­ä¹‰å‘é‡
            # new_weight[12101] = é›¶å‘é‡ (Padding)

            # padding_idx=12101 è¡¨ç¤º index 12101 æ˜¯ Padding
            # torch.cat([pretrained_weight, padding], dim=0) æŠŠé›¶å‘é‡æ”¾åœ¨ index 12101
            new_weight = torch.cat([pretrained_weight, padding], dim=0)  # [item_num+1, D]   [12102, 128]

            # 4. è¦†ç›–æƒé‡
            if new_weight.shape[0] == self.item_embeddings.weight.shape[0]:
                self.item_embeddings.weight.data.copy_(new_weight)
                # éªŒè¯æ›´æ–°çš„æ˜¯ ID Embeddingï¼Œä¸æ˜¯ LLMçš„Embedding
                print('æ›´æ–°çš„æ˜¯ ID Embeddingï¼Œä¸æ˜¯ LLMçš„Embedding', self.item_embeddings.weight.requires_grad)  # True âœ…
                print(f"   ID Embeddings initialized! Shape: {new_weight.shape}")
            else:
                print(f"   Shape mismatch: {new_weight.shape} vs {self.item_embeddings.weight.shape}")

    def initialize_prototypes(self):
        """
        ä½¿ç”¨ç‰©å“æ„å›¾ embedding åˆå§‹åŒ–åŸå‹çŸ©é˜µï¼ˆK-Means èšç±»ï¼‰
        """
        if self.prototype_initialized:
            return

        if self.item_emb_reduced is None: # â† æ£€æŸ¥ç¼“å­˜
            if self.item_intent_emb is None:
                print("[ProAlign] Warning: No item intent, using random prototypes.")
                nn.init.normal_(self.prototypes.data, 0, 0.02)
                self.prototype_initialized = True
                return
            # PCA é™ç»´
            pca = PCA(n_components=self.hidden_dim)
            self.item_emb_reduced = pca.fit_transform(self.item_intent_emb)

        print(f"[ProAlign] Initializing prototypes with K-Means (K={self.num_prototypes})...")

        # K-Means èšç±»
        kmeans = KMeans(n_clusters=self.num_prototypes, # åŸå‹æ•°é‡ Kï¼ˆæ¯”å¦‚ 64ï¼‰
                        random_state=42,                # éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
                        n_init=10                       # è¿è¡Œ 10 æ¬¡ä¸åŒçš„éšæœºåˆå§‹åŒ–ï¼Œé€‰æœ€å¥½çš„é‚£æ¬¡
                        ) # KMeans(n_clusters=64, n_init=10, random_state=42)
        # item_emb_reducedï¼šPCA ä¹‹åçš„ç‰©å“è¯­ä¹‰å‘é‡
        # å½¢çŠ¶æ˜¯ [V, D]ï¼Œåœ¨ä½ è¿™é‡Œæ˜¯ [12101, 128]
        # æ¯ä¸€è¡Œï¼šä¸€ä¸ªç‰©å“åœ¨ 128 ç»´è¯­ä¹‰ç©ºé—´é‡Œçš„å‘é‡
        #
        # fit çš„ä½œç”¨ï¼š
        # ç”¨ KMeans ç®—æ³•ï¼ŒæŠŠè¿™ 12101 ä¸ªç‚¹åˆ†æˆ K ä¸ªç°‡ï¼Œå¹¶æ±‚å‡ºæ¯ä¸ªç°‡çš„ä¸­å¿ƒ
        kmeans.fit(self.item_emb_reduced) # â† ä½¿ç”¨å·²ç¼“å­˜çš„ PCA ç»“æœåš K-Means  (12101, 128)
        # æŠŠ KMeans å­¦åˆ°çš„ K ä¸ªç°‡ä¸­å¿ƒ å–å‡ºæ¥ï¼Œå½“ä½œâ€œè¯­ä¹‰è´¨å¿ƒâ€
        centroids = kmeans.cluster_centers_    # (64,128)  64 ä¸ªç°‡ä¸­å¿ƒï¼Œæ¯ä¸ªç°‡ä¸­å¿ƒæ˜¯ä¸€ä¸ª 128 ç»´å‘é‡

        # èµ‹å€¼åŸå‹çŸ©é˜µ
        with torch.no_grad():
            centroids_tensor = torch.tensor(centroids, dtype=torch.float32)   # (64,128)
            self.prototypes.data = F.normalize(centroids_tensor, p=2, dim=-1) # (64,128)

        # ==================== [NEW] æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦å†»ç»“åŸå‹ ====================
        # åŸå§‹ä»£ç : self.prototypes.requires_grad = False (å§‹ç»ˆå†»ç»“)

        # get('freeze_prototypes', True) çš„æ„æ€ï¼š
        #
        # å»å­—å…¸é‡ŒæŸ¥ key 'freeze_prototypes'
        # å¦‚æœ æŸ¥åˆ°äº†ï¼Œå°±è¿”å›å®ƒå¯¹åº”çš„å€¼ï¼Œæ¯”å¦‚ True æˆ– False
        # å¦‚æœ æ²¡æŸ¥åˆ°è¿™ä¸ª keyï¼Œå°±è¿”å›åé¢çš„é»˜è®¤å€¼ True
        freeze_proto = self.key_words.get('freeze_prototypes', True)
        if freeze_proto:
            self.prototypes.requires_grad = False
            print(f"  âœ… Prototypes initialized and FROZEN. Shape: {self.prototypes.shape}") # (64,128)
        else:
            self.prototypes.requires_grad = True
            print(f"  âœ… Prototypes initialized and TRAINABLE. Shape: {self.prototypes.shape}")
        # ==================== [END NEW] ====================

        # é‡Šæ”¾å†…å­˜
        self.item_intent_emb = None
        # ä½œç”¨ï¼šé˜²æ­¢é‡å¤åˆå§‹åŒ–åŸå‹çŸ©é˜µ
        #
        # åŸå› ï¼š
        # K-Means èšç±»æ˜¯è€—æ—¶æ“ä½œ
        # å¦‚æœå¤šæ¬¡è°ƒç”¨ initialize_prototypes() ï¼Œä¸åº”è¯¥é‡å¤æ‰§è¡Œ
        # ç”¨å¸ƒå°”æ ‡å¿—é˜²æ­¢é‡å¤åˆå§‹åŒ–
        self.prototype_initialized = True

    # ==================== [NEW 2024-12-31] é¢„è®¡ç®—å›°éš¾è´Ÿæ ·æœ¬ï¼ˆä¸ BERT4Rec ç‰ˆæœ¬ä¸€è‡´ï¼‰====================
    def precompute_hard_negatives(self, top_k=10):
        """
        é¢„è®¡ç®—æ¯ä¸ªç‰©å“çš„ Top-K è¯­ä¹‰ç›¸ä¼¼ç‰©å“ä½œä¸ºå›°éš¾è´Ÿæ ·æœ¬

        åŸç†ï¼š
        - éšæœºè´Ÿæ ·æœ¬å¤ªå¼±ï¼ˆ"é¼ æ ‡" vs "å«ç”Ÿçº¸"ï¼‰ï¼Œæ¨¡å‹èµ°æ·å¾„
        - å›°éš¾è´Ÿæ ·æœ¬ï¼ˆ"æ¸¸æˆé¼ æ ‡" vs "åŠå…¬é¼ æ ‡"ï¼‰å¼ºè¿«æ¨¡å‹å­¦ä¹ ç»†ç²’åº¦åŒºåˆ†

        Args:
            top_k: æ¯ä¸ªç‰©å“ä¿ç•™çš„å›°éš¾è´Ÿæ ·æœ¬æ•°é‡
        """
        if self.item_intent_emb is None:
            print("[ProAlign] Warning: item_intent_emb not loaded, skip hard negative precomputation")
            return

        # ä½¿ç”¨åŸå§‹ç‰©å“æ„å›¾åµŒå…¥è®¡ç®—ç›¸ä¼¼åº¦
        item_emb = self.item_intent_emb  # [V, llm_dim]
        # [FIX 2024-12-31] numpy array â†’ torch.Tensor
        if isinstance(item_emb, np.ndarray):
            item_emb = torch.tensor(item_emb, dtype=torch.float32)
        item_emb_norm = F.normalize(item_emb, p=2, dim=-1)

        # è®¡ç®—ç‰©å“é—´ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        V = item_emb.size(0)
        batch_size = 1000  # åˆ†æ‰¹è®¡ç®—ï¼Œé¿å… OOM

        hard_neg_indices = []
        for i in range(0, V, batch_size):
            end_i = min(i + batch_size, V)
            batch_emb = item_emb_norm[i:end_i]  # [batch, D]
            sim_matrix = torch.matmul(batch_emb, item_emb_norm.t())  # [batch, V]

            # æ’é™¤è‡ªå·±ï¼ˆè®¾ä¸ºæå°å€¼ï¼‰
            for j in range(end_i - i):
                sim_matrix[j, i + j] = -1e9

            # å– Top-K æœ€ç›¸ä¼¼çš„ç‰©å“
            _, topk_indices = torch.topk(sim_matrix, top_k, dim=-1)  # [batch, K]
            hard_neg_indices.append(topk_indices)

        self.hard_neg_indices = torch.cat(hard_neg_indices, dim=0)  # [V, K]
        self.hard_neg_top_k = top_k
        self.item_intent_emb_for_align = item_emb  # ä¿å­˜ä¾› L_align ä½¿ç”¨
        print(f"[ProAlign] Hard negatives precomputed: {V} items Ã— Top-{top_k}")
    # ==================== [END NEW 2024-12-31] ====================

    # ==================== [NEW] åŸå‹å¯»å€å·¥å…·å‡½æ•° ====================
    def _proto_address(self, x, normalize_proto=False):
        """
        å¯¹ä»»æ„å½¢çŠ¶çš„è¾“å…¥ x[..., D] åšåŸå‹å¯»å€ï¼Œè¾“å‡º r[..., D]
        æ”¯æŒå•å¤´ / å¤šå¤´
        
        Args:
            x: [..., D] è¾“å…¥åµŒå…¥
            normalize_proto: æ˜¯å¦å¯¹åŸå‹è¿›è¡Œ L2 å½’ä¸€åŒ–
        
        Returns:
            r: [..., D] åŸå‹åŠ æƒè¡¨ç¤º
        """
        orig_shape = x.shape
        D = orig_shape[-1]
        x_flat = x.view(-1, D)  # [N, D]
        N = x_flat.size(0)

        P = self.prototypes
        if normalize_proto:
            P = F.normalize(P, p=2, dim=-1)

        if self.num_heads_proto == 1:
            # å•å¤´æ¨¡å¼
            score = torch.matmul(x_flat, P.t()) / self.temperature  # [N, K]
            pi = F.softmax(score, dim=-1)
            r = torch.matmul(pi, P)  # [N, D]
        else:
            # å¤šå¤´æ¨¡å¼ï¼šx -> [N, H, d]ï¼ŒP -> [K, H, d]
            x_h = x_flat.view(N, self.num_heads_proto, self.head_dim)                 # [N, H, d]
            P_h = P.view(self.num_prototypes, self.num_heads_proto, self.head_dim)   # [K, H, d]
            scores = torch.einsum('nhd,khd->nhk', x_h, P_h) / self.temperature        # [N, H, K]
            pi = F.softmax(scores, dim=-1)                                           # [N, H, K]
            r_h = torch.einsum('nhk,khd->nhd', pi, P_h)                               # [N, H, d]
            r = r_h.reshape(N, D)                                                    # [N, D]

        r = r * self.macro_scale
        return r.view(*orig_shape)
    # ==================== [END NEW] ====================

    def embed_ID(self, x):
        """è·å–ç‰©å“ ID embeddingï¼ˆå…¼å®¹åŸºç±»æ¥å£ï¼‰"""
        return self.item_embeddings(x)  # åŸºç±» SASRec_backbone çš„ forward() ä¼šè°ƒç”¨

    def return_item_emb(self):
        """
        è¿”å›å…¨é‡ç‰©å“ embeddingï¼ˆå…¼å®¹åŸºç±»æ¥å£ï¼‰
        
        ç”¨äºæ¨ç†æ—¶è®¡ç®—ç”¨æˆ·è¡¨ç¤ºä¸æ‰€æœ‰ç‰©å“çš„ç›¸ä¼¼åº¦
        è¿”å›çš„æ˜¯èåˆåçš„ç‰©å“è¡¨ç¤ºï¼šID embedding + åŸå‹è¯­ä¹‰ä¿¡æ¯
        """
        # ==================== [NEW 2025-01-17] æ¨ç†æ•ˆç‡ä¼˜åŒ–ï¼šä½¿ç”¨ç¼“å­˜ ====================
        if self._inference_mode and self._fused_item_cache is not None:
            return self._fused_item_cache
        # ==================== [END NEW 2025-01-17] ====================
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèï¼šw/o Prototype ====================
        if self.no_prototype:
            # æ¶ˆèæ¨¡å¼ï¼šä¸ä½¿ç”¨åŸå‹æœºåˆ¶ï¼Œç›´æ¥è¿”å› ID embedding
            item_emb = self.item_embeddings.weight  # [V+1, D]
            if self.fusion_mode == 'add':
                return item_emb
            else:
                # concat æ¨¡å¼éœ€è¦ä¿æŒç»´åº¦ä¸€è‡´
                r_dummy = torch.zeros_like(item_emb)  # [V+1, D]
                return torch.cat([item_emb, r_dummy], dim=-1)  # [V+1, 2D]
        # ==================== [END NEW 2025-01-17] ====================
        
        if self.fusion_mode == 'add':
            return self._get_fused_item_emb_add()  # åŠ æ³•èåˆï¼še_i + Î± * r_i
        else:
            return self._get_fused_item_emb_concat()  # æ‹¼æ¥èåˆï¼š[e_i, r_i]

    def _get_fused_item_emb_add(self):
        """
        åŠ æ³•èåˆï¼še_i + Î± * r_i
        
        ä¸ºæ¯ä¸ªç‰©å“è®¡ç®—ï¼šåŸå§‹ ID embedding + è¯­ä¹‰æƒé‡ Ã— åŸå‹åŠ æƒè¡¨ç¤º
        ç”¨äºæ¨ç†æ—¶ä¸ç”¨æˆ·è¡¨ç¤ºè®¡ç®—ç›¸ä¼¼åº¦
        """
        item_emb = self.item_embeddings.weight  # [V+1, D] è·å–æ‰€æœ‰ç‰©å“çš„ ID embeddingï¼ˆåŒ…æ‹¬ paddingï¼‰

        # ==================== [OLD] åŸå‹å¯»å€ï¼ˆå•å¤´ï¼Œå·²æ³¨é‡Šï¼‰====================
        # score_all = torch.matmul(item_emb, self.prototypes.t()) / self.temperature
        # pi_all = F.softmax(score_all, dim=-1)
        # r_all = torch.matmul(pi_all, self.prototypes)
        # r_all = r_all * self.macro_scale
        # ==================== [END OLD] ====================
        
        # ==================== [NEW-MultiHead] å¤šå¤´åŸå‹å¯»å€ ====================
        V = item_emb.size(0)  # ç‰©å“æ•°é‡ + 1ï¼ˆåŒ…æ‹¬ paddingï¼‰
        if self.num_heads_proto == 1:
            # å•å¤´æ¨¡å¼ï¼šæ ‡å‡†åŸå‹å¯»å€
            score_all = torch.matmul(item_emb, self.prototypes.t()) / self.temperature  # [V, K] æ¯ä¸ªç‰©å“ä¸æ¯ä¸ªåŸå‹çš„ç›¸ä¼¼åº¦
            pi_all = F.softmax(score_all, dim=-1)  # [V, K] æ¯ä¸ªç‰©å“çš„åŸå‹åˆ†å¸ƒ
            r_all = torch.matmul(pi_all, self.prototypes)  # [V, D] æ¯ä¸ªç‰©å“çš„åŠ æƒåŸå‹è¡¨ç¤º
        else:
            # å¤šå¤´æ¨¡å¼ï¼šå°† D ç»´åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ª head ç‹¬ç«‹å¯»å€
            item_heads = item_emb.view(V, self.num_heads_proto, self.head_dim)  # [V, H, d] åˆ‡åˆ†ç‰©å“åµŒå…¥
            proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d] åˆ‡åˆ†åŸå‹
            scores = torch.einsum('vhd,khd->vhk', item_heads, proto_heads) / self.temperature  # [V, H, K] æ¯ä¸ª head çš„ç›¸ä¼¼åº¦
            pi = F.softmax(scores, dim=-1)  # [V, H, K] æ¯ä¸ª head ç‹¬ç«‹ softmax
            r_heads = torch.einsum('vhk,khd->vhd', pi, proto_heads)  # [V, H, d] æ¯ä¸ª head çš„åŠ æƒåŸå‹
            r_all = r_heads.reshape(V, self.hidden_dim)  # [V, D] æ‹¼æ¥å›åŸå§‹ç»´åº¦
        r_all = r_all * self.macro_scale  # åº”ç”¨å¯å­¦ä¹ ç¼©æ”¾å› å­
        # ==================== [END NEW-MultiHead] ====================

        return item_emb + self.semantic_weight * r_all  # [V, D] åŠ æ³•èåˆï¼šID + Î±Ã—è¯­ä¹‰

    def _get_fused_item_emb_concat(self):
        """
        æ‹¼æ¥èåˆï¼š[e_i, r_i]
        
        ä¸ºæ¯ä¸ªç‰©å“è®¡ç®—ï¼š[åŸå§‹ ID embedding, åŸå‹åŠ æƒè¡¨ç¤º] æ‹¼æ¥
        è¾“å‡ºç»´åº¦æ˜¯ 2Dï¼Œç”¨äºæ¨ç†æ—¶ä¸ç”¨æˆ·è¡¨ç¤ºè®¡ç®—ç›¸ä¼¼åº¦
        """
        # self.item_embeddings æ˜¯ä¸€ä¸ª nn.Embedding å±‚ï¼Œé‡Œé¢æœ‰ä¸€ä¸ªå‚æ•°çŸ©é˜µï¼Œå½¢çŠ¶æ˜¯ï¼š[num_embeddings, embedding_dim]ï¼Œ[12102, 128]ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ä¸€ä¸ª item çš„å‘é‡è¡¨ç¤º
        # .weightï¼šæ‹¿åˆ°çš„å°±æ˜¯ è¿™æ•´ä¸ª embedding çŸ©é˜µæœ¬èº«
        #
        # æŠŠ Embedding å±‚é‡Œé‚£å¼  å®Œæ•´çš„ item embedding çŸ©é˜µ å–å‡ºæ¥
        item_emb = self.item_embeddings.weight  # [V+1, D] (12102,128) è·å–æ‰€æœ‰ç‰©å“çš„ ID embeddingï¼ˆåŒ…æ‹¬ paddingï¼‰

        # ==================== [OLD] åŸå‹å¯»å€ï¼ˆå•å¤´ï¼Œå·²æ³¨é‡Šï¼‰====================
        # score_all = torch.matmul(item_emb, self.prototypes.t()) / self.temperature
        # pi_all = F.softmax(score_all, dim=-1)
        # r_all = torch.matmul(pi_all, self.prototypes)
        # r_all = r_all * self.macro_scale
        # ==================== [END OLD] ====================
        
        # ==================== [NEW-MultiHead] å¤šå¤´åŸå‹å¯»å€ ====================
        V = item_emb.size(0)  # ç‰©å“æ•°é‡ + 1ï¼ˆåŒ…æ‹¬ paddingï¼‰12102
        if self.num_heads_proto == 1:
            # å•å¤´æ¨¡å¼ï¼šæ ‡å‡†åŸå‹å¯»å€
            score_all = torch.matmul(item_emb, self.prototypes.t()) / self.temperature  # [V, K] ç›¸ä¼¼åº¦åˆ†æ•°  (12102,128)@(128,64)â€”â€”>(12102,64)
            pi_all = F.softmax(score_all, dim=-1)  # [V, K] åŸå‹åˆ†å¸ƒ (12102,64)
            r_all = torch.matmul(pi_all, self.prototypes)  # [V, D] åŠ æƒåŸå‹è¡¨ç¤º  (12102,128)
        else:
            # å¤šå¤´æ¨¡å¼ï¼šå°† D ç»´åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ª head ç‹¬ç«‹å¯»å€
            item_heads = item_emb.view(V, self.num_heads_proto, self.head_dim)  # [V, H, d] åˆ‡åˆ†ç‰©å“åµŒå…¥
            proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d] åˆ‡åˆ†åŸå‹
            scores = torch.einsum('vhd,khd->vhk', item_heads, proto_heads) / self.temperature  # [V, H, K] æ¯ä¸ª head çš„ç›¸ä¼¼åº¦
            pi = F.softmax(scores, dim=-1)  # [V, H, K] æ¯ä¸ª head ç‹¬ç«‹ softmax
            r_heads = torch.einsum('vhk,khd->vhd', pi, proto_heads)  # [V, H, d] æ¯ä¸ª head çš„åŠ æƒåŸå‹
            r_all = r_heads.reshape(V, self.hidden_dim)  # [V, D] æ‹¼æ¥å›åŸå§‹ç»´åº¦
        r_all = r_all * self.macro_scale  # åº”ç”¨å¯å­¦ä¹ ç¼©æ”¾å› å­
        # ==================== [END NEW-MultiHead] ====================

        # æ‹¼æ¥ç‰©å“ä¾§çš„å¾®è§‚ä¸å®è§‚
        #
        # è¡¨ç¤º	    å˜é‡	            ç»´åº¦	        å«ä¹‰
        # å¾®è§‚	    item_emb        [V, D]	ç‰©å“çš„ ID embeddingï¼ˆå¯å­¦ä¹ ï¼Œç¼–ç ä¸ªä½“ç‰¹å¾ï¼‰
        # å®è§‚	    r_all	        [V, D]	ç‰©å“çš„ è¯­ä¹‰è¡¨ç¤ºï¼ˆåœ¨åŸå‹ç©ºé—´çš„è½¯åˆ†é…ç»“æœï¼‰
        #
        # ä¸¾ä¾‹
        # å‡è®¾ç‰©å“ ID=1234 æ˜¯ "iPhone 15 æ‰‹æœºå£³"ï¼š
        #
        # è¡¨ç¤º	                                    å†…å®¹
        # item_emb[1234]ï¼ˆå¾®è§‚ï¼‰	    è¯¥ç‰©å“çš„å”¯ä¸€ ID embeddingï¼Œç¼–ç "è¿™ä¸ªå…·ä½“çš„æ‰‹æœºå£³"
        # åŸå‹åˆ†å¸ƒ Ï€[1234]	        [æ‰‹æœºé…ä»¶: 0.6, è‹¹æœç”Ÿæ€: 0.3, ä¿æŠ¤å¥—: 0.1]
        # r_all[1234]ï¼ˆå®è§‚ï¼‰	    åŠ æƒåŸå‹ = "æ‰‹æœºé…ä»¶ç±» + è‹¹æœç”Ÿæ€ç±»" çš„è¯­ä¹‰è¡¨ç¤º
        return torch.cat([item_emb, r_all], dim=-1)  # [V, 2D] (12102,256) æ‹¼æ¥èåˆï¼š[ID embedding, è¯­ä¹‰è¡¨ç¤º]

    def forward(self, sequences):
        """
        å‰å‘ä¼ æ’­ï¼ˆé‡å†™åŸºç±»æ–¹æ³•ï¼‰    é‡å†™åï¼ŒProAlign å®Œå…¨ä½¿ç”¨è‡ªå·±çš„ forward()ï¼Œä¸ä¼šç”¨ SASRec_backbone çš„

        Args:
            sequences: [B, S] ç”¨æˆ·å†å²äº¤äº’åºåˆ—

        Returns:
            H_final: [B, D] (åŠ æ³•æ¨¡å¼) æˆ– [B, 2D] (æ‹¼æ¥æ¨¡å¼)
        """
        # è·å– ID åµŒå…¥
        inputs_emb = self.embed_ID(sequences) # (256,50,128)
        
        # ==================== [NEW-SLSI] åºåˆ—çº§è¯­ä¹‰æ³¨å…¥ ====================
        # æ ¸å¿ƒæ€æƒ³ï¼šåœ¨æ¯ä¸ªåºåˆ—ä½ç½®æ³¨å…¥è¯­ä¹‰ä¿¡æ¯ï¼Œè€Œéåªåœ¨æœ€åä¸€æ­¥
        # è®©è¯­ä¹‰ä¿¡æ¯å‚ä¸æ•´ä¸ªæ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹
        # [NEW 2025-01-17] å½“ no_prototype=True æ—¶è·³è¿‡ SLSIï¼ˆå› ä¸º SLSI ä¾èµ–åŸå‹æœºåˆ¶ï¼‰
        if self.use_slsi and not self.no_prototype:
            B, S, D = inputs_emb.shape
            
            # ==================== [NEW-SLSI-FIX 2024-12-16] Padding Mask ====================
            # æ ¸å¿ƒä¿®å¤ï¼šé˜²æ­¢ Padding ä½ç½®å‚ä¸åŸå‹å¯»å€
            # é—®é¢˜ï¼šå¦‚æœ inputs_emb åœ¨ padding ä½ç½®æ˜¯ 0ï¼Œsoftmax ä¼šç®—å‡ºå‡åŒ€åˆ†å¸ƒ (1/K)
            #       å¯¼è‡´ padding ä½ç½®è·å¾—"å¹³å‡è¯­ä¹‰å‘é‡"ï¼Œæ±¡æŸ“åç»­ Transformer è®¡ç®—
            # è§£å†³ï¼šæ˜¾å¼åˆ›å»º Maskï¼Œç¡®ä¿ Padding ä½ç½®çš„è¯­ä¹‰æ³¨å…¥ä¸º 0
            slsi_mask = torch.ne(sequences, self.item_num).float().unsqueeze(-1).to(self.device)  # [B, S, 1]
            # ==================== [END NEW-SLSI-FIX] ====================
            
            # ==================== [OLD] å•å¤´ SLSIï¼ˆå·²æ³¨é‡Šï¼‰====================
            # slsi_score = torch.matmul(inputs_emb, self.prototypes.t()) / self.temperature
            # slsi_pi = F.softmax(slsi_score, dim=-1)  # [B, S, K]
            # r_seq = torch.matmul(slsi_pi, self.prototypes)  # [B, S, D]
            # ==================== [END OLD] ====================
            
            # ==================== [NEW-SLSI-ContextAware] ä¸Šä¸‹æ–‡æ„ŸçŸ¥ SLSI ====================
            # é€‰æ‹© SLSI æ¨¡å¼ï¼šç‹¬ç«‹ä½ç½® vs ä¸Šä¸‹æ–‡æ„ŸçŸ¥
            if self.slsi_context_aware:
                # ==================== ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¨¡å¼ ====================
                # æ ¸å¿ƒæ€æƒ³ï¼šæ¯ä¸ªä½ç½®çš„è¯­ä¹‰æ³¨å…¥è€ƒè™‘å†å²ä¸Šä¸‹æ–‡
                # ä½¿ç”¨å› æœï¼ˆcausalï¼‰ç´¯ç§¯å‡å€¼ï¼šposition i åªçœ‹ [0, 1, ..., i]
                # è¿™æ ·ä¸ä¼šæ³„éœ²æœªæ¥ä¿¡æ¯
                
                # [NEW-SLSI-FIX] è®¡ç®—ç´¯ç§¯å’Œä¹‹å‰ï¼Œå…ˆç¡®ä¿ padding ä½ç½®æ˜¯ 0
                inputs_emb_masked = inputs_emb * slsi_mask  # åŒé‡ä¿é™©
                
                # Step 1: è®¡ç®—å› æœç´¯ç§¯å‡å€¼
                # cumsum[i] = sum(inputs_emb[0:i+1])
                # ç„¶åé™¤ä»¥ä½ç½®æ•°å¾—åˆ°å‡å€¼
                cumsum = torch.cumsum(inputs_emb_masked, dim=1)  # [B, S, D] ç´¯ç§¯å’Œ
                # ==================== [OLD] å›ºå®šä½ç½®æ•°é™¤æ³•ï¼ˆä¼šè¢« padding ç¨€é‡Šï¼‰====================
                # positions = torch.arange(1, S + 1, device=inputs_emb.device).float().view(1, S, 1)  # [1, S, 1]
                # context_repr = cumsum / positions  # [B, S, D] å› æœç´¯ç§¯å‡å€¼
                # ==================== [END OLD] ====================
                
                # ==================== [NEW 2024-12-17] æŒ‰ç´¯è®¡æœ‰æ•ˆ token æ•°é™¤ ====================
                # ä¿®å¤ï¼šå¦‚æœåºåˆ—ååŠæ®µæ˜¯ paddingï¼Œåº”è¯¥æŒ‰ç´¯è®¡æœ‰æ•ˆ token æ•°é™¤ï¼Œè€Œä¸æ˜¯å›ºå®šä½ç½®æ•°
                # è¿™æ · padding ä½ç½®ä¸ä¼š"ç¨€é‡Š"è¡¨ç¤º
                counts = torch.cumsum(slsi_mask, dim=1).clamp_min(1.0)  # [B, S, 1] ç´¯è®¡æœ‰æ•ˆ token æ•°
                context_repr = cumsum / counts  # [B, S, D] å› æœç´¯ç§¯å‡å€¼ï¼ˆæŒ‰æœ‰æ•ˆé•¿åº¦ï¼‰
                # ==================== [END NEW] ====================
                # context_repr[i] = mean(inputs_emb[0:i+1])ï¼ŒåªåŒ…å«å†å²å’Œå½“å‰ï¼Œä¸åŒ…å«æœªæ¥
                
                # Step 2: ç”¨ä¸Šä¸‹æ–‡è¡¨ç¤ºåšåŸå‹å¯»å€
                if self.num_heads_proto == 1:
                    # å•å¤´æ¨¡å¼
                    slsi_score = torch.matmul(context_repr, self.prototypes.t()) / self.temperature  # [B, S, K]
                    slsi_pi = F.softmax(slsi_score, dim=-1)  # [B, S, K]
                    r_seq = torch.matmul(slsi_pi, self.prototypes)  # [B, S, D]
                else:
                    # å¤šå¤´æ¨¡å¼
                    context_heads = context_repr.view(B, S, self.num_heads_proto, self.head_dim)  # [B, S, H, d]
                    proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d]
                    slsi_scores = torch.einsum('bshd,khd->bshk', context_heads, proto_heads) / self.temperature  # [B, S, H, K]
                    slsi_pi = F.softmax(slsi_scores, dim=-1)  # [B, S, H, K]
                    r_seq_heads = torch.einsum('bshk,khd->bshd', slsi_pi, proto_heads)  # [B, S, H, d]
                    r_seq = r_seq_heads.reshape(B, S, self.hidden_dim)  # [B, S, D]
            else:
                # ==================== ç‹¬ç«‹ä½ç½®æ¨¡å¼ï¼ˆåŸå§‹ SLSIï¼‰====================
                # æ¯ä¸ªä½ç½®ç‹¬ç«‹åšåŸå‹å¯»å€ï¼Œä¸è€ƒè™‘ä¸Šä¸‹æ–‡
                if self.num_heads_proto == 1:
                    # å•å¤´æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                    # è®¡ç®—æ¯ä¸ªä½ç½®çš„åŸå‹ç›¸ä¼¼åº¦
                    #
                    # inputs_emb: [B, S, D]ï¼Œæ¯ä¸ªä½ç½®çš„ ID embedding
                    # self.prototypes: [K, D]ï¼ŒK ä¸ªåŸå‹ï¼Œæ¯ä¸ª D ç»´
                    # ç»™åºåˆ—ä¸­æ¯ä¸ªä½ç½®ï¼Œç®—ä¸€éå®ƒå¯¹æ‰€æœ‰ K ä¸ªåŸå‹çš„ç›¸ä¼¼åº¦
                    slsi_score = torch.matmul(inputs_emb, self.prototypes.t()) / self.temperature  # [B,S,D]  @ [D,K] â€”â€”> [B, S, K]  (256,50,64)
                    # Softmax å½’ä¸€åŒ–  ç»™åºåˆ—ä¸­æ¯ä¸ªä½ç½®ï¼Œç®—ä¸€éå®ƒå¯¹æ‰€æœ‰ K ä¸ªåŸå‹çš„ç›¸ä¼¼åº¦
                    slsi_pi = F.softmax(slsi_score, dim=-1)  # [B, S, K]  (256,50,64)
                    # åŠ æƒåŸå‹ï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®çš„è¯­ä¹‰è¡¨ç¤º
                    r_seq = torch.matmul(slsi_pi, self.prototypes)  # [B, S, D]   (256,50,128)
                else:
                    # å¤šå¤´ SLSI
                    inputs_heads = inputs_emb.view(B, S, self.num_heads_proto, self.head_dim)  # [B, S, H, d]
                    proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d]
                    # [B, S, H, d] @ [K, H, d] -> [B, S, H, K]
                    slsi_scores = torch.einsum('bshd,khd->bshk', inputs_heads, proto_heads) / self.temperature
                    slsi_pi = F.softmax(slsi_scores, dim=-1)  # [B, S, H, K]
                    # [B, S, H, K] @ [K, H, d] -> [B, S, H, d]
                    r_seq_heads = torch.einsum('bshk,khd->bshd', slsi_pi, proto_heads)
                    r_seq = r_seq_heads.reshape(B, S, self.hidden_dim)  # [B, S, D]
            # ==================== [END NEW-SLSI-ContextAware] ====================
            
            # ==================== [NEW-SLSI-FIX 2024-12-16] å…³é”®ä¿®å¤ ====================
            # æ³¨å…¥è¯­ä¹‰åï¼Œç«‹å³ Maskï¼Œç¡®ä¿ Padding ä½ç½®çš„è¯­ä¹‰æ³¨å…¥ä¸º 0
            # è¿™æ ·åé¢çš„ Transformer å³ä½¿æœ‰ LayerNorm ä¹Ÿä¸ä¼šå—å™ªå£°å½±å“
            r_seq = r_seq * slsi_mask
            # ==================== [END NEW-SLSI-FIX] ====================
            
            # è¯­ä¹‰å¢å¼ºï¼šåŸå§‹åµŒå…¥ + åŠ æƒè¯­ä¹‰è¡¨ç¤º
            inputs_emb = inputs_emb + self.slsi_weight * r_seq # (256,50,128)
        # ==================== [END NEW-SLSI] ====================
        
        inputs_emb += self.positional_embeddings(torch.arange(self.seq_len).to(self.device)) # æ·»åŠ ä½ç½®ç¼–ç ï¼š[B,S,D] + [S,D] â†’ [B,S,D]ï¼Œè®©æ¨¡å‹çŸ¥é“æ¯ä¸ªç‰©å“åœ¨åºåˆ—ä¸­çš„ä½ç½®
        seq = self.emb_dropout(inputs_emb)  # Dropout æ­£åˆ™åŒ–ï¼šéšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        mask = torch.ne(sequences, self.item_num).float().unsqueeze(-1).to(self.device)  # åˆ›å»ºæ©ç ï¼šæ ‡è®°é padding ä½ç½®ä¸º 1ï¼Œpadding ä½ç½®ä¸º 0ï¼Œshape: [B,S,1]   (256,50,1)
        seq *= mask  # åº”ç”¨æ©ç ï¼šå°† padding ä½ç½®çš„åµŒå…¥ç½®é›¶
        seq_normalized = self.ln_1(seq)  # Layer Normalizationï¼šæ ‡å‡†åŒ–è¾“å…¥ï¼ŒåŠ é€Ÿè®­ç»ƒæ”¶æ•›
        mh_attn_out = self.mh_attn(seq_normalized, seq)  # å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼šæ•è·åºåˆ—ä¸­ç‰©å“ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œè¾“å‡º [B,S,D]
        ff_out = self.feed_forward(self.ln_2(mh_attn_out))  # å‰é¦ˆç½‘ç»œï¼šä¸¤å±‚å…¨è¿æ¥ + ReLUï¼Œå¢åŠ æ¨¡å‹è¡¨è¾¾èƒ½åŠ›
        ff_out *= mask  # å†æ¬¡åº”ç”¨æ©ç ï¼šç¡®ä¿ padding ä½ç½®ä¸äº§ç”Ÿè¾“å‡º
        ff_out = self.ln_3(ff_out)  # æœ€åä¸€å±‚ Layer Normalization   (256,50,128)
        h_u = ff_out[:, -1, :]  # [B, D] (256,128)  å¾®è§‚çŠ¶æ€ï¼šå–åºåˆ—æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºä½œä¸ºç”¨æˆ·è¡¨ç¤º

        # ==================== [OLD] åŸå‹å¯»å€ï¼ˆå•å¤´ï¼Œå·²æ³¨é‡Šï¼‰====================
        # score_stu = torch.matmul(h_u, self.prototypes.t()) / self.temperature
        # pi_stu = F.softmax(score_stu, dim=-1)  # [B, K]
        # r_u = torch.matmul(pi_stu, self.prototypes)  # [B, D] å®è§‚æ„å›¾
        # r_u = r_u * self.macro_scale
        # ==================== [END OLD] ====================
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèï¼šw/o Prototype ====================
        # å½“ no_prototype=True æ—¶ï¼Œè·³è¿‡åŸå‹å¯»å€ï¼Œç›´æ¥è¿”å› h_u
        if self.no_prototype:
            # æ¶ˆèæ¨¡å¼ï¼šä¸ä½¿ç”¨åŸå‹æœºåˆ¶ï¼Œç›´æ¥è¿”å› h_u
            if self.fusion_mode == 'add':
                H_final = h_u  # [B, D]
            else:
                # concat æ¨¡å¼éœ€è¦ä¿æŒç»´åº¦ä¸€è‡´ï¼Œç”¨é›¶å‘é‡å¡«å…… r_u çš„ä½ç½®
                r_u_dummy = torch.zeros_like(h_u)  # [B, D]
                H_final = torch.cat([h_u, r_u_dummy], dim=-1)  # [B, 2D]
            return H_final
        # ==================== [END NEW 2025-01-17] ====================
        
        # ==================== [NEW-MultiHead] å¤šå¤´è§£è€¦åŸå‹å¯»å€ ====================
        # æ ¸å¿ƒæ€æƒ³ï¼šå°† D ç»´åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ª head ç‹¬ç«‹å¯»å€åŸå‹
        # è¿™æ ·ä¸åŒæ„å›¾æ–¹å‘ä¸ä¼šè¢«"å¹³å‡åŒ–"æŠµæ¶ˆ
        B = h_u.size(0)
        
        # ==================== [NEW 2024-12-31] åŠ¨æ€æ³¨æ„åŠ›èåˆ ====================
        # ä¸ ProAlign_BERT4Rec ä¿æŒä¸€è‡´
        if self.use_attn_fusion and hasattr(self, 'proto_attn'):
            # ç”¨æˆ·çŠ¶æ€ä½œä¸º Queryï¼Œåœ¨åŸå‹åº“ä¸­åŠ¨æ€æ£€ç´¢æœ€åŒ¹é…çš„æ„å›¾
            query = h_u.unsqueeze(1)  # [B, 1, D]
            keys = self.prototypes.unsqueeze(0).expand(B, -1, -1)  # [B, K, D]

            # Attention: r_dynamic = softmax(QÂ·K^T / sqrt(d)) Â· V
            r_dynamic, _ = self.proto_attn(query, keys, keys)
            r_u = r_dynamic.squeeze(1)  # [B, D]
        # ==================== [END NEW 2024-12-31] ====================
        elif self.num_heads_proto == 1:
            # å•å¤´æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
            score_stu = torch.matmul(h_u, self.prototypes.t()) / self.temperature # (256,64)@(64,128)â€”â€”>(256,64)
            pi_stu = F.softmax(score_stu, dim=-1)  # [B, K]  (256,64)
            r_u = torch.matmul(pi_stu, self.prototypes)  # [B, D]  (256,128)
        else:
            # å¤šå¤´æ¨¡å¼
            # 1. å°† h_u å’Œ prototypes åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´
            h_u_heads = h_u.view(B, self.num_heads_proto, self.head_dim)  # [B, H, d]
            proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d]
            
            # 2. æ¯ä¸ª head ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›åˆ†å¸ƒ
            # einsum: 'bhd,khd->bhk' è¡¨ç¤ºæ¯ä¸ª head å†…åšç‚¹ç§¯
            scores = torch.einsum('bhd,khd->bhk', h_u_heads, proto_heads) / self.temperature  # [B, H, K]
            pi = F.softmax(scores, dim=-1)  # [B, H, K] æ¯ä¸ª head ç‹¬ç«‹ softmax
            
            # 3. æ¯ä¸ª head ç‹¬ç«‹åŠ æƒåŸå‹
            # einsum: 'bhk,khd->bhd' è¡¨ç¤ºæ¯ä¸ª head ç”¨è‡ªå·±çš„ pi åŠ æƒåŸå‹
            r_u_heads = torch.einsum('bhk,khd->bhd', pi, proto_heads)  # [B, H, d]
            
            # 4. æ‹¼æ¥å› [B, D]
            r_u = r_u_heads.reshape(B, self.hidden_dim)  # [B, D]
        
        r_u = r_u * self.macro_scale # (256,128)
        # ==================== [END NEW-MultiHead] ====================

        # ==================== èåˆï¼šå°†å¾®è§‚çŠ¶æ€ h_u å’Œå®è§‚æ„å›¾ r_u ç»„åˆ ====================
        if self.fusion_mode == 'add':  # åŠ æ³•èåˆæ¨¡å¼
            H_final = h_u + self.semantic_weight * r_u  # [B, D]ï¼Œç›´æ¥åŠ æƒç›¸åŠ ï¼šç”¨æˆ·è¡¨ç¤º = å¾®è§‚ + Î±Ã—å®è§‚
        else:  # concat æ‹¼æ¥èåˆæ¨¡å¼
            concat_feat = torch.cat([h_u, r_u], dim=-1)  # [B, 2D] (256,256)ï¼Œå…ˆæ‹¼æ¥ä¸¤ä¸ªè¡¨ç¤º
            g = self.gate(concat_feat)  # [B, 1] (256,1)ï¼Œé—¨æ§ç½‘ç»œè®¡ç®—è‡ªé€‚åº”æƒé‡ g âˆˆ (0,1)
            # ä¸¾ä¾‹è¯´æ˜
            # å‡è®¾ç”¨æˆ·å†å²ï¼š[iPhoneæ‰‹æœºå£³, AirPods, MacBookä¿æŠ¤å¥—, iPadè§¦æ§ç¬”]
            #
            # è¡¨ç¤º	        å†…å®¹
            # h_uï¼ˆå¾®è§‚ï¼‰	ç¼–ç äº†"ç”¨æˆ·ä¹°äº†è¿™ 4 ä¸ªå…·ä½“ç‰©å“"
            # åŸå‹åˆ†å¸ƒ Ï€	    [ç”µå­é…ä»¶: 0.7, æ•°ç äº§å“: 0.2, å…¶ä»–: 0.1]
            # r_uï¼ˆå®è§‚ï¼‰	0.7Ã—ç”µå­é…ä»¶åŸå‹ + 0.2Ã—æ•°ç åŸå‹ + ... = "è‹¹æœç”Ÿæ€é…ä»¶çˆ±å¥½è€…"
            H_final = torch.cat([h_u, g * r_u], dim=-1)  # [B, 2D] (256,256)ï¼Œç”¨é—¨æ§å€¼åŠ æƒå®è§‚æ„å›¾åæ‹¼æ¥

        return H_final  # è¿”å›æœ€ç»ˆç”¨æˆ·è¡¨ç¤ºï¼šåŠ æ³•æ¨¡å¼ [B,D]ï¼Œæ‹¼æ¥æ¨¡å¼ [B,2D]

    def predict(self, sequences):
        """æ¨ç†é¢„æµ‹ï¼ˆå…¼å®¹åŸºç±»æ¥å£ï¼‰ï¼šç»™å®šç”¨æˆ·åºåˆ—ï¼Œè¿”å›æ‰€æœ‰ç‰©å“çš„é¢„æµ‹åˆ†æ•°"""
        H_final = self.forward(sequences)  # å‰å‘ä¼ æ’­ï¼šè·å–ç”¨æˆ·æœ€ç»ˆè¡¨ç¤º [B, D] æˆ– [B, 2D]  (256,256)
        item_embs = self.return_item_emb()  # è·å–æ‰€æœ‰ç‰©å“çš„èåˆåµŒå…¥ [V+1, D] æˆ– [V+1, 2D] (12102,256)

        # å»æ‰ padding embeddingï¼ˆæœ€åä¸€è¡Œæ˜¯ padding çš„åµŒå…¥å‘é‡ï¼‰
        if self.fusion_mode == 'add':
            item_embs = item_embs[:-1]  # [V, D]ï¼Œå»æ‰ç¬¬ V+1 è¡Œï¼ˆpaddingï¼‰
        else:
            item_embs = item_embs[:-1]  # [V, 2D]  (12101,256)ï¼Œå»æ‰ç¬¬ V+1 è¡Œï¼ˆpaddingï¼‰

        scores = torch.matmul(H_final, item_embs.t())  # [B, V]ï¼Œ(256,12101)  è®¡ç®—ç”¨æˆ·ä¸æ‰€æœ‰ç‰©å“çš„ç›¸ä¼¼åº¦åˆ†æ•°
        return scores  # è¿”å›é¢„æµ‹åˆ†æ•°çŸ©é˜µï¼Œç”¨äºæ’åºæ¨è

    def calculate_loss_with_align(self, sequences, target, user_ids, neg_ratio, temperature):
        """
        è®¡ç®—å¸¦å¯¹é½æŸå¤±çš„æ€»æŸå¤±ï¼ˆProAlign ä¸“ç”¨ï¼‰
        
        Args:
            sequences: [B, S] ç”¨æˆ·å†å²äº¤äº’åºåˆ—
            target: [B] ç›®æ ‡ç‰©å“ IDï¼ˆæ­£æ ·æœ¬ï¼‰
            user_ids: [B] ç”¨æˆ· IDï¼ˆç”¨äºè·å–ç”¨æˆ·æ„å›¾ embeddingï¼‰
            neg_ratio: int è´Ÿé‡‡æ ·æ¯”ä¾‹ï¼ˆæ¯ä¸ªæ­£æ ·æœ¬é‡‡å¤šå°‘è´Ÿæ ·æœ¬ï¼‰
            temperature: float InfoNCE æ¸©åº¦å‚æ•°

        Returns:
            loss: L = L_rec + Î± * L_align + Î² * L_cluster
        """
        H_final = self.forward(sequences)  # å‰å‘ä¼ æ’­ï¼šè·å–ç”¨æˆ·æœ€ç»ˆè¡¨ç¤º [B, D] æˆ– [B, 2D] (256,256)

        # ==================== L_rec: ä¸»æ¨èæŸå¤±ï¼ˆInfoNCEï¼‰====================
        item_embs = self.return_item_emb()  # è·å–æ‰€æœ‰ç‰©å“çš„èåˆåµŒå…¥ (12102,256)

        # æ­£æ ·æœ¬ï¼šè·å–ç›®æ ‡ç‰©å“çš„èåˆåµŒå…¥
        if self.fusion_mode == 'add':
            pos_embs = self._get_target_fused_emb_add(target)  # [B, D]
        else:
            pos_embs = self._get_target_fused_emb_concat(target)  # [B, 2D]   (256,)â€”â€”>(256,256)  æ­£æ ·æœ¬åµŒå…¥

        # è´Ÿé‡‡æ ·ï¼šä¸ºæ¯ä¸ªæ ·æœ¬éšæœºé‡‡æ · neg_ratio ä¸ªè´Ÿæ ·æœ¬
        batch_size = target.shape[0]  # æ‰¹å¤§å° B
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))  # [B, neg_ratio] (256,64) éšæœºé‡‡æ ·
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()  # [B, neg_ratio] (256,64) æ‰©å±•æ­£æ ·æœ¬
        mask = neg_samples == expanded_target  # æ£€æŸ¥è´Ÿæ ·æœ¬æ˜¯å¦ä¸æ­£æ ·æœ¬é‡å¤
        while mask.any():  # å¦‚æœæœ‰é‡å¤ï¼Œé‡æ–°é‡‡æ ·
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))  # é‡æ–°é‡‡æ ·
            neg_samples = torch.where(mask, new_samples, neg_samples)  # åªæ›¿æ¢é‡å¤çš„ä½ç½®
            mask = neg_samples == expanded_target  # é‡æ–°æ£€æŸ¥
        neg_samples = neg_samples.to(target.device)  # ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡

        # è·å–è´Ÿæ ·æœ¬çš„èåˆåµŒå…¥
        if self.fusion_mode == 'add':
            neg_embs = self._get_target_fused_emb_add(neg_samples)  # [B, neg_ratio, D]
        else:
            neg_embs = self._get_target_fused_emb_concat(neg_samples)  # [B, neg_ratio, 2D] (256,64,256)

        # L2 å½’ä¸€åŒ–ï¼šå°†å‘é‡æŠ•å½±åˆ°å•ä½è¶…çƒé¢ï¼Œä½¿ç‚¹ç§¯ç­‰ä»·äºä½™å¼¦ç›¸ä¼¼åº¦
        H_final_norm = F.normalize(H_final, p=2, dim=-1)  # [B, D] æˆ– [B, 2D]   (256,256)
        pos_embs_norm = F.normalize(pos_embs, p=2, dim=-1)  # [B, D] æˆ– [B, 2D]  (256,256)
        neg_embs_norm = F.normalize(neg_embs, p=2, dim=-1)  # [B, neg_ratio, D] æˆ– [B, neg_ratio, 2D]  (256,64,256)

        # InfoNCE æŸå¤±è®¡ç®—
        pos_logits = (H_final_norm * pos_embs_norm).sum(dim=-1, keepdim=True)  # [B, 1] æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        neg_logits = torch.bmm(neg_embs_norm, H_final_norm.unsqueeze(-1)).squeeze(-1)  # [B, neg_ratio] è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        logits = torch.cat([pos_logits, neg_logits], dim=-1) / temperature  # [B, 1+neg_ratio] (256,65) æ‹¼æ¥å¹¶é™¤ä»¥æ¸©åº¦
        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)  # [B] (256,)æ ‡ç­¾å…¨ä¸º 0ï¼ˆæ­£æ ·æœ¬åœ¨ç¬¬ 0 ä½ï¼‰
        L_rec = F.cross_entropy(logits, labels)  # äº¤å‰ç†µæŸå¤±

        # ==================== L_align: æ„å›¾å¯¹é½æŸå¤± ====================
        # é€šè¿‡ --align_mode å‚æ•°é€‰æ‹©å¯¹é½æ–¹å¼ï¼š
        #   'kl'     : åŸå‹åˆ†å¸ƒå¯¹é½ï¼ˆKL æ•£åº¦ï¼‰- è®© h_u å’Œ z_next åœ¨åŸå‹ç©ºé—´çš„åˆ†å¸ƒä¸€è‡´
        #   'infonce': è·¨è§†å›¾å¯¹æ¯”å­¦ä¹ ï¼ˆInfoNCEï¼‰- ç›´æ¥æ‹‰è¿‘ h_u å’Œ z_next çš„å‘é‡è¡¨ç¤º
        L_align = torch.tensor(0.0, device=sequences.device)  # åˆå§‹åŒ–å¯¹é½æŸå¤±ä¸º 0
        align_mode = self.key_words.get('align_mode', 'infonce')  # è·å–å¯¹é½æ¨¡å¼ï¼Œé»˜è®¤ InfoNCE
        cl_temperature = self.key_words.get('cl_temperature', 1.0)  # å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°

        # åªæœ‰å½“ç”¨æˆ·æ„å›¾ embedding å­˜åœ¨ä¸”æœ‰ user_ids æ—¶æ‰è®¡ç®—å¯¹é½æŸå¤±
        if self.user_intent_emb is not None and user_ids is not None:
            z_next = self.user_intent_emb[user_ids.cpu()].to(sequences.device)  # [256, 3072] è·å–ç”¨æˆ·æœªæ¥æ„å›¾ embedding
            z_next_proj = self.adapter(z_next)  # [256, 128] é€šè¿‡ Adapter é™ç»´ï¼š3072 â†’ 128

            # æå– h_uï¼ˆå¾®è§‚çŠ¶æ€ï¼‰ï¼šä» H_final ä¸­å–å‰ hidden_dim ç»´
            h_u = H_final[:, :self.hidden_dim]  # [256, 128]

            # L2 å½’ä¸€åŒ–ï¼šç”¨äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            h_u_norm = F.normalize(h_u, p=2, dim=-1)  # [256, 128] å½’ä¸€åŒ–åæ¨¡é•¿=1
            z_next_norm = F.normalize(z_next_proj, p=2, dim=-1)  # [256, 128] å½’ä¸€åŒ–åæ¨¡é•¿=1

            if align_mode == 'kl':
                # ==================== æ–¹æ¡ˆ Aï¼šåŸå‹åˆ†å¸ƒå¯¹é½ï¼ˆKL æ•£åº¦ï¼‰ ====================
                # æ€è·¯ï¼šè®© h_u å’Œ z_next åœ¨åŸå‹ç©ºé—´çš„åˆ†å¸ƒä¸€è‡´
                proto_norm = F.normalize(self.prototypes, p=2, dim=-1)  # [K, D] å½’ä¸€åŒ–åŸå‹

                # å­¦ç”Ÿåˆ†å¸ƒï¼ˆh_u åœ¨åŸå‹ç©ºé—´çš„è½¯åˆ†é…ï¼‰
                score_stu = torch.matmul(h_u_norm, proto_norm.t()) / self.temperature  # [B, K] å­¦ç”Ÿåˆ†æ•°

                # æ•™å¸ˆåˆ†å¸ƒï¼ˆz_next åœ¨åŸå‹ç©ºé—´çš„è½¯åˆ†é…ï¼‰
                score_tea = torch.matmul(z_next_norm, proto_norm.t()) / self.temperature  # [B, K] æ•™å¸ˆåˆ†æ•°
                pi_tea = F.softmax(score_tea, dim=-1)  # [B, K] æ•™å¸ˆè½¯åˆ†å¸ƒï¼ˆç›®æ ‡åˆ†å¸ƒï¼‰

                # KL æ•£åº¦ï¼šåº¦é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚ï¼Œè®©å­¦ç”Ÿåˆ†å¸ƒé€¼è¿‘æ•™å¸ˆåˆ†å¸ƒ
                log_pi_stu = F.log_softmax(score_stu, dim=-1)  # [B, K] å­¦ç”Ÿ log æ¦‚ç‡
                L_align = F.kl_div(log_pi_stu, pi_tea, reduction='batchmean')  # KL(å­¦ç”Ÿ || æ•™å¸ˆ)

            elif align_mode == 'infonce':
                # # ==================== [NEW] Forward Prediction Contrastive Learning ====================
                # # æ ¸å¿ƒï¼šè®©æ¨¡å‹"å­¦ä¼šé¢„æµ‹æœªæ¥æ„å›¾"ï¼Œè€Œä¸æ˜¯ç®€å•èåˆ
                # #
                # # h_u:     ID-based user representation (what user has done)
                # # h_pred:  Predicted future intent (what model thinks user will want)
                # # z_next:  LLM-based future intent (ground truth from LLM)
                # #
                # # h_u â†’ Predictor â†’ h_pred, then InfoNCE(h_pred, z_next)
                #
                # if self.use_forward_predictor:
                #     # [NEW] Forward Prediction: h_u â†’ Predictor â†’ h_pred
                #     h_pred = self.forward_predictor(h_u)  # [B, D]
                #     h_pred_norm = F.normalize(h_pred, p=2, dim=-1)
                #     query_norm = h_pred_norm
                # else:
                #     # [OLD] Direct alignment (ç”¨äºæ¶ˆèå®éªŒå¯¹æ¯”)
                #     query_norm = h_u_norm
                #
                # batch_size_align = query_norm.size(0)
                #
                # # è·¨è§†å›¾ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]
                # #         zâ‚€    zâ‚    zâ‚‚
                # # queryâ‚€ [  â˜…     Â·     Â·  ]   label=0
                # # queryâ‚ [  Â·     â˜…     Â·  ]   label=1
                # # queryâ‚‚ [  Â·     Â·     â˜…  ]   label=2
                # # â˜… = æ­£æ ·æœ¬ï¼ˆå¯¹è§’çº¿ï¼‰ï¼ŒÂ· = è´Ÿæ ·æœ¬
                # sim_matrix = torch.matmul(query_norm, z_next_norm.t()) / cl_temperature
                #
                # # æ­£æ ·æœ¬åœ¨å¯¹è§’çº¿ä¸Š
                # labels_align = torch.arange(batch_size_align, device=sequences.device)
                #
                # # åŒå‘å¯¹æ¯”
                # loss_h2z = F.cross_entropy(sim_matrix, labels_align)      # query â†’ z_next
                # loss_z2h = F.cross_entropy(sim_matrix.t(), labels_align)  # z_next â†’ query
                #
                # L_align = (loss_h2z + loss_z2h) / 2

                # ==================== [OLD] ä»¥ä¸‹ä¸ºæ—§ç‰ˆç›´æ¥å¯¹é½ä»£ç ï¼ˆå·²æ³¨é‡Šä¿ç•™ï¼‰====================
                # æ–¹æ¡ˆ Bï¼šè·¨è§†å›¾ InfoNCE
                # æ€è·¯ï¼šç›´æ¥æ‹‰è¿‘ h_u å’Œ z_next çš„å‘é‡è¡¨ç¤º
                # æ­£æ ·æœ¬ï¼š(h_u[i], z_next[i]) - åŒä¸€ç”¨æˆ·çš„ä¸¤ç§è¡¨ç¤º
                # è´Ÿæ ·æœ¬ï¼šbatch å†…å…¶ä»–ç”¨æˆ·
                #
                # sim_matrix [B, B]:
                #         zâ‚€    zâ‚    zâ‚‚
                # hâ‚€  [  â˜…     Â·     Â·  ]   label=0
                # hâ‚  [  Â·     â˜…     Â·  ]   label=1
                # hâ‚‚  [  Â·     Â·     â˜…  ]   label=2
                # â˜… = æ­£æ ·æœ¬ï¼ˆå¯¹è§’çº¿ï¼‰ï¼ŒÂ· = è´Ÿæ ·æœ¬

                batch_size_align = h_u_norm.size(0)  # æ‰¹å¤§å°  256

                # è·¨è§†å›¾ç›¸ä¼¼åº¦çŸ©é˜µ [B, B]ï¼šè®¡ç®—æ‰€æœ‰ h_u å’Œ z_next ä¹‹é—´çš„ç›¸ä¼¼åº¦
                sim_matrix = torch.matmul(h_u_norm, z_next_norm.t()) / cl_temperature  # [256, 256] ç›¸ä¼¼åº¦çŸ©é˜µ

                # ç›¸ä¼¼åº¦çŸ©é˜µ sim_matrix çš„ç¤ºæ„ï¼š
                # å‡è®¾ batch_size = 4ï¼Œh_u å’Œ z_next çš„é¡ºåºä¸€ä¸€å¯¹åº”ï¼š
                #
                #   sim_matrix =
                #   [
                #     [(h_0, z_0), (h_0, z_1), (h_0, z_2), (h_0, z_3)],
                #     [(h_1, z_0), (h_1, z_1), (h_1, z_2), (h_1, z_3)],
                #     [(h_2, z_0), (h_2, z_1), (h_2, z_2), (h_2, z_3)],
                #     [(h_3, z_0), (h_3, z_1), (h_3, z_2), (h_3, z_3)],
                #   ]
                #
                # - ç¬¬ 0 è¡Œï¼šh_0 å’Œæ‰€æœ‰ z çš„ç›¸ä¼¼åº¦ -> æ­£æ ·æœ¬æ˜¯ (h_0, z_0) -> åœ¨ç¬¬ 0 åˆ—
                # - ç¬¬ 1 è¡Œï¼šh_1 å’Œæ‰€æœ‰ z çš„ç›¸ä¼¼åº¦ -> æ­£æ ·æœ¬æ˜¯ (h_1, z_1) -> åœ¨ç¬¬ 1 åˆ—
                # - ç¬¬ 2 è¡Œï¼šh_2 å’Œæ‰€æœ‰ z çš„ç›¸ä¼¼åº¦ -> æ­£æ ·æœ¬æ˜¯ (h_2, z_2) -> åœ¨ç¬¬ 2 åˆ—
                # - ç¬¬ 3 è¡Œï¼šh_3 å’Œæ‰€æœ‰ z çš„ç›¸ä¼¼åº¦ -> æ­£æ ·æœ¬æ˜¯ (h_3, z_3) -> åœ¨ç¬¬ 3 åˆ—
                #
                # æ‰€ä»¥ï¼šæ­£æ ·æœ¬å¯¹ (h_i, z_i) æ­£å¥½å°±æ˜¯ sim_matrix çš„å¯¹è§’çº¿å…ƒç´ 


                # æ­£æ ·æœ¬åœ¨å¯¹è§’çº¿ä¸Šï¼šç¬¬ i ä¸ªç”¨æˆ·çš„ h_u[i] åº”è¯¥ä¸ z_next[i] æœ€ç›¸ä¼¼
                labels_align = torch.arange(batch_size_align, device=sequences.device)  # [0, 1, 2, ..., B-1]

                # åŒå‘å¯¹æ¯”æŸå¤±ï¼š
                loss_h2z = F.cross_entropy(sim_matrix, labels_align)  # h_u â†’ z_nextï¼šç»™å®š h_u[i]ï¼Œæ‰¾å¯¹åº”çš„ z_next[i]
                loss_z2h = F.cross_entropy(sim_matrix.t(), labels_align)  # z_next â†’ h_uï¼šç»™å®š z_next[i]ï¼Œæ‰¾å¯¹åº”çš„ h_u[i]

                L_align = (loss_h2z + loss_z2h) / 2  # å–å¹³å‡ä½œä¸ºæœ€ç»ˆå¯¹é½æŸå¤±

        # ==================== L_cluster: èšç±»æ­£åˆ™åŒ–æŸå¤± ====================
        # ç›®çš„ï¼šè®©ç›®æ ‡ç‰©å“çš„ ID embedding é è¿‘å…¶å¯¹åº”çš„åŸå‹è¡¨ç¤º
        # è¿™ä¿ƒä½¿ ID embedding å­¦ä¹ åˆ°ä¸åŸå‹ä¸€è‡´çš„è¯­ä¹‰ç»“æ„

        # ä¸ºä»€ä¹ˆåŠ ä¸Šè¿™ä¸ªæŸå¤±å‡½æ•°
        # é—®é¢˜ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ ID Embedding ä¼š"å¤±å»è¯­ä¹‰"
        # åˆå§‹åŒ–æ—¶ï¼ˆsemantic_init=Trueï¼‰ï¼š
        #   - æŠ¤è‚¤å“A çš„ embedding é è¿‘ æŠ¤è‚¤å“B
        #   - æ‰‹æœºå£³ çš„ embedding é è¿‘ æ‰‹æœºè†œ
        #   âœ… åŒç±»ç‰©å“çš„ embedding ç›¸ä¼¼ï¼ˆå› ä¸ºæ¥è‡ª LLMï¼‰
        #
        # è®­ç»ƒ 500 epochs åï¼š
        #   - åªæœ‰ L_rec æŸå¤±ï¼ˆæ¨èæŸå¤±ï¼‰
        #   - ä¼˜åŒ–ç›®æ ‡ï¼šè®©ç”¨æˆ·å–œæ¬¢çš„ç‰©å“å¾—åˆ†é«˜
        #   - ç»“æœï¼šembedding ä¸ºäº†"æ‹Ÿåˆè®­ç»ƒæ•°æ®"è€Œç§»åŠ¨
        #   âŒ å¯èƒ½å¯¼è‡´æŠ¤è‚¤å“A å’Œ æŠ¤è‚¤å“B çš„ embedding ä¸å†ç›¸ä¼¼ï¼
        #
        #
        # L_cluster çš„ä½œç”¨ï¼šé˜²æ­¢è¯­ä¹‰ç»“æ„è¢«ç ´å
        # L_cluster = MSE(e_target, r_target)
        #
        # e_target = ç‰©å“å½“å‰çš„ embeddingï¼ˆå¯èƒ½å·²ç»åç¦»ï¼‰
        # r_target = ç‰©å“"åº”è¯¥åœ¨"çš„ä½ç½®ï¼ˆæ ¹æ® LLM è¯­ä¹‰ï¼‰
        # L_cluster = æƒ©ç½šä¸¤è€…çš„è·ç¦»
        # å°±åƒå¼¹ç°§ä¸€æ ·ï¼ŒæŠŠ embedding æ‹‰å›åˆ°"è¯­ä¹‰æ­£ç¡®"çš„ä½ç½®
        #
        #
        # ç±»æ¯”
        # æƒ³è±¡ä½ åœ¨è®­ç»ƒä¸€ä¸ªæ¨èç³»ç»Ÿï¼š
        #
        # æ²¡æœ‰ L_cluster	æœ‰ L_cluster
        # åªå…³å¿ƒ"æ¨èå‡†ä¸å‡†"	        åŒæ—¶å…³å¿ƒ"æ¨èå‡†"å’Œ"è¯­ä¹‰å¯¹"
        # è‹¹æœå’Œæ©˜å­å¯èƒ½è¢«è®­ç»ƒåˆ°å¾ˆè¿œ	è‹¹æœå’Œæ©˜å­ä¿æŒåœ¨"æ°´æœ"é™„è¿‘
        # å¤±å»æ³›åŒ–èƒ½åŠ›	            ä¿æŒæ³›åŒ–èƒ½åŠ›
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèï¼šw/o Prototype ====================
        # å½“ no_prototype=True æ—¶ç¦ç”¨ L_clusterï¼ˆå› ä¸º L_cluster ä¾èµ–åŸå‹æœºåˆ¶ï¼‰
        if self.no_prototype:
            L_cluster = torch.tensor(0.0, device=sequences.device)
        else:
            e_target = self.item_embeddings(target)  # [B, D] (256,128) è·å–ç›®æ ‡ç‰©å“çš„ ID embedding
            e_target_norm = F.normalize(e_target, p=2, dim=-1)  # [B, D]  (256,128)  L2 å½’ä¸€åŒ–
            proto_norm = F.normalize(self.prototypes, p=2, dim=-1)  # [K, D] å½’ä¸€åŒ–åŸå‹çŸ©é˜µ
            
            # ==================== [OLD] å•å¤´åŸå‹å¯»å€ï¼ˆå·²æ³¨é‡Šï¼‰====================
            # score_target = torch.matmul(e_target_norm, proto_norm.t()) / self.temperature
            # pi_target = F.softmax(score_target, dim=-1)
            # r_target = torch.matmul(pi_target, self.prototypes)
            # r_target = r_target * self.macro_scale
            # ==================== [END OLD] ====================
            
            # ==================== [NEW-MultiHead] å¤šå¤´åŸå‹å¯»å€ ====================
            B_proto = e_target.size(0)  # æ‰¹å¤§å°
            if self.num_heads_proto == 1:
                # å•å¤´æ¨¡å¼ï¼šæ ‡å‡†åŸå‹å¯»å€
                score_target = torch.matmul(e_target_norm, proto_norm.t()) / self.temperature  # [B, K] (256,64)ç›¸ä¼¼åº¦åˆ†æ•°
                pi_target = F.softmax(score_target, dim=-1)  # [B, K] (256,64)åŸå‹åˆ†å¸ƒï¼ˆè½¯åˆ†é…ï¼‰
                r_target = torch.matmul(pi_target, self.prototypes)  # [B, D] (256,128)åŠ æƒåŸå‹è¡¨ç¤º
            else:
                # å¤šå¤´æ¨¡å¼ï¼šå°† D ç»´åˆ‡åˆ†ä¸º H ä¸ªå­ç©ºé—´ï¼Œæ¯ä¸ª head ç‹¬ç«‹å¯»å€
                e_target_heads = e_target_norm.view(B_proto, self.num_heads_proto, self.head_dim)  # [B, H, d]
                proto_heads = proto_norm.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d]
                scores = torch.einsum('bhd,khd->bhk', e_target_heads, proto_heads) / self.temperature  # [B, H, K]
                pi_target = F.softmax(scores, dim=-1)  # [B, H, K] æ¯ä¸ª head ç‹¬ç«‹ softmax
                proto_unnorm = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)  # [K, H, d]
                r_heads = torch.einsum('bhk,khd->bhd', pi_target, proto_unnorm)  # [B, H, d] æ¯ä¸ª head çš„åŠ æƒåŸå‹
                r_target = r_heads.reshape(B_proto, self.hidden_dim)  # [B, D] æ‹¼æ¥å›åŸå§‹ç»´åº¦
            r_target = r_target * self.macro_scale  # åº”ç”¨å¯å­¦ä¹ ç¼©æ”¾å› å­ (256,128)
            # ==================== [END NEW-MultiHead] ====================
            
            # MSE æŸå¤±ï¼šè®© ID embedding é€¼è¿‘å¯¹åº”çš„åŸå‹è¡¨ç¤º
            # detach() é˜»æ­¢æ¢¯åº¦å›ä¼ åˆ°åŸå‹ï¼Œåªæ›´æ–° ID embedding
            L_cluster = F.mse_loss(e_target, r_target.detach())
        # ==================== [END NEW 2025-01-17] ====================

        # ==================== æ€»æŸå¤± ====================
        # L = L_rec + Î± * L_align + Î² * L_cluster
        # L_rec: ä¸»æ¨èä»»åŠ¡æŸå¤±ï¼ˆInfoNCEï¼‰
        # L_align: ç”¨æˆ·æ„å›¾å¯¹é½æŸå¤±ï¼ˆè®© ID è¡¨ç¤ºä¸ LLM æ„å›¾å¯¹é½ï¼‰
        # L_cluster: èšç±»æ­£åˆ™åŒ–æŸå¤±ï¼ˆè®© ID embedding ä¿æŒè¯­ä¹‰ç»“æ„ï¼‰
        loss = L_rec + self.alpha * L_align + self.beta_proto * L_cluster

        return loss  # è¿”å›æ€»æŸå¤±ç”¨äºåå‘ä¼ æ’­

    def _get_target_fused_emb_add(self, target):
        """è·å–ç›®æ ‡ç‰©å“çš„åŠ æ³•èåˆ embedding"""
        e_i = self.item_embeddings(target)
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèï¼šw/o Prototype ====================
        if self.no_prototype:
            return e_i  # æ¶ˆèæ¨¡å¼ï¼šç›´æ¥è¿”å›çº¯ ID embedding
        # ==================== [END NEW 2025-01-17] ====================
        
        # ==================== [OLD] å•å¤´åŸå‹å¯»å€ï¼ˆå·²æ³¨é‡Šï¼‰====================
        # score = torch.matmul(e_i, self.prototypes.t()) / self.temperature
        # pi = F.softmax(score, dim=-1)
        # r_i = torch.matmul(pi, self.prototypes)
        # r_i = r_i * self.macro_scale
        # ==================== [END OLD] ====================
        
        # ==================== [NEW-MultiHead] å¤šå¤´åŸå‹å¯»å€ ====================
        if self.num_heads_proto == 1:
            score = torch.matmul(e_i, self.prototypes.t()) / self.temperature
            pi = F.softmax(score, dim=-1)
            r_i = torch.matmul(pi, self.prototypes)
        else:
            # target å¯èƒ½æ˜¯ [B] æˆ– [B, N]ï¼Œéœ€è¦å¤„ç†ä¸åŒç»´åº¦
            orig_shape = e_i.shape[:-1]  # ä¿å­˜åŸå§‹å½¢çŠ¶ï¼ˆé™¤äº†æœ€åä¸€ç»´ï¼‰
            e_i_flat = e_i.view(-1, self.hidden_dim)  # å±•å¹³ä¸º [*, D]
            N = e_i_flat.size(0)
            
            e_i_heads = e_i_flat.view(N, self.num_heads_proto, self.head_dim)  # [*, H, d]
            proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)
            scores = torch.einsum('nhd,khd->nhk', e_i_heads, proto_heads) / self.temperature
            pi = F.softmax(scores, dim=-1)
            r_heads = torch.einsum('nhk,khd->nhd', pi, proto_heads)
            r_i_flat = r_heads.reshape(N, self.hidden_dim)
            r_i = r_i_flat.view(*orig_shape, self.hidden_dim)  # æ¢å¤åŸå§‹å½¢çŠ¶
        r_i = r_i * self.macro_scale
        # ==================== [END NEW-MultiHead] ====================
        
        return e_i + self.semantic_weight * r_i

    def _get_target_fused_emb_concat(self, target):
        """è·å–ç›®æ ‡ç‰©å“çš„æ‹¼æ¥èåˆ embedding"""
        e_i = self.item_embeddings(target) # (256,128)
        
        # ==================== [NEW 2025-01-17] åŸå‹æœºåˆ¶æ¶ˆèï¼šw/o Prototype ====================
        if self.no_prototype:
            r_i_dummy = torch.zeros_like(e_i)  # æ¶ˆèæ¨¡å¼ï¼šç”¨é›¶å‘é‡å¡«å……
            return torch.cat([e_i, r_i_dummy], dim=-1)
        # ==================== [END NEW 2025-01-17] ====================
        
        # ==================== [OLD] å•å¤´åŸå‹å¯»å€ï¼ˆå·²æ³¨é‡Šï¼‰====================
        # score = torch.matmul(e_i, self.prototypes.t()) / self.temperature
        # pi = F.softmax(score, dim=-1)
        # r_i = torch.matmul(pi, self.prototypes)
        # r_i = r_i * self.macro_scale
        # ==================== [END OLD] ====================
        
        # ==================== [NEW-MultiHead] å¤šå¤´åŸå‹å¯»å€ ====================
        if self.num_heads_proto == 1:
            score = torch.matmul(e_i, self.prototypes.t()) / self.temperature # (256,64)
            pi = F.softmax(score, dim=-1) # (256,64)
            r_i = torch.matmul(pi, self.prototypes) # (256,128)
        else:
            # target å¯èƒ½æ˜¯ [B] æˆ– [B, N]ï¼Œéœ€è¦å¤„ç†ä¸åŒç»´åº¦
            orig_shape = e_i.shape[:-1]
            e_i_flat = e_i.view(-1, self.hidden_dim)
            N = e_i_flat.size(0)
            
            e_i_heads = e_i_flat.view(N, self.num_heads_proto, self.head_dim)
            proto_heads = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)
            scores = torch.einsum('nhd,khd->nhk', e_i_heads, proto_heads) / self.temperature
            pi = F.softmax(scores, dim=-1)
            r_heads = torch.einsum('nhk,khd->nhd', pi, proto_heads)
            r_i_flat = r_heads.reshape(N, self.hidden_dim)
            r_i = r_i_flat.view(*orig_shape, self.hidden_dim)
        r_i = r_i * self.macro_scale
        # ==================== [END NEW-MultiHead] ====================
        
        return torch.cat([e_i, r_i], dim=-1) # (256,256)

    # ==================== è¾…åŠ©æ–¹æ³•ï¼šè®¡ç®—å¯¹é½æŸå¤±å’Œèšç±»æŸå¤± ====================
    def _compute_align_cluster_loss(self, sequences, target, h_u):
        """
        è®¡ç®— L_align å’Œ L_clusterï¼ˆä¾›ä¸‰ç§æŸå¤±å‡½æ•°å…±ç”¨ï¼‰

        Args:
            sequences: [B, S] è¾“å…¥åºåˆ—
            target: [B] ç›®æ ‡ç‰©å“ ID
            h_u: [B, D] SASRec ç¼–ç çš„ç”¨æˆ·è¡¨ç¤º

        Returns:
            L_align: å¯¹é½æŸå¤±
            L_cluster: èšç±»æŸå¤±
        """
        # ==================== L_align: æ„å›¾å¯¹é½æŸå¤± ====================
        L_align = torch.tensor(0.0, device=sequences.device)
        if self.user_intent_emb is not None:
            # æ³¨æ„ï¼šå½“å‰æ•°æ®é›†æ²¡æœ‰ user_idï¼Œæš‚æ—¶è·³è¿‡
            # å¦‚æœéœ€è¦å¯ç”¨ï¼Œéœ€è¦åœ¨æ•°æ®é›†ä¸­æ·»åŠ  user_id
            pass

        # ==================== L_cluster: èšç±»æŸå¤± ====================
        e_target = self.item_embeddings(target)
        e_target_norm = F.normalize(e_target, p=2, dim=-1)
        proto_norm = F.normalize(self.prototypes, p=2, dim=-1)
        
        # [FIX 2024-12-16] æ·»åŠ å¤šå¤´åŸå‹æ”¯æŒï¼ˆä¸ GRU/BERT4Rec ä¿æŒä¸€è‡´ï¼‰
        B = e_target.size(0)
        if self.num_heads_proto == 1:
            # å•å¤´æ¨¡å¼ï¼ˆåŸé€»è¾‘ï¼‰
            score_target = torch.matmul(e_target_norm, proto_norm.t()) / self.temperature
            pi_target = F.softmax(score_target, dim=-1)
            r_target = torch.matmul(pi_target, self.prototypes)
        else:
            # å¤šå¤´æ¨¡å¼
            e_target_heads = e_target_norm.view(B, self.num_heads_proto, self.head_dim)
            proto_heads = proto_norm.view(self.num_prototypes, self.num_heads_proto, self.head_dim)
            scores = torch.einsum('bhd,khd->bhk', e_target_heads, proto_heads) / self.temperature
            pi_target = F.softmax(scores, dim=-1)
            proto_unnorm = self.prototypes.view(self.num_prototypes, self.num_heads_proto, self.head_dim)
            r_heads = torch.einsum('bhk,khd->bhd', pi_target, proto_unnorm)
            r_target = r_heads.reshape(B, self.hidden_dim)
        r_target = r_target * self.macro_scale
        L_cluster = F.mse_loss(e_target, r_target.detach())

        return L_align, L_cluster

    # ==================== å…¼å®¹åŸºç±»çš„æŸå¤±å‡½æ•°æ¥å£ ====================
    def calculate_ce_loss(self, sequences, target):
        """
        Cross-Entropy æŸå¤±ï¼ˆå…¨é‡ softmaxï¼‰

        L = L_rec + Î± * L_align + Î² * L_cluster
        """
        H_final = self.forward(sequences)

        # L_rec: å…¨é‡ç‰©å“ softmax
        item_embs = self.return_item_emb()[:-1]  # å»æ‰ padding
        logits = torch.matmul(H_final, item_embs.t())
        L_rec = self.ce_loss(logits, target)

        # æå– h_uï¼ˆç”¨äºå¯¹é½æŸå¤±ï¼‰
        h_u = H_final[:, :self.hidden_dim] if self.fusion_mode == 'concat' else H_final

        # L_align + L_cluster
        L_align, L_cluster = self._compute_align_cluster_loss(sequences, target, h_u)

        # æ€»æŸå¤±
        loss = L_rec + self.alpha * L_align + self.beta_proto * L_cluster
        return loss

    def calculate_bce_loss(self, sequences, target, neg_ratio):
        """
        Binary Cross-Entropy æŸå¤±ï¼ˆè´Ÿé‡‡æ ·äºŒåˆ†ç±»ï¼‰

        L = L_rec + Î± * L_align + Î² * L_cluster
        """
        H_final = self.forward(sequences)

        # ==================== è´Ÿé‡‡æ · ====================
        batch_size = target.shape[0]
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()
        mask = neg_samples == expanded_target
        while mask.any():
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
            neg_samples = torch.where(mask, new_samples, neg_samples)
            mask = neg_samples == expanded_target
        neg_samples = neg_samples.to(target.device)

        # ==================== è·å–èåˆåçš„åµŒå…¥ ====================
        if self.fusion_mode == 'add':
            pos_embs = self._get_target_fused_emb_add(target)
            neg_embs = self._get_target_fused_emb_add(neg_samples)
        else:
            pos_embs = self._get_target_fused_emb_concat(target)
            neg_embs = self._get_target_fused_emb_concat(neg_samples)

        # ==================== BCE æŸå¤± ====================
        pos_logits = (H_final * pos_embs).sum(dim=-1)
        neg_logits = (H_final.unsqueeze(1) * neg_embs).sum(dim=-1)

        pos_labels = torch.ones(pos_logits.shape, device=self.device)
        neg_labels = torch.zeros(neg_logits.shape, device=self.device)

        L_rec = self.bce_loss(pos_logits, pos_labels) + self.bce_loss(neg_logits, neg_labels)

        # æå– h_u
        h_u = H_final[:, :self.hidden_dim] if self.fusion_mode == 'concat' else H_final

        # L_align + L_cluster
        L_align, L_cluster = self._compute_align_cluster_loss(sequences, target, h_u)

        # æ€»æŸå¤±
        loss = L_rec + self.alpha * L_align + self.beta_proto * L_cluster
        return loss

    def calculate_infonce_loss(self, sequences, target, neg_ratio, temperature):
        """
        InfoNCE å¯¹æ¯”å­¦ä¹ æŸå¤±

        L = L_rec + Î± * L_align + Î² * L_cluster
        """
        H_final = self.forward(sequences)

        # ==================== è´Ÿé‡‡æ · ====================
        batch_size = target.shape[0]
        neg_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
        expanded_target = target.view(batch_size, 1).expand(batch_size, neg_ratio).cpu()
        mask = neg_samples == expanded_target
        while mask.any():
            new_samples = torch.randint(0, self.item_num, (batch_size, neg_ratio))
            neg_samples = torch.where(mask, new_samples, neg_samples)
            mask = neg_samples == expanded_target
        neg_samples = neg_samples.to(target.device)

        # ==================== è·å–èåˆåçš„åµŒå…¥ ====================
        if self.fusion_mode == 'add':
            pos_embs = self._get_target_fused_emb_add(target)
            neg_embs = self._get_target_fused_emb_add(neg_samples)
        else:
            pos_embs = self._get_target_fused_emb_concat(target)
            neg_embs = self._get_target_fused_emb_concat(neg_samples)

        # ==================== InfoNCE æŸå¤± ====================
        # L2 å½’ä¸€åŒ–
        H_final_norm = F.normalize(H_final, p=2, dim=-1)
        pos_embs_norm = F.normalize(pos_embs, p=2, dim=-1)
        neg_embs_norm = F.normalize(neg_embs, p=2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        pos_logits = (H_final_norm * pos_embs_norm).sum(dim=-1, keepdim=True)
        neg_logits = torch.bmm(neg_embs_norm, H_final_norm.unsqueeze(-1)).squeeze(-1)

        # æ‹¼æ¥å¹¶é™¤ä»¥æ¸©åº¦
        logits = torch.cat([pos_logits, neg_logits], dim=-1) / temperature
        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)

        L_rec = F.cross_entropy(logits, labels)

        # æå– h_u
        h_u = H_final[:, :self.hidden_dim] if self.fusion_mode == 'concat' else H_final

        # L_align + L_cluster
        L_align, L_cluster = self._compute_align_cluster_loss(sequences, target, h_u)

        # æ€»æŸå¤±
        loss = L_rec + self.alpha * L_align + self.beta_proto * L_cluster
        return loss

    # ==================== [NEW] RASD (Retrieval Augmented Self-Distillation) æŸå¤± ====================
    def calculate_rasd_loss(self, sequences, sim_seqs, user_sim_func='cl'):
        """
        è®¡ç®— RASD å¯¹é½æŸå¤±ï¼ˆé€‚é… ProAlign çš„ç‰ˆæœ¬ï¼‰
        
        æ€è·¯ï¼šç”¨ç›¸ä¼¼ç”¨æˆ·çš„è¡¨ç¤ºä½œä¸º"æ•™å¸ˆ"ï¼Œè®©å½“å‰ç”¨æˆ·çš„è¡¨ç¤ºå‘æ•™å¸ˆé æ‹¢
        
        Args:
            sequences: [B, S] å½“å‰ç”¨æˆ·çš„ç‰©å“åºåˆ—
            sim_seqs: [B, K, S] ç›¸ä¼¼ç”¨æˆ·çš„ç‰©å“åºåˆ—ï¼ˆK ä¸ªç›¸ä¼¼ç”¨æˆ·ï¼‰
            user_sim_func: 'cl' (å¯¹æ¯”å­¦ä¹ ) æˆ– 'kd' (çŸ¥è¯†è’¸é¦/MSE)
        
        Returns:
            rasd_loss: æ ‡é‡æŸå¤±å€¼
        """
        B, K, S = sim_seqs.shape
        
        # 1. è·å–å½“å‰ç”¨æˆ·çš„è¡¨ç¤º
        h_u = self.forward(sequences)  # [B, D] æˆ– [B, 2D]
        
        # 2. è·å–ç›¸ä¼¼ç”¨æˆ·çš„è¡¨ç¤º
        sim_seqs_flat = sim_seqs.view(B * K, S)  # [B*K, S]
        h_sim = self.forward(sim_seqs_flat)  # [B*K, D] æˆ– [B*K, 2D]
        
        # 3. å…³é”®ï¼šstop gradientï¼Œç›¸ä¼¼ç”¨æˆ·ä½œä¸º"æ•™å¸ˆ"ä¸æ›´æ–°æ¢¯åº¦
        h_sim = h_sim.detach()
        
        # 4. é‡å¡‘å¹¶å–å¹³å‡
        h_sim = h_sim.view(B, K, -1)  # [B, K, D] æˆ– [B, K, 2D]
        h_sim_avg = h_sim.mean(dim=1)  # [B, D] æˆ– [B, 2D] å¤šä¸ªç›¸ä¼¼ç”¨æˆ·çš„å¹³å‡è¡¨ç¤º
        
        # 5. è®¡ç®—å¯¹é½æŸå¤±
        if user_sim_func == 'cl':
            # å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼š1 - cosine_similarity
            h_u_norm = F.normalize(h_u, p=2, dim=-1)
            h_sim_norm = F.normalize(h_sim_avg, p=2, dim=-1)
            rasd_loss = 1.0 - (h_u_norm * h_sim_norm).sum(dim=-1).mean()
        elif user_sim_func == 'kd':
            # çŸ¥è¯†è’¸é¦æŸå¤± (MSE)
            rasd_loss = F.mse_loss(h_u, h_sim_avg)
        else:
            raise ValueError(f"Unknown user_sim_func: {user_sim_func}")
        
        return rasd_loss
    # ==================== [END NEW] ====================

    # ==================== [NEW 2025-01-17] æ¨ç†æ•ˆç‡ä¼˜åŒ– ====================
    def precompute_for_inference(self):
        """
        æ¨ç†å‰é¢„è®¡ç®—ç‰©å“åŸå‹è¡¨ç¤ºï¼Œå®ç°ä¸ SASRec åŒé‡çº§çš„æ¨ç†æ•ˆç‡
        
        ä¼˜åŒ–åŸç†ï¼š
        1. SLSI çš„åŸå‹å¯»å€ä» O(BÃ—SÃ—DÃ—K) é™ä¸º O(1) æŸ¥è¡¨
        2. ç‰©å“ä¾§èåˆåµŒå…¥é¢„è®¡ç®—ï¼Œæ¨ç†æ—¶ç›´æ¥ä½¿ç”¨
        
        è°ƒç”¨æ—¶æœºï¼šåœ¨ model.eval() åã€æ¨ç†å‰è°ƒç”¨ä¸€æ¬¡
        
        Usage:
            model.eval()
            model.precompute_for_inference()
            with torch.no_grad():
                scores = model.predict(sequences)
        """
        if self.no_prototype:
            print("[ProAlign-Efficient] Prototype disabled, skip precompute")
            self._inference_mode = True
            return
            
        with torch.no_grad():
            # ========== 1. é¢„è®¡ç®—æ‰€æœ‰ç‰©å“çš„åŸå‹è¡¨ç¤º r_i ==========
            item_emb = self.item_embeddings.weight  # [V+1, D]
            V = item_emb.size(0)
            P = self.prototypes
            
            if self.num_heads_proto == 1:
                score_all = torch.matmul(item_emb, P.t()) / self.temperature
                pi_all = F.softmax(score_all, dim=-1)
                r_all = torch.matmul(pi_all, P)
            else:
                item_heads = item_emb.view(V, self.num_heads_proto, self.head_dim)
                proto_heads = P.view(self.num_prototypes, self.num_heads_proto, self.head_dim)
                scores = torch.einsum('vhd,khd->vhk', item_heads, proto_heads) / self.temperature
                pi = F.softmax(scores, dim=-1)
                r_heads = torch.einsum('vhk,khd->vhd', pi, proto_heads)
                r_all = r_heads.reshape(V, self.hidden_dim)
            
            r_all = r_all * self.macro_scale
            self._item_proto_cache = r_all  # [V+1, D] ç‰©å“åŸå‹è¡¨ç¤ºç¼“å­˜
            
            # ========== 2. é¢„è®¡ç®—èåˆåçš„ç‰©å“åµŒå…¥ï¼ˆç”¨äºæ‰“åˆ†ï¼‰==========
            if self.fusion_mode == 'add':
                self._fused_item_cache = item_emb + self.semantic_weight * r_all
            else:
                self._fused_item_cache = torch.cat([item_emb, r_all], dim=-1)
            
            self._inference_mode = True
            print(f"[ProAlign-Efficient] Inference cache ready: {V} items, mode={self.fusion_mode}")
    
    def clear_inference_cache(self):
        """æ¸…é™¤æ¨ç†ç¼“å­˜ï¼Œæ¢å¤è®­ç»ƒæ¨¡å¼"""
        self._item_proto_cache = None
        self._fused_item_cache = None
        self._inference_mode = False
    # ==================== [END NEW 2025-01-17] ====================


# ==================== [END NEW] ProAlign ç±» ====================


# ==================== [NEW] PPD è°ƒåº¦å™¨ ====================
# ç§»æ¤è‡ª BSARec proalign_sasrec.py
# æ¸è¿›å¼åŸå‹è’¸é¦ï¼ˆProgressive Prototype Distillationï¼‰
# ==================================================================================

class PPDScheduler:
    """
    Progressive Prototype Distillation è°ƒåº¦å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    - è®­ç»ƒæ—©æœŸï¼šä¿æŒ LLM è¯­ä¹‰å…ˆéªŒï¼ˆå†»ç»“åŸå‹ï¼‰
    - è®­ç»ƒä¸­æœŸï¼šå¸æ”¶ååŒè¿‡æ»¤ä¿¡å·ï¼ˆè§£å†»åŸå‹ï¼‰
    - è®­ç»ƒåæœŸï¼šEMA ç¨³å®šæ”¶æ•›ï¼ˆé˜²æ­¢æ³¢åŠ¨ï¼‰

    Args:
        model: ProAlign æ¨¡å‹å®ä¾‹
        total_epochs: æ€»è®­ç»ƒ epoch æ•°
        warmup_ratio: Phase 1 å æ¯”ï¼Œé»˜è®¤ 0.3 (30%)
        transition_ratio: Phase 2 ç»“æŸç‚¹å æ¯”ï¼Œé»˜è®¤ 0.7 (70%)
        ema_decay: EMA è¡°å‡ç³»æ•°ï¼Œé»˜è®¤ 0.99
        verbose: æ˜¯å¦æ‰“å°çŠ¶æ€å˜åŒ–
    """

    def __init__(self, model, total_epochs, warmup_ratio=0.3, transition_ratio=0.7,
                 ema_decay=0.99, verbose=True):
        self.model = model
        self.total_epochs = total_epochs
        self.warmup_epochs = int(total_epochs * warmup_ratio)
        self.transition_epochs = int(total_epochs * transition_ratio)
        self.ema_decay = ema_decay
        self.verbose = verbose

        # EMA å½±å­å‰¯æœ¬ï¼ˆç”¨äº Phase 3ï¼‰
        self.prototype_shadow = None

        # è®°å½•å½“å‰ phaseï¼ˆé¿å…é‡å¤æ‰“å°ï¼‰
        self.current_phase = None

    def step(self, epoch):
        """
        æ¯ä¸ª epoch å¼€å§‹æ—¶è°ƒç”¨ï¼Œæ›´æ–°åŸå‹çŠ¶æ€

        Args:
            epoch: å½“å‰ epoch ç¼–å· (0-indexed)
        """
        if epoch < self.warmup_epochs:
            # ====== Phase 1: å†»ç»“ (Warmup) ======
            # ä¿æŒ LLM è¯­ä¹‰ç»“æ„ä¸å˜ï¼Œè®© ID Embedding å…ˆå­¦ä¹ 
            self._set_phase(1, epoch)
            self.model.prototypes.requires_grad = False

        elif epoch < self.transition_epochs:
            # ====== Phase 2: è§£å†» (Transition) ======
            # å¼€å§‹å¸æ”¶ååŒè¿‡æ»¤ä¿¡å·ï¼ŒåŸå‹å¯è®­ç»ƒ
            self._set_phase(2, epoch)
            self.model.prototypes.requires_grad = True

        else:
            # ====== Phase 3: EMA ç¨³å®š (Refinement) ======
            # å¯è®­ç»ƒï¼Œä½†ç”¨ EMA é˜²æ­¢å‰§çƒˆæ³¢åŠ¨
            self._set_phase(3, epoch)
            self.model.prototypes.requires_grad = True

            # EMA æ›´æ–°åŸå‹
            with torch.no_grad():
                if self.prototype_shadow is None:
                    # é¦–æ¬¡è¿›å…¥ Phase 3ï¼Œåˆå§‹åŒ–å½±å­å‰¯æœ¬
                    self.prototype_shadow = self.model.prototypes.data.clone()
                else:
                    # EMA æ›´æ–°ï¼šshadow = decay * shadow + (1 - decay) * current
                    self.prototype_shadow = (
                            self.ema_decay * self.prototype_shadow +
                            (1 - self.ema_decay) * self.model.prototypes.data
                    )
                    # ç”¨ EMA å¹³æ»‘åçš„å€¼è¦†ç›–å½“å‰åŸå‹
                    self.model.prototypes.data = self.prototype_shadow.clone()

    def _set_phase(self, phase, epoch):
        """æ‰“å° phase å˜åŒ–ä¿¡æ¯"""
        if self.current_phase != phase:
            self.current_phase = phase
            if self.verbose:
                phase_names = {
                    1: "Phase 1: FROZEN (Warmup)",
                    2: "Phase 2: TRAINABLE (Transition)",
                    3: "Phase 3: TRAINABLE + EMA (Refinement)"
                }
                print(f"[PPD] Epoch {epoch}: Entering {phase_names[phase]}")

    def get_current_phase(self):
        """è¿”å›å½“å‰ phase ç¼–å·"""
        return self.current_phase

    def get_phase_info(self):
        """è¿”å› phase é…ç½®ä¿¡æ¯ï¼ˆç”¨äº loggingï¼‰"""
        return {
            "warmup_epochs": self.warmup_epochs,
            "transition_epochs": self.transition_epochs,
            "total_epochs": self.total_epochs,
            "ema_decay": self.ema_decay
        }

# ==================== [END NEW] PPD è°ƒåº¦å™¨ ====================


# ==================== [NEW 2024-12-15] IRLLRec æ¨¡å‹ ====================
# =============================================================================
# IRLLRec: Intent Representation Learning with LLM for Recommendation
# è®ºæ–‡: SIGIR 2025
# 
# æ ¸å¿ƒåˆ›æ–°ï¼ˆç®€åŒ–ç‰ˆï¼Œé€‚é…åºåˆ—æ¨èï¼‰ï¼š
# 1. æ„å›¾åŸå‹å­¦ä¹  (user_intent, item_intent)
# 2. æ„å›¾åˆ†è§£ (r = softmax(e @ C) @ C.T)
# 3. å¤šå±‚æ¬¡è’¸é¦ (L_kd + L_kd_int + L_kd_int_2 + L_ITM)
# 4. åŠ¨é‡ç¼–ç å™¨ (int_mlp_m, EMA æ›´æ–°)
# 
# æ³¨æ„ï¼šç®€åŒ–ç‰ˆä¸åŒ…å« GSLï¼ˆå›¾ç»“æ„å­¦ä¹ ï¼‰ï¼Œå› ä¸ºåºåˆ—æ¨èæ²¡æœ‰æ˜¾å¼çš„ç”¨æˆ·-ç‰©å“å›¾
# =============================================================================
class IRLLRec(SASRec_backbone):
    """
    IRLLRec: Intent Representation Learning (SIGIR 2025)
    åŸºäº SASRec backbone çš„åºåˆ—æ¨èé€‚é…ç‰ˆæœ¬
    
    æ ¸å¿ƒç»„ä»¶ï¼š
    - æ„å›¾åŸå‹çŸ©é˜µ: user_intent [emb_size, K], item_intent [emb_size, K]
    - æ„å›¾åˆ†è§£: r = softmax(e @ C) @ C.T
    - Profile MLP: usr/itm_emb_np.pkl â†’ hidden_dim (ç²—ç²’åº¦)
    - Intent MLP: user/item_intent_emb_3.pkl â†’ hidden_dim (ç»†ç²’åº¦)
    - åŠ¨é‡ Intent MLP: EMA æ›´æ–°çš„æ•™å¸ˆæ¨¡å‹
    
    æŸå¤±å‡½æ•°ï¼š
    - L_rec: æ¨èæŸå¤± (CE/BPR)
    - L_kd: Profile çº§åˆ«å¯¹é½ (InfoNCE)
    - L_kd_int: Intent çº§åˆ«å¯¹é½ (InfoNCE) â­æ ¸å¿ƒ
    - L_kd_int_2: åŠ å™ªå¯¹æ¯”æŸå¤± (Translation Alignment)
    - L_ITM: åŠ¨é‡è’¸é¦æŸå¤± (Interaction-Text Matching)
    """
    
    def __init__(self, device, **key_words):
        super().__init__(device, **key_words)
        
        # ==================== ä¿å­˜å‚æ•° ====================
        self.key_words = key_words
        self.device = device
        
        # ==================== ID Embedding ====================
        self.item_embeddings = nn.Embedding(
            num_embeddings=self.item_num + 1,
            embedding_dim=self.hidden_dim,
            padding_idx=self.item_num
        )
        
        # ==================== IRLLRec è¶…å‚æ•° ====================
        # æ„å›¾åŸå‹æ•°é‡ K
        self.intent_num = key_words.get('intent_num', 128)
        # Profile è’¸é¦æƒé‡
        self.kd_weight = key_words.get('kd_weight', 0.01)
        # Profile è’¸é¦æ¸©åº¦
        self.kd_temperature = key_words.get('kd_temperature', 0.2)
        # Intent è’¸é¦æƒé‡
        self.kd_int_weight = key_words.get('kd_int_weight', 0.02)
        # Intent è’¸é¦æ¸©åº¦
        self.kd_int_temperature = key_words.get('kd_int_temperature', 0.2)
        # åŠ å™ªå¯¹æ¯”æŸå¤±æƒé‡
        self.kd_int_weight_2 = key_words.get('kd_int_weight_2', 1e-7)
        # åŠ¨é‡è’¸é¦æŸå¤±æƒé‡
        self.kd_int_weight_3 = key_words.get('kd_int_weight_3', 1e-7)
        # åŠ¨é‡ç³»æ•°
        self.momentum = key_words.get('momentum', 0.999)
        # LLM ç»´åº¦
        self.llm_dim = key_words.get('llm_dim', 3072)
        self.profile_dim = key_words.get('profile_dim', 1536)
        
        # ==================== æ„å›¾åŸå‹çŸ©é˜µ ====================
        # ç”¨æˆ·æ„å›¾åŸå‹ [hidden_dim, intent_num]
        self.user_intent = nn.Parameter(
            torch.empty(self.hidden_dim, self.intent_num)
        )
        nn.init.xavier_uniform_(self.user_intent)
        # ç‰©å“æ„å›¾åŸå‹ [hidden_dim, intent_num]
        self.item_intent = nn.Parameter(
            torch.empty(self.hidden_dim, self.intent_num)
        )
        nn.init.xavier_uniform_(self.item_intent)
        
        # ==================== LLM åµŒå…¥ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰====================
        # Profile åµŒå…¥ï¼ˆç²—ç²’åº¦ï¼‰
        self.usrprf_embeds = None  # [user_num, profile_dim]
        self.itmprf_embeds = None  # [item_num, profile_dim]
        # Intent åµŒå…¥ï¼ˆç»†ç²’åº¦ï¼‰
        self.usrint_embeds = None  # [user_num, llm_dim]
        self.itmint_embeds = None  # [item_num, llm_dim]
        
        # ==================== MLP æ˜ å°„ç½‘ç»œ ====================
        # Profile MLP: profile_dim â†’ hidden_dim
        self.mlp = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆç­‰åŠ è½½æ•°æ®åçŸ¥é“ç»´åº¦ï¼‰
        
        # Intent MLP (å­¦ç”Ÿ): llm_dim â†’ hidden_dim
        self.int_mlp = nn.Sequential(
            nn.Linear(self.llm_dim, (self.llm_dim + self.hidden_dim) // 2),
            nn.LeakyReLU(),
            nn.Linear((self.llm_dim + self.hidden_dim) // 2, self.hidden_dim)
        )
        
        # Intent MLP (æ•™å¸ˆ/åŠ¨é‡): ç»“æ„ç›¸åŒï¼ŒEMA æ›´æ–°
        self.int_mlp_m = nn.Sequential(
            nn.Linear(self.llm_dim, (self.llm_dim + self.hidden_dim) // 2),
            nn.LeakyReLU(),
            nn.Linear((self.llm_dim + self.hidden_dim) // 2, self.hidden_dim)
        )
        
        # ==================== åˆå§‹åŒ–æƒé‡ ====================
        self._init_irllrec_weights()
        
        # ==================== å¤åˆ¶å‚æ•°åˆ°åŠ¨é‡æ¨¡å‹ ====================
        self._copy_params_to_momentum()
        
    def _init_irllrec_weights(self):
        """åˆå§‹åŒ– IRLLRec ç‰¹æœ‰çš„æƒé‡"""
        # åˆå§‹åŒ– ID Embedding
        nn.init.normal_(self.item_embeddings.weight, 0, 0.02)
        # åˆå§‹åŒ– Intent MLP
        for module in self.int_mlp:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
        for module in self.int_mlp_m:
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
    
    @torch.no_grad()
    def _copy_params_to_momentum(self):
        """å¤åˆ¶å­¦ç”Ÿæ¨¡å‹å‚æ•°åˆ°æ•™å¸ˆæ¨¡å‹ï¼ˆåˆå§‹åŒ–æ—¶è°ƒç”¨ï¼‰"""
        for param, param_m in zip(self.int_mlp.parameters(), 
                                   self.int_mlp_m.parameters()):
            param_m.data.copy_(param.data)
            param_m.requires_grad = False
    
    @torch.no_grad()
    def _momentum_update(self):
        """EMA æ›´æ–°æ•™å¸ˆæ¨¡å‹å‚æ•°ï¼ˆæ¯ä¸ª batch è°ƒç”¨ï¼‰"""
        for param, param_m in zip(self.int_mlp.parameters(), 
                                   self.int_mlp_m.parameters()):
            param_m.data = param_m.data * self.momentum + \
                          param.data * (1.0 - self.momentum)
    
    def load_embeddings(self, usrprf_path, itmprf_path, usrint_path, itmint_path):
        """
        åŠ è½½ LLM åµŒå…¥æ–‡ä»¶
        
        Args:
            usrprf_path: ç”¨æˆ· Profile åµŒå…¥è·¯å¾„ (usr_emb_np.pkl)ï¼Œå¯ä¸º None
            itmprf_path: ç‰©å“ Profile åµŒå…¥è·¯å¾„ (itm_emb_np.pkl)ï¼Œå¯ä¸º None
            usrint_path: ç”¨æˆ· Intent åµŒå…¥è·¯å¾„ (user_intent_emb_3.pkl æˆ– usr_intent_emb.pkl)
            itmint_path: ç‰©å“ Intent åµŒå…¥è·¯å¾„ (item_intent_emb_3.pkl æˆ– itm_intent_emb.pkl)
        """
        import os
        
        # ==================== [COMMENTED] Profile åµŒå…¥ï¼ˆç²—ç²’åº¦ï¼‰- AlphaFuse ä¸­æ²¡æœ‰è¿™ä¸¤ä¸ªæ–‡ä»¶ ====================
        # åŠ è½½ Profile åµŒå…¥ï¼ˆç²—ç²’åº¦ï¼‰
        # if usrprf_path is not None and os.path.exists(usrprf_path):
        #     with open(usrprf_path, 'rb') as f:
        #         usrprf = pickle.load(f)
        #     self.usrprf_embeds = torch.tensor(usrprf, dtype=torch.float32).to(self.device)
        #     print(f"[IRLLRec] Loaded user profile embedding: {self.usrprf_embeds.shape}")
        #     
        #     # åˆå§‹åŒ– Profile MLPï¼ˆæ ¹æ®å®é™…ç»´åº¦ï¼‰
        #     actual_profile_dim = self.usrprf_embeds.shape[1]
        #     self.mlp = nn.Sequential(
        #         nn.Linear(actual_profile_dim, (actual_profile_dim + self.hidden_dim) // 2),
        #         nn.LeakyReLU(),
        #         nn.Linear((actual_profile_dim + self.hidden_dim) // 2, self.hidden_dim)
        #     ).to(self.device)
        #     # åˆå§‹åŒ–æƒé‡
        #     for module in self.mlp:
        #         if isinstance(module, nn.Linear):
        #             nn.init.xavier_uniform_(module.weight)
        # else:
        #     print(f"[IRLLRec] Skipping user profile embedding (not available)")
        # 
        # if itmprf_path is not None and os.path.exists(itmprf_path):
        #     with open(itmprf_path, 'rb') as f:
        #         itmprf = pickle.load(f)
        #     self.itmprf_embeds = torch.tensor(itmprf, dtype=torch.float32).to(self.device)
        #     print(f"[IRLLRec] Loaded item profile embedding: {self.itmprf_embeds.shape}")
        # else:
        #     print(f"[IRLLRec] Skipping item profile embedding (not available)")
        # ==================== [END COMMENTED] ====================
        
        # Profile åµŒå…¥åœ¨ AlphaFuse ä¸­ä¸å¯ç”¨ï¼Œè·³è¿‡
        print(f"[IRLLRec] Profile embeddings (usr_emb_np.pkl, itm_emb_np.pkl) not used in AlphaFuse")
        
        # åŠ è½½ Intent åµŒå…¥ï¼ˆç»†ç²’åº¦ï¼‰
        if os.path.exists(usrint_path):
            with open(usrint_path, 'rb') as f:
                usrint = pickle.load(f)
            self.usrint_embeds = torch.tensor(usrint, dtype=torch.float32).to(self.device)
            print(f"[IRLLRec] Loaded user intent embedding: {self.usrint_embeds.shape}")
            
            # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…ï¼Œå¦‚æœä¸åŒ¹é…åˆ™é‡æ–°åˆå§‹åŒ– MLP
            actual_llm_dim = self.usrint_embeds.shape[1]
            if actual_llm_dim != self.llm_dim:
                print(f"[IRLLRec] Adjusting int_mlp for dim {actual_llm_dim}")
                self.llm_dim = actual_llm_dim
                self.int_mlp = nn.Sequential(
                    nn.Linear(self.llm_dim, (self.llm_dim + self.hidden_dim) // 2),
                    nn.LeakyReLU(),
                    nn.Linear((self.llm_dim + self.hidden_dim) // 2, self.hidden_dim)
                ).to(self.device)
                self.int_mlp_m = nn.Sequential(
                    nn.Linear(self.llm_dim, (self.llm_dim + self.hidden_dim) // 2),
                    nn.LeakyReLU(),
                    nn.Linear((self.llm_dim + self.hidden_dim) // 2, self.hidden_dim)
                ).to(self.device)
                self._init_irllrec_weights()
                self._copy_params_to_momentum()
        else:
            print(f"[IRLLRec] Warning: User intent file not found: {usrint_path}")
        
        if os.path.exists(itmint_path):
            with open(itmint_path, 'rb') as f:
                itmint = pickle.load(f)
            self.itmint_embeds = torch.tensor(itmint, dtype=torch.float32).to(self.device)
            print(f"[IRLLRec] Loaded item intent embedding: {self.itmint_embeds.shape}")
        else:
            print(f"[IRLLRec] Warning: Item intent file not found: {itmint_path}")
    
    def embed_ID(self, x):
        """è·å–ç‰©å“ ID åµŒå…¥"""
        return self.item_embeddings(x)
    
    def return_item_emb(self):
        """è¿”å›å…¨é‡ç‰©å“åµŒå…¥"""
        return self.item_embeddings.weight
    
    def intent_decompose(self, embeds, intent_matrix):
        """
        æ„å›¾åˆ†è§£ï¼šå°†åµŒå…¥æ˜ å°„åˆ°æ„å›¾ç©ºé—´
        
        å…¬å¼ 9-10ï¼š
        P(c^k | e) = softmax(e @ C)
        r = P @ C.T = softmax(e @ C) @ C.T
        
        Args:
            embeds: [B, D] è¾“å…¥åµŒå…¥
            intent_matrix: [D, K] æ„å›¾åŸå‹çŸ©é˜µ
        
        Returns:
            [B, D] æ„å›¾åˆ†è§£åçš„åµŒå…¥
        """
        # softmax(e @ C) @ C.T
        return torch.softmax(embeds @ intent_matrix, dim=-1) @ intent_matrix.T
    
    def cal_infonce_loss(self, anchor, positive, negatives, temperature):
        """
        InfoNCE å¯¹æ¯”æŸå¤±
        
        Args:
            anchor: [B, D] é”šç‚¹åµŒå…¥
            positive: [B, D] æ­£æ ·æœ¬åµŒå…¥
            negatives: [N, D] è´Ÿæ ·æœ¬æ± ï¼ˆåŒ…å«æ­£æ ·æœ¬ï¼‰
            temperature: æ¸©åº¦å‚æ•°
        
        Returns:
            InfoNCE æŸå¤±
        """
        # L2 å½’ä¸€åŒ–
        anchor_norm = F.normalize(anchor, p=2, dim=-1)
        positive_norm = F.normalize(positive, p=2, dim=-1)
        negatives_norm = F.normalize(negatives, p=2, dim=-1)
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = (anchor_norm * positive_norm).sum(dim=-1) / temperature  # [B]
        
        # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ï¼ˆä¸æ‰€æœ‰ negatives è®¡ç®—ï¼‰
        neg_sim = anchor_norm @ negatives_norm.T / temperature  # [B, N]
        
        # InfoNCE: -log(exp(pos) / sum(exp(neg)))
        # numerator = -pos_sim
        # denominator = torch.logsumexp(neg_sim, dim=-1)
        # loss = (numerator + denominator).sum()
        
        # ç®€åŒ–è®¡ç®—ï¼šä½¿ç”¨ logsumexp æŠ€å·§
        logits = neg_sim  # [B, N]
        loss = -pos_sim + torch.logsumexp(logits, dim=-1)
        
        return loss.sum()
    
    def ssl_con_loss(self, embeds1, embeds2):
        """
        SSL å¯¹æ¯”æŸå¤±ï¼ˆç”¨äºåŠ å™ªå¯¹æ¯”ï¼‰
        
        Args:
            embeds1: [N, D] åµŒå…¥1
            embeds2: [N, D] åµŒå…¥2
        
        Returns:
            å¯¹æ¯”æŸå¤±
        """
        embeds1_norm = F.normalize(embeds1, p=2, dim=-1)
        embeds2_norm = F.normalize(embeds2, p=2, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = embeds1_norm @ embeds2_norm.T  # [N, N]
        
        # å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬
        pos = torch.diag(sim_matrix)
        
        # å¯¹æ¯”æŸå¤±
        loss = -pos + torch.logsumexp(sim_matrix, dim=-1)
        
        return loss.mean()
    
    def calculate_irllrec_loss(self, seq_output, user_ids, item_ids):
        """
        è®¡ç®— IRLLRec çš„æ‰€æœ‰è’¸é¦æŸå¤±
        
        Args:
            seq_output: [B, D] åºåˆ—ç¼–ç è¾“å‡ºï¼ˆä»£è¡¨ç”¨æˆ·è¡¨ç¤ºï¼‰
            user_ids: [B] ç”¨æˆ· ID
            item_ids: [B] ç‰©å“ IDï¼ˆæ­£æ ·æœ¬ï¼‰
        
        Returns:
            dict: åŒ…å«æ‰€æœ‰æŸå¤±é¡¹çš„å­—å…¸
        """
        losses = {}
        B = seq_output.shape[0]
        
        # ============================================================
        # æ­¥éª¤1ï¼šæ„å›¾åˆ†è§£
        # ============================================================
        # ç”¨æˆ·æ„å›¾åˆ†è§£
        user_int = self.intent_decompose(seq_output, self.user_intent)  # [B, D]
        
        # ç‰©å“åµŒå…¥å’Œæ„å›¾åˆ†è§£
        item_embs = self.embed_ID(item_ids)  # [B, D]
        item_int = self.intent_decompose(item_embs, self.item_intent)  # [B, D]
        
        # ============================================================
        # æ­¥éª¤2ï¼šL_kd - Profile çº§åˆ«å¯¹é½ï¼ˆå¦‚æœæœ‰ Profile åµŒå…¥ï¼‰
        # ============================================================
        if self.usrprf_embeds is not None and self.mlp is not None:
            # è·å– batch å¯¹åº”çš„ Profile åµŒå…¥
            usrprf_batch = self.usrprf_embeds[user_ids]  # [B, profile_dim]
            usrprf_mapped = self.mlp(usrprf_batch)  # [B, D]
            
            # ç”¨æˆ·ä¾§ Profile å¯¹é½
            kd_loss_user = self.cal_infonce_loss(
                seq_output, usrprf_mapped, 
                self.mlp(self.usrprf_embeds), 
                self.kd_temperature
            )
            
            # ç‰©å“ä¾§ Profile å¯¹é½ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.itmprf_embeds is not None:
                itmprf_batch = self.itmprf_embeds[item_ids]  # [B, profile_dim]
                itmprf_mapped = self.mlp(itmprf_batch)  # [B, D]
                
                kd_loss_item = self.cal_infonce_loss(
                    item_embs, itmprf_mapped,
                    itmprf_mapped,  # ä½¿ç”¨ batch å†…ä½œä¸ºè´Ÿæ ·æœ¬
                    self.kd_temperature
                )
                kd_loss = (kd_loss_user + kd_loss_item) / B
            else:
                kd_loss = kd_loss_user / B
            
            losses['kd_loss'] = kd_loss * self.kd_weight
        else:
            losses['kd_loss'] = torch.tensor(0.0).to(self.device)
        
        # ============================================================
        # æ­¥éª¤3ï¼šL_kd_int - Intent çº§åˆ«å¯¹é½ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        # ============================================================
        if self.usrint_embeds is not None:
            # è·å– batch å¯¹åº”çš„ Intent åµŒå…¥
            usrint_batch = self.usrint_embeds[user_ids]  # [B, llm_dim]
            usrint_mapped = self.int_mlp(usrint_batch)  # [B, D]
            
            # ç”¨æˆ·ä¾§ Intent å¯¹é½ï¼šäº¤äº’æ„å›¾ â†” æ–‡æœ¬æ„å›¾
            kd_int_loss_user = self.cal_infonce_loss(
                user_int, usrint_mapped,
                self.int_mlp(self.usrint_embeds),
                self.kd_int_temperature
            )
            
            # ç‰©å“ä¾§ Intent å¯¹é½ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.itmint_embeds is not None:
                itmint_batch = self.itmint_embeds[item_ids]  # [B, llm_dim]
                itmint_mapped = self.int_mlp(itmint_batch)  # [B, D]
                
                kd_int_loss_item = self.cal_infonce_loss(
                    item_int, itmint_mapped,
                    itmint_mapped,
                    self.kd_int_temperature
                )
                kd_int_loss = (kd_int_loss_user + kd_int_loss_item) / B
            else:
                kd_int_loss = kd_int_loss_user / B
            
            losses['kd_int_loss'] = kd_int_loss * self.kd_int_weight
        else:
            losses['kd_int_loss'] = torch.tensor(0.0).to(self.device)
        
        # ============================================================
        # æ­¥éª¤4ï¼šL_kd_int_2 - åŠ å™ªå¯¹æ¯”æŸå¤± (Translation Alignment)
        # ============================================================
        if self.usrint_embeds is not None:
            # è·å–äº¤äº’æ„å›¾å’Œæ–‡æœ¬æ„å›¾
            all_user_int = self.intent_decompose(seq_output, self.user_intent)
            all_text_int = self.int_mlp(self.usrint_embeds[user_ids])
            
            # æ·»åŠ é«˜æ–¯å™ªå£°
            noise_r = torch.randn_like(all_user_int)
            noise_z = torch.randn_like(all_text_int)
            
            r_prime = all_user_int + all_user_int * noise_r
            z_prime = all_text_int + all_text_int * noise_z
            
            # å¯¹æ¯”æŸå¤±
            kd_int_2_loss = self.ssl_con_loss(z_prime, r_prime)
            losses['kd_int_2_loss'] = kd_int_2_loss * self.kd_int_weight_2
        else:
            losses['kd_int_2_loss'] = torch.tensor(0.0).to(self.device)
        
        # ============================================================
        # æ­¥éª¤5ï¼šL_ITM - åŠ¨é‡è’¸é¦æŸå¤±
        # ============================================================
        if self.usrint_embeds is not None:
            # æ›´æ–°åŠ¨é‡æ¨¡å‹
            self._momentum_update()
            
            # å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            student_out = self.int_mlp(self.usrint_embeds[user_ids])
            # æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆæ— æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_out = self.int_mlp_m(self.usrint_embeds[user_ids])
            
            # KL æ•£åº¦æŸå¤±ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨ MSEï¼‰
            itm_loss = F.mse_loss(student_out, teacher_out)
            losses['itm_loss'] = itm_loss * self.kd_int_weight_3
        else:
            losses['itm_loss'] = torch.tensor(0.0).to(self.device)
        
        # ============================================================
        # æ±‡æ€»æ‰€æœ‰æŸå¤±
        # ============================================================
        losses['total_irllrec_loss'] = (
            losses['kd_loss'] + 
            losses['kd_int_loss'] + 
            losses['kd_int_2_loss'] + 
            losses['itm_loss']
        )
        
        return losses

    # ==================== [NEW 2024-12-16] é‡å†™ calculate_infonce_loss ====================
    # é—®é¢˜ï¼šåŸæ¥ IRLLRec æ²¡æœ‰é‡å†™è¿™ä¸ªæ–¹æ³•ï¼Œå¯¼è‡´è’¸é¦æŸå¤±ä¸ä¼šè¢«è®¡ç®—
    # è§£å†³ï¼šé‡å†™æ–¹æ³•ï¼Œåœ¨åŸºç¡€ InfoNCE æŸå¤±ä¸Šæ·»åŠ  IRLLRec ç‰¹æœ‰çš„è’¸é¦æŸå¤±
    def calculate_infonce_loss(self, sequences, target, neg_ratio, temperature, user_ids=None):
        """
        é‡å†™ InfoNCE æŸå¤±ï¼Œé›†æˆ IRLLRec ç‰¹æœ‰çš„è’¸é¦æŸå¤±
        
        æ³¨æ„ï¼šéœ€è¦ train.py ä¼ å…¥ user_ids æ‰èƒ½è®¡ç®—å®Œæ•´çš„è’¸é¦æŸå¤±
        å¦‚æœæ²¡æœ‰ user_idsï¼Œåªè¿”å›åŸºç¡€ InfoNCE æŸå¤±
        """
        # Step 1: è®¡ç®—åŸºç¡€ InfoNCE æŸå¤±ï¼ˆè°ƒç”¨çˆ¶ç±»æ–¹æ³•ï¼‰
        rec_loss = super().calculate_infonce_loss(sequences, target, neg_ratio, temperature)
        
        # Step 2: å¦‚æœæ²¡æœ‰ user_ids æˆ– Intent åµŒå…¥ï¼Œåªè¿”å›åŸºç¡€æŸå¤±
        if user_ids is None or self.usrint_embeds is None:
            return rec_loss
        
        # Step 3: è·å–åºåˆ—è¡¨ç¤º
        seq_output = self.forward(sequences)  # [B, D]
        
        # Step 4: è®¡ç®— IRLLRec è’¸é¦æŸå¤±
        irll_losses = self.calculate_irllrec_loss(seq_output, user_ids, target)
        
        # Step 5: è¿”å›æ€»æŸå¤±
        total_loss = rec_loss + irll_losses['total_irllrec_loss']
        
        return total_loss
    # ==================== [END NEW] ====================

# ==================== [END NEW] IRLLRec æ¨¡å‹ ====================